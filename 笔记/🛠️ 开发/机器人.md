
# 机器人

https://github.com/huggingface/lerobot

## ICRT

[code](https://github.com/Max-Fu/icrt/tree/main) | [model](https://huggingface.co/mlfu7/ICRT) | [Datasets](https://huggingface.co/datasets/Ravenh97/ICRT-MT)

### Install
按照教程基本就装好了
```shell
# create conda env
conda create -n icrt python=3.10 -y
conda activate icrt
# install torch 参考【Log】-【Linux环境】-【PyTorch】

conda install -c conda-forge ffmpeg
# download repo
git clone https://github.com/Max-Fu/icrt.git
cd icrt
pip install -e .
```

安装Jupyter Notebook: 参考【Log】-【Linux环境】-【Python常见库安装】-【Jupyter Notebook】


### Quick Start

#### 模型下载
先只下载`crossmae_rtx`和`icrt_vitb_droid_pretrained`两个文件夹中的文件即可。然后在运行过程中还可能会有其他模型会自动从Hugging Face上拉取

#### 数据集准备
下载任意一个hdf5文件即可，每一个hdf5文件中都有一个对应的`keys.json`文件，可以用来查看文件中都有哪些序列。

#### 开始推演
准备好模型和数据集后就可以直接使用，使用写好的Jupyter Notebook，文件是`tools/inference.ipynb`，基本上最大的问题是中途用了一个未知的模型要从HuggingFace下载

如果想启用数据集可视化代码可能会遇到动画最大容量限制的警告，按照警告的内容把允许容量设大一些
```python
plt.rcParams['animation.embed_limit'] = 300
```

### 参数配置
打开`run.yaml`文件可以看到配置，其中包含有llama2的路径。

在`Breadcrumbsicrt/icrt/models/policy/icrt.py`中有：
```python
        if self.scratch_llama_config is not None:
            llama_config_path = self.scratch_llama_config
        else:
            llama_config_path = os.path.join(llama_ckpt_dir, "params.json")
```

可以看到，当`scratch_llama_config`有值时，是直接走的对应的配置文件，这个配置文件在路径`config/model_config`下：


::: code-group
```json [工程中的custom_transformer.json]
{
  "dim": 768,
  "multiple_of": 256,
  "n_heads": 12,
  "n_layers": 12,
  "norm_eps": 1e-05,
  "vocab_size": -1
}
```

```json [Llama2的params.json]
{
  "dim": 4096,
  "multiple_of": 256,
  "n_heads": 32,
  "n_layers": 32,
  "norm_eps": 1e-05,
  "vocab_size": -1
}
```
:::

## 手眼标定

手眼标定（Hand-Eye Calibration）是机器人技术中的一个重要概念，它涉及到如何将机器人的机械臂（手）与视觉系统（眼）进行协调和校准，以确保两者能够协同工作，实现精准的定位和操作。在工业自动化、医疗手术机器人、服务机器人等领域中，手眼标定技术的应用非常广泛。

具体来说，手眼标定的目标是确定机器人的末端执行器（例如机械臂的夹爪）与相机之间的相对位置和姿态关系。这包括两部分：
1. 眼在手上（Eye-in-Hand）：这种情况下，相机安装在机器人的末端执行器上。目标是确定相机相对于末端执行器的位置和方向，以便于通过视觉反馈调整机械臂的动作，使其能够准确地到达目标位置。
2. 眼在手外（Eye-to-Hand）：相机固定在外部环境中，不随机械臂移动。这时需要确定的是相机与机械臂基座之间的相对位置和方向，这样即使相机不动，也能根据视觉信息控制机械臂完成任务。

为了实现手眼标定，通常需要执行一系列的实验步骤，这些步骤可能包括：
1. 收集数据：让机器人执行一系列已知运动，并记录下每个位置上的图像。
2. 特征点检测：从图像中提取特征点或标记，这些点在不同的图像之间可以对应起来。
3. 求解变换矩阵：利用数学方法（如最小二乘法等）计算出描述相机与机械臂之间相对位置和姿态关系的变换矩阵。
4. 验证和优化：通过实际测试来验证标定结果的准确性，并根据需要进行调整优化。

物体检测与定位： 在完成手眼标定后，利用手眼标定得到的变换矩阵，将检测到的物体在相机坐标系下的位置转换为机械臂坐标系下的位置

机械臂规划与控制：一旦确定了目标物体在机械臂坐标系下的位置，接下来就是规划和控制机械臂进行抓取

### 机械臂坐标系

机械臂坐标系是用于描述机械臂各部件位置和姿态的一种参考框架。在机器人学中，坐标系的定义和使用是非常重要的，因为它们提供了统一的参考，使得机械臂的运动控制和路径规划变得更加精确和可靠。机械臂坐标系通常包括以下几个主要部分：

1. 基坐标系（Base Frame）

基坐标系是机械臂的固定参考框架，通常设置在机械臂的底座或安装点。它是整个机械臂系统的全局参考点，所有其他坐标系都相对于基坐标系进行定义。基坐标系通常用$O_0$表示，
其原点位于机械臂的固定点，三个轴$X_0$,$Y_0$,$Z_0$定义了空间的方向。

2. 关节坐标系（Joint Frames）

每个关节都有自己的局部坐标系，用于描述该关节的位置和姿态。关节坐标系通常用$O_i$表示，其中$i$是关节的编号。每个关节坐标系的原点通常位于关节的旋转中心或平移方向的起点，
三个轴$X_i,Y_i,Z_i$定义了关节的运动方向。

3. 工具坐标系（Tool Frame）

工具坐标系是安装在机械臂末端执行器（如夹爪、焊枪等）上的局部坐标系，用于描述工具的位置和姿态。工具坐标系通常用$O_T$表示，其原点位于工具的有效作用点（TCP，Tool Center Point），
三个轴$X_T,Y_T,Z_T$定义了工具的方向。

4. 目标坐标系（Target Frame）

目标坐标系是描述目标物体或任务空间的坐标系，用于定义机械臂需要达到的位置和姿态。目标坐标系通常用$O_G$表示，其原点和轴根据具体任务的需求进行定义。

#### 坐标系之间的变换
在机械臂的运动控制中，需要频繁地在不同的坐标系之间进行变换。这些变换通常通过齐次变换矩阵来表示，齐次变换矩阵包含了平移和旋转的信息。常见的变换包括：
- 基坐标系到关节坐标系的变换：描述每个关节相对于基坐标系的位置和姿态。
- 关节坐标系之间的变换：描述相邻关节之间的相对位置和姿态。
- 关节坐标系到工具坐标系的变换：描述工具相对于最后一个关节的位置和姿态。
- 基坐标系到目标坐标系的变换：描述目标物体相对于基坐标系的位置和姿态。

举例说明

假设有一个简单的三自由度机械臂，其关节坐标系分别为$O_1, O_2, O_3$，工具坐标系为$O_T$，基坐标系为$O_0$。要将工具移动到目标位置$O_G$ ，需要进行以下步骤：
1. 确定基坐标系到目标坐标系的变换：计算$T_{0G}$，即从基坐标系$O_0$到目标坐标系$O_G$的变换矩阵。
2. 确定基坐标系到工具坐标系的变换：计算$T_{0T}$，即从基坐标系$O_0$到工具坐标系$O_T$的变换矩阵。
3. 求解关节角度：通过逆运动学计算，找到使$T_{0T} = T_{0G}$的关节角度$\theta_1, \theta_2, \theta_3$。
4. 控制机械臂：根据计算出的关节角度，控制机械臂的运动，使工具到达目标位置。

通过这些步骤，机械臂可以精确地控制其末端执行器的位置和姿态，完成各种复杂的任务。

#### 示例流程
1. 标定阶段：
   - 让机械臂在不同位置拍摄图像。
   - 使用标定板（如棋盘格）进行特征点检测。
   - 计算相机与机械臂之间的变换矩阵。

2. 抓取阶段：
   - 拍摄包含目标物体的图像。
   - 使用计算机视觉算法检测目标物体的位置和姿态。
   - 利用变换矩阵将物体位置转换到机械臂坐标系。
   - 规划机械臂的运动路径。
   - 控制机械臂到达目标位置并进行抓取。
   - 验证抓取是否成功，必要时进行调整。
