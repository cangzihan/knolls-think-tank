---
tags:
  - 深度学习
---

# 模型文件

## GGML
GGML（Georgi Gerganov Machine Learning） 是一种轻量级的机器学习库和模型格式，由 Georgi Gerganov 开发，主要用于高效的 CPU 推理。它最早用于 LLaMA.cpp，后来也被许多本地 LLM 推理工具（如 llama.cpp 和 gpt4all）采用。

特点：
专为 CPU 推理优化，适用于 AVX、AVX2、AVX-512 以及 Apple Silicon（M1/M2）。
支持量化（Quantization），可以降低 LLM 运行时的显存/内存需求，比如 4-bit（Q4_0、Q4_K）、5-bit、8-bit 等。
广泛兼容性，支持不同的 LLM，如 LLaMA、Mistral、Falcon 等。
存储方式：二进制格式，支持 ggml 库直接加载。

## GGUF
GGUF（GPTQ for GGML Unified Format） 是 GGML 的进化版，专门为 更高效的 LLM 存储和推理 设计，取代了旧的 GGML 格式。

特点：
更灵活的格式，支持更多元数据（如 tokenizer 信息、特定参数等），避免额外的转换需求。
提升性能，改进了量化方式，适用于更多 LLM 变体，如 LLaMA 2/3、Mistral、Gemma 等。
支持 GPU 加速，相比 GGML，在 OpenCL、Metal、CUDA 上的性能更好。
官方支持：主要用于 llama.cpp 等项目，并逐渐成为标准格式。

`qwen2.5-coder-7b-instruct-q6_k.gguf`的含义：

- qwen（千问）是阿里云（Alibaba Cloud）推出的大语言模型系列的名称。
- 2.5 代表 Qwen 2.5 版本，这是该模型的第二代半更新版，通常比 2.0 具有更好的性能和优化。
- coder 表示该模型专门针对 代码生成和理解 进行了优化，相比于通用语言模型，它在编程任务上表现更佳。
- 代表 模型的参数量，7B 即 7 Billion（70 亿） 参数。参数越多，模型通常在推理能力上更强，但同时也需要更多计算资源。
- instruct说明该模型是 "Instruct"（指令微调）版本。 这意味着它专门经过指令微调（Instruction tuning），可以更好地理解和响应指令，例如代码解释、生成等任务。
- q6_k: Q6_K 代表 量化（Quantization）方法，用于减少模型大小、降低运行内存消耗，同时尽量保留推理性能。 Q6_K 是 GGUF 量化格式 中的一种方案，通常表示 6-bit 量化（相较于 16-bit FP16 或 32-bit FP32，降低了存储需求）。 Q6_K 相比 Q4 或 Q5 量化，保留了更多精度，但仍比 Q8 更省显存。

### 如何使用GGUF文件
1. 使用[transformers库](https://github.com/huggingface/transformers/blob/main/docs/source/zh/gguf.md)加载模型。
```python
from transformers import AutoTokenizer, AutoModelForCausalLM

model_id = "TheBloke/TinyLlama-1.1B-Chat-v1.0-GGUF"
filename = "tinyllama-1.1b-chat-v1.0.Q6_K.gguf"

tokenizer = AutoTokenizer.from_pretrained(model_id, gguf_file=filename)
model = AutoModelForCausalLM.from_pretrained(model_id, gguf_file=filename)
```
2. LLM Farm移动端部署（适用于IOS、iPad OS）：直接将单个GGUF文件在LLM Farm APP中导入
3. 使用Ollama
   1. 将GGUF文件放入特定路径内
      - Windows系统：把模型放在`C:\Users\【用户名】\.ollama\models`下
   2. 创建一个文件命名为`Modelfile`内容参考`FROM ./Tifa-DeepsexV2-7b-Q4_KM.gguf`
   3. 在文件夹内启动终端，`ollama create deepsex2`
   4. 导入成功后查看`ollama list`
   5. Test the model`ollama run deepsex2`

## Safetensors
https://huggingface.co/docs/safetensors/index

Safetensors is a new simple format for storing tensors safely (as opposed to pickle) and that is still fast (zero-copy).
Safetensors is really fast 🚀.

### Installation
```shell
pip install safetensors
```

### Usage

#### Save
```python
import torch
from safetensors.torch import save_file

tensors = {
    "embedding": torch.zeros((2, 2)),
    "attention": torch.zeros((2, 3))
}
save_file(tensors, "model.safetensors")
```

#### Load
```python
from safetensors import safe_open

tensors = {}
with safe_open("model.safetensors", framework="pt", device=0) as f:
    for k in f.keys():
        tensors[k] = f.get_tensor(k)
```

一个打印模型内容的程序：
```python
import pprint
from safetensors import safe_open

saved_keys = []
def print_tree(lst, depth=3):
  for key in lst:
    parts = key.split('.')
    if parts[:depth] not in saved_keys:
      saved_keys.append(parts[:depth])
   #   for i in range(min(depth, len(parts))):
   #     print('  ' * i + parts[i])

  tree = {}
  for key in saved_keys:
    if key[0] not in tree:
      tree[key[0]] = {}
    current = tree[key[0]]
    for d in range(1, len(key))[:-2]:
      if key[d] not in current:
        current[key[d]] = {}
      current = current[key[d]]
    if key[-2] not in current:
      current[key[-2]] = [key[-1]]
    else:
      current[key[-2]].append(key[-1])

  pprint.pprint(tree)
  return tree


model_path = "anypastelAnythingV45_anypastelAnythingV45.safetensors"
tensors_anything = {}
with safe_open(model_path, framework="pt", device=0) as f:
    for k in f.keys():
        tensors_anything[k] = f.get_tensor(k)
tree_dict = print_tree(tensors_anything.keys())
```

运行结果：
::: code-group
```json [anypastelAnythingV45 / GuoFeng3_Fix]
{
  "cond_stage_model": {"transformer": ["text_model"]},
  "first_stage_model": {
    "decoder": [
      "conv_in",
      "conv_out",
      "mid",
      "norm_out",
      "up"
    ],
    "encoder": [
      "conv_in",
      "conv_out",
      "down",
      "mid",
      "norm_out"
    ],
    "post_quant_conv": ["bias", "weight"],
    "quant_conv": ["bias", "weight"]
  },
  "model": {
    "diffusion_model": [
      "input_blocks",
      "middle_block",
      "out",
      "output_blocks",
      "time_embed"
    ]
  }
}
```

```json [chilloutmix_NiPrunedFp32Fix]
{
  "cond_stage_model": {"transformer": ["text_model"]},
  "first_stage_model": {
    "decoder": [
      "conv_in",
      "conv_out",
      "mid",
      "norm_out",
      "up"
    ],
    "encoder": [
      "conv_in",
      "conv_out",
      "down",
      "mid",
      "norm_out"
    ],
    "post_quant_conv": ["bias", "weight"],
    "quant_conv": ["bias", "weight"]
  },
  "model": {
    "diffusion_model": [
      "input_blocks",
      "middle_block",
      "out",
      "output_blocks",
      "time_embed"
    ]
  },
  "model_ema": {"model_ema": ["decay", "num_updates"]}
}
```

```json [koreanDollLikeness_v10(LORA)]
{
  "cond_stage_model": {"transformer": ["text_model"]},
  "embedding_manager": {"embedder": ["transformer"]},
  "first_stage_model": {"decoder": ["conv_in",
                                   "conv_out",
                                   "mid",
                                   "norm_out",
                                   "up"],
                       "encoder": ["conv_in",
                                   "conv_out",
                                   "down",
                                   "mid",
                                   "norm_out"],
                       "post_quant_conv": ["bias", "weight"],
                       "quant_conv": ["bias", "weight"]},
  "lora_te_text_model_encoder_layers_0_mlp_fc1": {"lora_down": ["weight"],
                                                 "lora_te_text_model_encoder_layers_0_mlp_fc1": ["alpha"],
                                                 "lora_up": ["weight"]},
  "lora_te_text_model_encoder_layers_0_mlp_fc2": {"lora_down": ["weight"],
                                                 "lora_te_text_model_encoder_layers_0_mlp_fc2": ["alpha"],
                                                 "lora_up": ["weight"]},
  "lora_te_text_model_encoder_layers_0_self_attn_k_proj": {"lora_down": ["weight"],
                                                          "lora_te_text_model_encoder_layers_0_self_attn_k_proj": ["alpha"],
                                                          "lora_up": ["weight"]},
  ...
  "lora_te_text_model_encoder_layers_9_self_attn_v_proj": {"lora_down": ["weight"],
                                                          "lora_te_text_model_encoder_layers_9_self_attn_v_proj": ["alpha"],
                                                          "lora_up": ["weight"]},
  "lora_unet_down_blocks_0_attentions_0_proj_in": {"lora_down": ["weight"],
                                                  "lora_unet_down_blocks_0_attentions_0_proj_in": ["alpha"],
                                                  "lora_up": ["weight"]},
 ...
  "lora_unet_down_blocks_2_attentions_1_transformer_blocks_0_ff_net_2": {"lora_down": ["weight"],
                                                                        "lora_unet_down_blocks_2_attentions_1_transformer_blocks_0_ff_net_2": ["alpha"],
                                                                        "lora_up": ["weight"]},
  "lora_unet_mid_block_attentions_0_proj_in": {"lora_down": ["weight"],
                                              "lora_unet_mid_block_attentions_0_proj_in": ["alpha"],
                                              "lora_up": ["weight"]},
 ...
  "lora_unet_mid_block_attentions_0_transformer_blocks_0_ff_net_2": {"lora_down": ["weight"],
                                                                    "lora_unet_mid_block_attentions_0_transformer_blocks_0_ff_net_2": ["alpha"],
                                                                    "lora_up": ["weight"]},
  "lora_unet_up_blocks_1_attentions_0_proj_in": {"lora_down": ["weight"],
                                                "lora_unet_up_blocks_1_attentions_0_proj_in": ["alpha"],
                                                "lora_up": ["weight"]},
 ...
  "lora_unet_up_blocks_3_attentions_2_transformer_blocks_0_ff_net_2": {"lora_down": ["weight"],
                                                                      "lora_unet_up_blocks_3_attentions_2_transformer_blocks_0_ff_net_2": ["alpha"],
                                                                      "lora_up": ["weight"]},
  "model": {
    "diffusion_model": [
      "input_blocks",
      "middle_block",
      "out",
      "output_blocks",
      "time_embed"
   ]
 },
  "model_ema": {"model_ema": ["decay", "num_updates"]}
}
```

```json [MoXinV1(LORA)]
{
  "cond_stage_model": {"transformer": ["text_model"]},
  "embedding_manager": {"embedder": ["transformer"]},
  "first_stage_model": {"decoder": ["conv_in",
                                   "conv_out",
                                   "mid",
                                   "norm_out",
                                   "up"],
                       "encoder": ["conv_in",
                                   "conv_out",
                                   "down",
                                   "mid",
                                   "norm_out"],
                       "post_quant_conv": ["bias", "weight"],
                       "quant_conv": ["bias", "weight"]},
  "lora_te_text_model_encoder_layers_0_mlp_fc1": {"lora_down": ["weight"],
                                                 "lora_te_text_model_encoder_layers_0_mlp_fc1": ["alpha"],
                                                 "lora_up": ["weight"]},
  "lora_te_text_model_encoder_layers_0_mlp_fc2": {"lora_down": ["weight"],
                                                 "lora_te_text_model_encoder_layers_0_mlp_fc2": ["alpha"],
                                                 "lora_up": ["weight"]},
  "lora_te_text_model_encoder_layers_0_self_attn_k_proj": {"lora_down": ["weight"],
                                                          "lora_te_text_model_encoder_layers_0_self_attn_k_proj": ["alpha"],
                                                          "lora_up": ["weight"]},
  ...
  "lora_te_text_model_encoder_layers_9_self_attn_v_proj": {"lora_down": ["weight"],
                                                          "lora_te_text_model_encoder_layers_9_self_attn_v_proj": ["alpha"],
                                                          "lora_up": ["weight"]},
  "lora_unet_down_blocks_0_attentions_0_proj_in": {"lora_down": ["weight"],
                                                  "lora_unet_down_blocks_0_attentions_0_proj_in": ["alpha"],
                                                  "lora_up": ["weight"]},
 ...
  "lora_unet_down_blocks_2_attentions_1_transformer_blocks_0_ff_net_2": {"lora_down": ["weight"],
                                                                        "lora_unet_down_blocks_2_attentions_1_transformer_blocks_0_ff_net_2": ["alpha"],
                                                                        "lora_up": ["weight"]},
  "lora_unet_mid_block_attentions_0_proj_in": {"lora_down": ["weight"],
                                              "lora_unet_mid_block_attentions_0_proj_in": ["alpha"],
                                              "lora_up": ["weight"]},
 ...
  "lora_unet_mid_block_attentions_0_transformer_blocks_0_ff_net_2": {"lora_down": ["weight"],
                                                                    "lora_unet_mid_block_attentions_0_transformer_blocks_0_ff_net_2": ["alpha"],
                                                                    "lora_up": ["weight"]},
  "lora_unet_up_blocks_1_attentions_0_proj_in": {"lora_down": ["weight"],
                                                "lora_unet_up_blocks_1_attentions_0_proj_in": ["alpha"],
                                                "lora_up": ["weight"]},
 ...,
  "lora_unet_up_blocks_3_attentions_2_transformer_blocks_0_ff_net_2": {"lora_down": ["weight"],
                                                                      "lora_unet_up_blocks_3_attentions_2_transformer_blocks_0_ff_net_2": ["alpha"],
                                                                      "lora_up": ["weight"]},
  "model": {
    "diffusion_model": [
      "input_blocks",
      "middle_block",
      "out",
      "output_blocks",
      "time_embed"
    ]
  },
  "model_ema": {"model_ema": ["decay", "num_updates"]}
}
```

```json [sd3_medium]
{
  "first_stage_model": {
    "decoder": [
      "conv_in",
      "conv_out",
      "mid",
      "norm_out",
      "up"
    ],
    "encoder": [
      "conv_in",
      "conv_out",
      "down",
      "mid",
      "norm_out"]
  },
  "model": {
    "diffusion_model": [
      "context_embedder",
      "final_layer",
      "joint_blocks",
      "pos_embed",
      "t_embedder",
      "x_embedder",
      "y_embedder"
    ]
  }
}
```
:::

