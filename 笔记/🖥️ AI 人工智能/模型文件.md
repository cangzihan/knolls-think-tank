---
tags:
  - Ê∑±Â∫¶Â≠¶‰π†
---

# Ê®°ÂûãÊñá‰ª∂

## Safetensors
https://huggingface.co/docs/safetensors/index

Safetensors is a new simple format for storing tensors safely (as opposed to pickle) and that is still fast (zero-copy).
Safetensors is really fast üöÄ.

### Installation
```shell
pip install safetensors
```

### Usage

#### Save
```python
import torch
from safetensors.torch import save_file

tensors = {
    "embedding": torch.zeros((2, 2)),
    "attention": torch.zeros((2, 3))
}
save_file(tensors, "model.safetensors")
```

#### Load
```python
from safetensors import safe_open

tensors = {}
with safe_open("model.safetensors", framework="pt", device=0) as f:
    for k in f.keys():
        tensors[k] = f.get_tensor(k)
```

‰∏Ä‰∏™ÊâìÂç∞Ê®°ÂûãÂÜÖÂÆπÁöÑÁ®ãÂ∫èÔºö
```python
import pprint
from safetensors import safe_open

saved_keys = []
def print_tree(lst, depth=3):
  for key in lst:
    parts = key.split('.')
    if parts[:depth] not in saved_keys:
      saved_keys.append(parts[:depth])
   #   for i in range(min(depth, len(parts))):
   #     print('  ' * i + parts[i])

  tree = {}
  for key in saved_keys:
    if key[0] not in tree:
      tree[key[0]] = {}
    current = tree[key[0]]
    for d in range(1, len(key))[:-2]:
      if key[d] not in current:
        current[key[d]] = {}
      current = current[key[d]]
    if key[-2] not in current:
      current[key[-2]] = [key[-1]]
    else:
      current[key[-2]].append(key[-1])

  pprint.pprint(tree)
  return tree


model_path = "anypastelAnythingV45_anypastelAnythingV45.safetensors"
tensors_anything = {}
with safe_open(model_path, framework="pt", device=0) as f:
    for k in f.keys():
        tensors_anything[k] = f.get_tensor(k)
tree_dict = print_tree(tensors_anything.keys())
```

ËøêË°åÁªìÊûúÔºö
::: code-group
```json [anypastelAnythingV45 / GuoFeng3_Fix]
{
  "cond_stage_model": {"transformer": ["text_model"]},
  "first_stage_model": {
    "decoder": [
      "conv_in",
      "conv_out",
      "mid",
      "norm_out",
      "up"
    ],
    "encoder": [
      "conv_in",
      "conv_out",
      "down",
      "mid",
      "norm_out"
    ],
    "post_quant_conv": ["bias", "weight"],
    "quant_conv": ["bias", "weight"]
  },
  "model": {
    "diffusion_model": [
      "input_blocks",
      "middle_block",
      "out",
      "output_blocks",
      "time_embed"
    ]
  }
}
```

```json [chilloutmix_NiPrunedFp32Fix]
{
  "cond_stage_model": {"transformer": ["text_model"]},
  "first_stage_model": {
    "decoder": [
      "conv_in",
      "conv_out",
      "mid",
      "norm_out",
      "up"
    ],
    "encoder": [
      "conv_in",
      "conv_out",
      "down",
      "mid",
      "norm_out"
    ],
    "post_quant_conv": ["bias", "weight"],
    "quant_conv": ["bias", "weight"]
  },
  "model": {
    "diffusion_model": [
      "input_blocks",
      "middle_block",
      "out",
      "output_blocks",
      "time_embed"
    ]
  },
  "model_ema": {"model_ema": ["decay", "num_updates"]}
}
```

```json [koreanDollLikeness_v10(LORA)]
{
  "cond_stage_model": {"transformer": ["text_model"]},
  "embedding_manager": {"embedder": ["transformer"]},
  "first_stage_model": {"decoder": ["conv_in",
                                   "conv_out",
                                   "mid",
                                   "norm_out",
                                   "up"],
                       "encoder": ["conv_in",
                                   "conv_out",
                                   "down",
                                   "mid",
                                   "norm_out"],
                       "post_quant_conv": ["bias", "weight"],
                       "quant_conv": ["bias", "weight"]},
  "lora_te_text_model_encoder_layers_0_mlp_fc1": {"lora_down": ["weight"],
                                                 "lora_te_text_model_encoder_layers_0_mlp_fc1": ["alpha"],
                                                 "lora_up": ["weight"]},
  "lora_te_text_model_encoder_layers_0_mlp_fc2": {"lora_down": ["weight"],
                                                 "lora_te_text_model_encoder_layers_0_mlp_fc2": ["alpha"],
                                                 "lora_up": ["weight"]},
  "lora_te_text_model_encoder_layers_0_self_attn_k_proj": {"lora_down": ["weight"],
                                                          "lora_te_text_model_encoder_layers_0_self_attn_k_proj": ["alpha"],
                                                          "lora_up": ["weight"]},
  ...
  "lora_te_text_model_encoder_layers_9_self_attn_v_proj": {"lora_down": ["weight"],
                                                          "lora_te_text_model_encoder_layers_9_self_attn_v_proj": ["alpha"],
                                                          "lora_up": ["weight"]},
  "lora_unet_down_blocks_0_attentions_0_proj_in": {"lora_down": ["weight"],
                                                  "lora_unet_down_blocks_0_attentions_0_proj_in": ["alpha"],
                                                  "lora_up": ["weight"]},
 ...
  "lora_unet_down_blocks_2_attentions_1_transformer_blocks_0_ff_net_2": {"lora_down": ["weight"],
                                                                        "lora_unet_down_blocks_2_attentions_1_transformer_blocks_0_ff_net_2": ["alpha"],
                                                                        "lora_up": ["weight"]},
  "lora_unet_mid_block_attentions_0_proj_in": {"lora_down": ["weight"],
                                              "lora_unet_mid_block_attentions_0_proj_in": ["alpha"],
                                              "lora_up": ["weight"]},
 ...
  "lora_unet_mid_block_attentions_0_transformer_blocks_0_ff_net_2": {"lora_down": ["weight"],
                                                                    "lora_unet_mid_block_attentions_0_transformer_blocks_0_ff_net_2": ["alpha"],
                                                                    "lora_up": ["weight"]},
  "lora_unet_up_blocks_1_attentions_0_proj_in": {"lora_down": ["weight"],
                                                "lora_unet_up_blocks_1_attentions_0_proj_in": ["alpha"],
                                                "lora_up": ["weight"]},
 ...
  "lora_unet_up_blocks_3_attentions_2_transformer_blocks_0_ff_net_2": {"lora_down": ["weight"],
                                                                      "lora_unet_up_blocks_3_attentions_2_transformer_blocks_0_ff_net_2": ["alpha"],
                                                                      "lora_up": ["weight"]},
  "model": {
    "diffusion_model": [
      "input_blocks",
      "middle_block",
      "out",
      "output_blocks",
      "time_embed"
   ]
 },
  "model_ema": {"model_ema": ["decay", "num_updates"]}
}
```

```json [MoXinV1(LORA)]
{
  "cond_stage_model": {"transformer": ["text_model"]},
  "embedding_manager": {"embedder": ["transformer"]},
  "first_stage_model": {"decoder": ["conv_in",
                                   "conv_out",
                                   "mid",
                                   "norm_out",
                                   "up"],
                       "encoder": ["conv_in",
                                   "conv_out",
                                   "down",
                                   "mid",
                                   "norm_out"],
                       "post_quant_conv": ["bias", "weight"],
                       "quant_conv": ["bias", "weight"]},
  "lora_te_text_model_encoder_layers_0_mlp_fc1": {"lora_down": ["weight"],
                                                 "lora_te_text_model_encoder_layers_0_mlp_fc1": ["alpha"],
                                                 "lora_up": ["weight"]},
  "lora_te_text_model_encoder_layers_0_mlp_fc2": {"lora_down": ["weight"],
                                                 "lora_te_text_model_encoder_layers_0_mlp_fc2": ["alpha"],
                                                 "lora_up": ["weight"]},
  "lora_te_text_model_encoder_layers_0_self_attn_k_proj": {"lora_down": ["weight"],
                                                          "lora_te_text_model_encoder_layers_0_self_attn_k_proj": ["alpha"],
                                                          "lora_up": ["weight"]},
  ...
  "lora_te_text_model_encoder_layers_9_self_attn_v_proj": {"lora_down": ["weight"],
                                                          "lora_te_text_model_encoder_layers_9_self_attn_v_proj": ["alpha"],
                                                          "lora_up": ["weight"]},
  "lora_unet_down_blocks_0_attentions_0_proj_in": {"lora_down": ["weight"],
                                                  "lora_unet_down_blocks_0_attentions_0_proj_in": ["alpha"],
                                                  "lora_up": ["weight"]},
 ...
  "lora_unet_down_blocks_2_attentions_1_transformer_blocks_0_ff_net_2": {"lora_down": ["weight"],
                                                                        "lora_unet_down_blocks_2_attentions_1_transformer_blocks_0_ff_net_2": ["alpha"],
                                                                        "lora_up": ["weight"]},
  "lora_unet_mid_block_attentions_0_proj_in": {"lora_down": ["weight"],
                                              "lora_unet_mid_block_attentions_0_proj_in": ["alpha"],
                                              "lora_up": ["weight"]},
 ...
  "lora_unet_mid_block_attentions_0_transformer_blocks_0_ff_net_2": {"lora_down": ["weight"],
                                                                    "lora_unet_mid_block_attentions_0_transformer_blocks_0_ff_net_2": ["alpha"],
                                                                    "lora_up": ["weight"]},
  "lora_unet_up_blocks_1_attentions_0_proj_in": {"lora_down": ["weight"],
                                                "lora_unet_up_blocks_1_attentions_0_proj_in": ["alpha"],
                                                "lora_up": ["weight"]},
 ...,
  "lora_unet_up_blocks_3_attentions_2_transformer_blocks_0_ff_net_2": {"lora_down": ["weight"],
                                                                      "lora_unet_up_blocks_3_attentions_2_transformer_blocks_0_ff_net_2": ["alpha"],
                                                                      "lora_up": ["weight"]},
  "model": {
    "diffusion_model": [
      "input_blocks",
      "middle_block",
      "out",
      "output_blocks",
      "time_embed"
    ]
  },
  "model_ema": {"model_ema": ["decay", "num_updates"]}
}
```

```json [sd3_medium]
{
  "first_stage_model": {
    "decoder": [
      "conv_in",
      "conv_out",
      "mid",
      "norm_out",
      "up"
    ],
    "encoder": [
      "conv_in",
      "conv_out",
      "down",
      "mid",
      "norm_out"]
  },
  "model": {
    "diffusion_model": [
      "context_embedder",
      "final_layer",
      "joint_blocks",
      "pos_embed",
      "t_embedder",
      "x_embedder",
      "y_embedder"
    ]
  }
}
```
:::

