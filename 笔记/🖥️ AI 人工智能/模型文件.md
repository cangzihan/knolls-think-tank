---
tags:
  - æ·±åº¦å­¦ä¹ 
---

# æ¨¡å‹æ–‡ä»¶

## GGML
GGMLï¼ˆGeorgi Gerganov Machine Learningï¼‰ æ˜¯ä¸€ç§è½»é‡çº§çš„æœºå™¨å­¦ä¹ åº“å’Œæ¨¡å‹æ ¼å¼ï¼Œç”± Georgi Gerganov å¼€å‘ï¼Œä¸»è¦ç”¨äºé«˜æ•ˆçš„ CPU æ¨ç†ã€‚å®ƒæœ€æ—©ç”¨äº LLaMA.cppï¼Œåæ¥ä¹Ÿè¢«è®¸å¤šæœ¬åœ° LLM æ¨ç†å·¥å…·ï¼ˆå¦‚ llama.cpp å’Œ gpt4allï¼‰é‡‡ç”¨ã€‚

ç‰¹ç‚¹ï¼š
ä¸“ä¸º CPU æ¨ç†ä¼˜åŒ–ï¼Œé€‚ç”¨äº AVXã€AVX2ã€AVX-512 ä»¥åŠ Apple Siliconï¼ˆM1/M2ï¼‰ã€‚
æ”¯æŒé‡åŒ–ï¼ˆQuantizationï¼‰ï¼Œå¯ä»¥é™ä½ LLM è¿è¡Œæ—¶çš„æ˜¾å­˜/å†…å­˜éœ€æ±‚ï¼Œæ¯”å¦‚ 4-bitï¼ˆQ4_0ã€Q4_Kï¼‰ã€5-bitã€8-bit ç­‰ã€‚
å¹¿æ³›å…¼å®¹æ€§ï¼Œæ”¯æŒä¸åŒçš„ LLMï¼Œå¦‚ LLaMAã€Mistralã€Falcon ç­‰ã€‚
å­˜å‚¨æ–¹å¼ï¼šäºŒè¿›åˆ¶æ ¼å¼ï¼Œæ”¯æŒ ggml åº“ç›´æ¥åŠ è½½ã€‚

## GGUF
GGUFï¼ˆGPTQ for GGML Unified Formatï¼‰ æ˜¯ GGML çš„è¿›åŒ–ç‰ˆï¼Œä¸“é—¨ä¸º æ›´é«˜æ•ˆçš„ LLM å­˜å‚¨å’Œæ¨ç† è®¾è®¡ï¼Œå–ä»£äº†æ—§çš„ GGML æ ¼å¼ã€‚

ç‰¹ç‚¹ï¼š
æ›´çµæ´»çš„æ ¼å¼ï¼Œæ”¯æŒæ›´å¤šå…ƒæ•°æ®ï¼ˆå¦‚ tokenizer ä¿¡æ¯ã€ç‰¹å®šå‚æ•°ç­‰ï¼‰ï¼Œé¿å…é¢å¤–çš„è½¬æ¢éœ€æ±‚ã€‚
æå‡æ€§èƒ½ï¼Œæ”¹è¿›äº†é‡åŒ–æ–¹å¼ï¼Œé€‚ç”¨äºæ›´å¤š LLM å˜ä½“ï¼Œå¦‚ LLaMA 2/3ã€Mistralã€Gemma ç­‰ã€‚
æ”¯æŒ GPU åŠ é€Ÿï¼Œç›¸æ¯” GGMLï¼Œåœ¨ OpenCLã€Metalã€CUDA ä¸Šçš„æ€§èƒ½æ›´å¥½ã€‚
å®˜æ–¹æ”¯æŒï¼šä¸»è¦ç”¨äº llama.cpp ç­‰é¡¹ç›®ï¼Œå¹¶é€æ¸æˆä¸ºæ ‡å‡†æ ¼å¼ã€‚

`qwen2.5-coder-7b-instruct-q6_k.gguf`çš„å«ä¹‰ï¼š

- qwenï¼ˆåƒé—®ï¼‰æ˜¯é˜¿é‡Œäº‘ï¼ˆAlibaba Cloudï¼‰æ¨å‡ºçš„å¤§è¯­è¨€æ¨¡å‹ç³»åˆ—çš„åç§°ã€‚
- 2.5 ä»£è¡¨ Qwen 2.5 ç‰ˆæœ¬ï¼Œè¿™æ˜¯è¯¥æ¨¡å‹çš„ç¬¬äºŒä»£åŠæ›´æ–°ç‰ˆï¼Œé€šå¸¸æ¯” 2.0 å…·æœ‰æ›´å¥½çš„æ€§èƒ½å’Œä¼˜åŒ–ã€‚
- coder è¡¨ç¤ºè¯¥æ¨¡å‹ä¸“é—¨é’ˆå¯¹ ä»£ç ç”Ÿæˆå’Œç†è§£ è¿›è¡Œäº†ä¼˜åŒ–ï¼Œç›¸æ¯”äºé€šç”¨è¯­è¨€æ¨¡å‹ï¼Œå®ƒåœ¨ç¼–ç¨‹ä»»åŠ¡ä¸Šè¡¨ç°æ›´ä½³ã€‚
- ä»£è¡¨ æ¨¡å‹çš„å‚æ•°é‡ï¼Œ7B å³ 7 Billionï¼ˆ70 äº¿ï¼‰ å‚æ•°ã€‚å‚æ•°è¶Šå¤šï¼Œæ¨¡å‹é€šå¸¸åœ¨æ¨ç†èƒ½åŠ›ä¸Šæ›´å¼ºï¼Œä½†åŒæ—¶ä¹Ÿéœ€è¦æ›´å¤šè®¡ç®—èµ„æºã€‚
- instructè¯´æ˜è¯¥æ¨¡å‹æ˜¯ "Instruct"ï¼ˆæŒ‡ä»¤å¾®è°ƒï¼‰ç‰ˆæœ¬ã€‚ è¿™æ„å‘³ç€å®ƒä¸“é—¨ç»è¿‡æŒ‡ä»¤å¾®è°ƒï¼ˆInstruction tuningï¼‰ï¼Œå¯ä»¥æ›´å¥½åœ°ç†è§£å’Œå“åº”æŒ‡ä»¤ï¼Œä¾‹å¦‚ä»£ç è§£é‡Šã€ç”Ÿæˆç­‰ä»»åŠ¡ã€‚
- q6_k: Q6_K ä»£è¡¨ é‡åŒ–ï¼ˆQuantizationï¼‰æ–¹æ³•ï¼Œç”¨äºå‡å°‘æ¨¡å‹å¤§å°ã€é™ä½è¿è¡Œå†…å­˜æ¶ˆè€—ï¼ŒåŒæ—¶å°½é‡ä¿ç•™æ¨ç†æ€§èƒ½ã€‚ Q6_K æ˜¯ GGUF é‡åŒ–æ ¼å¼ ä¸­çš„ä¸€ç§æ–¹æ¡ˆï¼Œé€šå¸¸è¡¨ç¤º 6-bit é‡åŒ–ï¼ˆç›¸è¾ƒäº 16-bit FP16 æˆ– 32-bit FP32ï¼Œé™ä½äº†å­˜å‚¨éœ€æ±‚ï¼‰ã€‚ Q6_K ç›¸æ¯” Q4 æˆ– Q5 é‡åŒ–ï¼Œä¿ç•™äº†æ›´å¤šç²¾åº¦ï¼Œä½†ä»æ¯” Q8 æ›´çœæ˜¾å­˜ã€‚

### å¦‚ä½•ä½¿ç”¨GGUFæ–‡ä»¶
1. ä½¿ç”¨[transformersåº“](https://github.com/huggingface/transformers/blob/main/docs/source/zh/gguf.md)åŠ è½½æ¨¡å‹ã€‚
```python
from transformers import AutoTokenizer, AutoModelForCausalLM

model_id = "TheBloke/TinyLlama-1.1B-Chat-v1.0-GGUF"
filename = "tinyllama-1.1b-chat-v1.0.Q6_K.gguf"

tokenizer = AutoTokenizer.from_pretrained(model_id, gguf_file=filename)
model = AutoModelForCausalLM.from_pretrained(model_id, gguf_file=filename)
```
2. LLM Farmç§»åŠ¨ç«¯éƒ¨ç½²ï¼ˆé€‚ç”¨äºIOSã€iPad OSï¼‰ï¼šç›´æ¥å°†å•ä¸ªGGUFæ–‡ä»¶åœ¨LLM Farm APPä¸­å¯¼å…¥
3. ä½¿ç”¨Ollama
   1. å°†GGUFæ–‡ä»¶æ”¾å…¥ç‰¹å®šè·¯å¾„å†…
      - Windowsç³»ç»Ÿï¼šæŠŠæ¨¡å‹æ”¾åœ¨`C:\Users\ã€ç”¨æˆ·åã€‘\.ollama\models`ä¸‹
   2. åˆ›å»ºä¸€ä¸ªæ–‡ä»¶å‘½åä¸º`Modelfile`å†…å®¹å‚è€ƒ`FROM ./Tifa-DeepsexV2-7b-Q4_KM.gguf`
   3. åœ¨æ–‡ä»¶å¤¹å†…å¯åŠ¨ç»ˆç«¯ï¼Œ`ollama create deepsex2`
   4. å¯¼å…¥æˆåŠŸåæŸ¥çœ‹`ollama list`
   5. Test the model`ollama run deepsex2`

## Safetensors
https://huggingface.co/docs/safetensors/index

Safetensors is a new simple format for storing tensors safely (as opposed to pickle) and that is still fast (zero-copy).
Safetensors is really fast ğŸš€.

### Installation
```shell
pip install safetensors
```

### Usage

#### Save
```python
import torch
from safetensors.torch import save_file

tensors = {
    "embedding": torch.zeros((2, 2)),
    "attention": torch.zeros((2, 3))
}
save_file(tensors, "model.safetensors")
```

#### Load
```python
from safetensors import safe_open

tensors = {}
with safe_open("model.safetensors", framework="pt", device=0) as f:
    for k in f.keys():
        tensors[k] = f.get_tensor(k)
```

ä¸€ä¸ªæ‰“å°æ¨¡å‹å†…å®¹çš„ç¨‹åºï¼š
```python
import pprint
from safetensors import safe_open

saved_keys = []
def print_tree(lst, depth=3):
  for key in lst:
    parts = key.split('.')
    if parts[:depth] not in saved_keys:
      saved_keys.append(parts[:depth])
   #   for i in range(min(depth, len(parts))):
   #     print('  ' * i + parts[i])

  tree = {}
  for key in saved_keys:
    if key[0] not in tree:
      tree[key[0]] = {}
    current = tree[key[0]]
    for d in range(1, len(key))[:-2]:
      if key[d] not in current:
        current[key[d]] = {}
      current = current[key[d]]
    if key[-2] not in current:
      current[key[-2]] = [key[-1]]
    else:
      current[key[-2]].append(key[-1])

  pprint.pprint(tree)
  return tree


model_path = "anypastelAnythingV45_anypastelAnythingV45.safetensors"
tensors_anything = {}
with safe_open(model_path, framework="pt", device=0) as f:
    for k in f.keys():
        tensors_anything[k] = f.get_tensor(k)
tree_dict = print_tree(tensors_anything.keys())
```

è¿è¡Œç»“æœï¼š
::: code-group
```json [anypastelAnythingV45 / GuoFeng3_Fix]
{
  "cond_stage_model": {"transformer": ["text_model"]},
  "first_stage_model": {
    "decoder": [
      "conv_in",
      "conv_out",
      "mid",
      "norm_out",
      "up"
    ],
    "encoder": [
      "conv_in",
      "conv_out",
      "down",
      "mid",
      "norm_out"
    ],
    "post_quant_conv": ["bias", "weight"],
    "quant_conv": ["bias", "weight"]
  },
  "model": {
    "diffusion_model": [
      "input_blocks",
      "middle_block",
      "out",
      "output_blocks",
      "time_embed"
    ]
  }
}
```

```json [chilloutmix_NiPrunedFp32Fix]
{
  "cond_stage_model": {"transformer": ["text_model"]},
  "first_stage_model": {
    "decoder": [
      "conv_in",
      "conv_out",
      "mid",
      "norm_out",
      "up"
    ],
    "encoder": [
      "conv_in",
      "conv_out",
      "down",
      "mid",
      "norm_out"
    ],
    "post_quant_conv": ["bias", "weight"],
    "quant_conv": ["bias", "weight"]
  },
  "model": {
    "diffusion_model": [
      "input_blocks",
      "middle_block",
      "out",
      "output_blocks",
      "time_embed"
    ]
  },
  "model_ema": {"model_ema": ["decay", "num_updates"]}
}
```

```json [koreanDollLikeness_v10(LORA)]
{
  "cond_stage_model": {"transformer": ["text_model"]},
  "embedding_manager": {"embedder": ["transformer"]},
  "first_stage_model": {"decoder": ["conv_in",
                                   "conv_out",
                                   "mid",
                                   "norm_out",
                                   "up"],
                       "encoder": ["conv_in",
                                   "conv_out",
                                   "down",
                                   "mid",
                                   "norm_out"],
                       "post_quant_conv": ["bias", "weight"],
                       "quant_conv": ["bias", "weight"]},
  "lora_te_text_model_encoder_layers_0_mlp_fc1": {"lora_down": ["weight"],
                                                 "lora_te_text_model_encoder_layers_0_mlp_fc1": ["alpha"],
                                                 "lora_up": ["weight"]},
  "lora_te_text_model_encoder_layers_0_mlp_fc2": {"lora_down": ["weight"],
                                                 "lora_te_text_model_encoder_layers_0_mlp_fc2": ["alpha"],
                                                 "lora_up": ["weight"]},
  "lora_te_text_model_encoder_layers_0_self_attn_k_proj": {"lora_down": ["weight"],
                                                          "lora_te_text_model_encoder_layers_0_self_attn_k_proj": ["alpha"],
                                                          "lora_up": ["weight"]},
  ...
  "lora_te_text_model_encoder_layers_9_self_attn_v_proj": {"lora_down": ["weight"],
                                                          "lora_te_text_model_encoder_layers_9_self_attn_v_proj": ["alpha"],
                                                          "lora_up": ["weight"]},
  "lora_unet_down_blocks_0_attentions_0_proj_in": {"lora_down": ["weight"],
                                                  "lora_unet_down_blocks_0_attentions_0_proj_in": ["alpha"],
                                                  "lora_up": ["weight"]},
 ...
  "lora_unet_down_blocks_2_attentions_1_transformer_blocks_0_ff_net_2": {"lora_down": ["weight"],
                                                                        "lora_unet_down_blocks_2_attentions_1_transformer_blocks_0_ff_net_2": ["alpha"],
                                                                        "lora_up": ["weight"]},
  "lora_unet_mid_block_attentions_0_proj_in": {"lora_down": ["weight"],
                                              "lora_unet_mid_block_attentions_0_proj_in": ["alpha"],
                                              "lora_up": ["weight"]},
 ...
  "lora_unet_mid_block_attentions_0_transformer_blocks_0_ff_net_2": {"lora_down": ["weight"],
                                                                    "lora_unet_mid_block_attentions_0_transformer_blocks_0_ff_net_2": ["alpha"],
                                                                    "lora_up": ["weight"]},
  "lora_unet_up_blocks_1_attentions_0_proj_in": {"lora_down": ["weight"],
                                                "lora_unet_up_blocks_1_attentions_0_proj_in": ["alpha"],
                                                "lora_up": ["weight"]},
 ...
  "lora_unet_up_blocks_3_attentions_2_transformer_blocks_0_ff_net_2": {"lora_down": ["weight"],
                                                                      "lora_unet_up_blocks_3_attentions_2_transformer_blocks_0_ff_net_2": ["alpha"],
                                                                      "lora_up": ["weight"]},
  "model": {
    "diffusion_model": [
      "input_blocks",
      "middle_block",
      "out",
      "output_blocks",
      "time_embed"
   ]
 },
  "model_ema": {"model_ema": ["decay", "num_updates"]}
}
```

```json [MoXinV1(LORA)]
{
  "cond_stage_model": {"transformer": ["text_model"]},
  "embedding_manager": {"embedder": ["transformer"]},
  "first_stage_model": {"decoder": ["conv_in",
                                   "conv_out",
                                   "mid",
                                   "norm_out",
                                   "up"],
                       "encoder": ["conv_in",
                                   "conv_out",
                                   "down",
                                   "mid",
                                   "norm_out"],
                       "post_quant_conv": ["bias", "weight"],
                       "quant_conv": ["bias", "weight"]},
  "lora_te_text_model_encoder_layers_0_mlp_fc1": {"lora_down": ["weight"],
                                                 "lora_te_text_model_encoder_layers_0_mlp_fc1": ["alpha"],
                                                 "lora_up": ["weight"]},
  "lora_te_text_model_encoder_layers_0_mlp_fc2": {"lora_down": ["weight"],
                                                 "lora_te_text_model_encoder_layers_0_mlp_fc2": ["alpha"],
                                                 "lora_up": ["weight"]},
  "lora_te_text_model_encoder_layers_0_self_attn_k_proj": {"lora_down": ["weight"],
                                                          "lora_te_text_model_encoder_layers_0_self_attn_k_proj": ["alpha"],
                                                          "lora_up": ["weight"]},
  ...
  "lora_te_text_model_encoder_layers_9_self_attn_v_proj": {"lora_down": ["weight"],
                                                          "lora_te_text_model_encoder_layers_9_self_attn_v_proj": ["alpha"],
                                                          "lora_up": ["weight"]},
  "lora_unet_down_blocks_0_attentions_0_proj_in": {"lora_down": ["weight"],
                                                  "lora_unet_down_blocks_0_attentions_0_proj_in": ["alpha"],
                                                  "lora_up": ["weight"]},
 ...
  "lora_unet_down_blocks_2_attentions_1_transformer_blocks_0_ff_net_2": {"lora_down": ["weight"],
                                                                        "lora_unet_down_blocks_2_attentions_1_transformer_blocks_0_ff_net_2": ["alpha"],
                                                                        "lora_up": ["weight"]},
  "lora_unet_mid_block_attentions_0_proj_in": {"lora_down": ["weight"],
                                              "lora_unet_mid_block_attentions_0_proj_in": ["alpha"],
                                              "lora_up": ["weight"]},
 ...
  "lora_unet_mid_block_attentions_0_transformer_blocks_0_ff_net_2": {"lora_down": ["weight"],
                                                                    "lora_unet_mid_block_attentions_0_transformer_blocks_0_ff_net_2": ["alpha"],
                                                                    "lora_up": ["weight"]},
  "lora_unet_up_blocks_1_attentions_0_proj_in": {"lora_down": ["weight"],
                                                "lora_unet_up_blocks_1_attentions_0_proj_in": ["alpha"],
                                                "lora_up": ["weight"]},
 ...,
  "lora_unet_up_blocks_3_attentions_2_transformer_blocks_0_ff_net_2": {"lora_down": ["weight"],
                                                                      "lora_unet_up_blocks_3_attentions_2_transformer_blocks_0_ff_net_2": ["alpha"],
                                                                      "lora_up": ["weight"]},
  "model": {
    "diffusion_model": [
      "input_blocks",
      "middle_block",
      "out",
      "output_blocks",
      "time_embed"
    ]
  },
  "model_ema": {"model_ema": ["decay", "num_updates"]}
}
```

```json [sd3_medium]
{
  "first_stage_model": {
    "decoder": [
      "conv_in",
      "conv_out",
      "mid",
      "norm_out",
      "up"
    ],
    "encoder": [
      "conv_in",
      "conv_out",
      "down",
      "mid",
      "norm_out"]
  },
  "model": {
    "diffusion_model": [
      "context_embedder",
      "final_layer",
      "joint_blocks",
      "pos_embed",
      "t_embedder",
      "x_embedder",
      "y_embedder"
    ]
  }
}
```
:::

