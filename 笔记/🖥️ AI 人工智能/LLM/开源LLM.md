---
tags:
  - ChatGLM
  - Baichuan
  - Llama
  - é€šä¹‰åƒé—®
  - LLM
---

# å¼€æºLLM

## å®ç”¨å·¥å…·
ä¸€äº›å¥½ç”¨çš„é¡¹ç›®ï¼š
- https://github.com/mlc-ai/mlc-llm
- https://github.com/wangzhaode/mnn-llm
- https://lmstudio.ai/
- https://github.com/ollama/ollama

### FastChat
[Github](https://github.com/lm-sys/FastChat?tab=readme-ov-file)

FastChat æ˜¯ä¸€ä¸ªå¼ºå¤§çš„å¹³å°ï¼Œå¯ä»¥å¸®åŠ©ä½ è®­ç»ƒã€éƒ¨ç½²å’Œè¯„ä¼°åŸºäºå¤§å‹è¯­è¨€æ¨¡å‹çš„èŠå¤©æœºå™¨äººã€‚å®ƒæä¾›äº†è®­ç»ƒå’Œè¯„ä¼°ä»£ç ï¼Œä»¥åŠåˆ†å¸ƒå¼å¤šæ¨¡å‹æœåŠ¡ç³»ç»Ÿï¼Œä½¿ä½ èƒ½å¤Ÿè½»æ¾åœ°æ„å»ºå’Œç®¡ç†è‡ªå·±çš„èŠå¤©æœºå™¨äººé¡¹ç›®ã€‚

#### Install
```shell
pip3 install "fschat[model_worker,webui]"
```

è§£å†³streamæ¥å£é—®é¢˜

- æ–¹æ³•1

ä¸æ¨èï¼Œä¼šå’Œæ–°ç‰ˆæœ¬gradioå†²çª
```shell
pip install pydantic==1.10.14
```

- æ–¹æ³•2

åœ¨APIçš„è¾“å‡ºä¸­ï¼Œæ‰¾åˆ°æŠ¥é”™çš„openaiè·¯å¾„ï¼Œå¦‚æŠ¥é”™ä¸º
```shell
File "/home/XXX/.conda/envs/py310_really_python_3_10/lib/python3.10/site-packages/fastchat/serve/openai_api_server.py", line 942, in <module>
2024-05-13 10:52:07 | ERROR | stderr |     uvicorn.run(app, host=args.host, port=args.port, log_level="info")
```
å°±ä¿®æ”¹æ–‡ä»¶`/home/XXX/.conda/envs/py310_really_python_3_10/lib/python3.10/site-packages/fastchat/serve/openai_api_server.py`çš„3ä¸ªåœ°æ–¹

`line 506`:
```python
    for i in range(n):
        # First chunk with role
        choice_data = ChatCompletionResponseStreamChoice(
            index=i,
            delta=DeltaMessage(role="assistant"),
            finish_reason=None,
        )
        chunk = ChatCompletionStreamResponse(
            id=id, choices=[choice_data], model=model_name
        )
        yield f"data: {chunk.json(exclude_unset=True, ensure_ascii=False)}\n\n" # [!code --]
        yield f"data: {json.dumps(chunk.dict(exclude_unset=True), ensure_ascii=False)}\n\n" # [!code ++]

        previous_text = ""
```

`line 536`å’Œ`line 539`:
```python
            if delta_text is None:
                if content.get("finish_reason", None) is not None:
                    finish_stream_events.append(chunk)
                continue
            yield f"data: {chunk.json(exclude_unset=True, ensure_ascii=False)}\n\n" # [!code --]
            yield f"data: {json.dumps(chunk.dict(exclude_unset=True), ensure_ascii=False)}\n\n" # [!code ++]
    # There is not "content" field in the last delta message, so exclude_none to exclude field "content".
    for finish_chunk in finish_stream_events:
        yield f"data: {finish_chunk.json(exclude_none=True, ensure_ascii=False)}\n\n" # [!code --]
        yield f"data: {json.dumps(finish_chunk.dict(exclude_none=True), ensure_ascii=False)}\n\n" # [!code ++]
    yield "data: [DONE]\n\n"


@app.post("/v1/completions", dependencies=[Depends(check_api_key)])
```

å®¢æˆ·ç«¯ï¼š
```shell
pip install openai
```

#### éƒ¨ç½²LLM
éœ€è¦åŒæ—¶å¼€å¯å¤šä¸ªç»ˆç«¯
```shell
# Controller
python3 -m fastchat.serve.controller
# Worker
# worker 0
CUDA_VISIBLE_DEVICES=0 python3 -m fastchat.serve.model_worker --model-path lmsys/vicuna-7b-v1.5 --controller http://localhost:21001 --port 31000 --worker http://localhost:31000
# worker 1
CUDA_VISIBLE_DEVICES=1 python3 -m fastchat.serve.model_worker --model-path lmsys/fastchat-t5-3b-v1.0 --controller http://localhost:21001 --port 31001 --worker http://localhost:31001
# åŒå¡æ–¹æ¡ˆå‘½ä»¤ï¼ˆæ§åˆ¶ä¸¤ä¸ªGPUå„å ä¸€åŠæ˜¾å­˜ï¼‰
CUDA_VISIBLE_DEVICES=0,1 python3 -m fastchat.serve.model_worker --model-path cyberagent/calm3-22b-chat --controller http://localhost:21001 --port 31000 --worker http://localhost:31000 --num-gpus=2 --max-gpu-memory 26GiB
# æ˜¾å­˜ä¸å¤Ÿï¼Œéƒ¨ç½²é‡åŒ–ç‰ˆæœ¬
python -m fastchat.serve.model_worker --model-path Qwen/Qwen2.5-7B-Instruct --controller http://localhost:21001 --port 31000 --worker http://localhost:31000 --load-8bit

# Web UI
python3 -m fastchat.serve.gradio_web_server
```

https://rudeigerc.dev/posts/llm-inference-with-fastchat/

### Unsloth
Unslothæ˜¯ä¸€ä¸ªç”¨äºå¾®è°ƒå¤§æ¨¡å‹çš„å·¥å…·

[å®˜ç½‘](https://unsloth.ai/) | [æ–‡æ¡£](https://docs.unsloth.ai/)

### Ollama
[å®˜ç½‘](https://ollama.com/) | [Github](https://github.com/ollama/ollama)

#### å¸¸ç”¨å‘½ä»¤
æŸ¥çœ‹æ¨¡å‹åˆ—è¡¨
```shell
ollama list
```

è¿è¡Œæ¨¡å‹
```shell
ollama run ã€æ¨¡å‹åã€‘
```
nishi
## åŸºæœ¬ç¯å¢ƒæ­å»º

å¾ˆå¤šLLMéœ€è¦çš„ç¯å¢ƒéƒ½æ˜¯ç±»ä¼¼çš„ï¼Œè¿™é‡Œé»˜è®¤åœ¨è¯´ç”µè„‘/æœåŠ¡å™¨ç«¯ã€å‡è®¾å·²ç»è£…å¥½äº†GPUé©±åŠ¨ã€Cudaç­‰åŸºæœ¬ç¯å¢ƒã€‚
```shell
# å®‰è£…Torch
pip3 install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu124

pip install accelerate
pip install transformers
```

## Baichuan
Baichuan 2 æ˜¯ç™¾å·æ™ºèƒ½æ¨å‡ºçš„å¼€æºLLMï¼Œæ‰€æœ‰ç‰ˆæœ¬ä¸ä»…å¯¹å­¦æœ¯ç ”ç©¶å®Œå…¨å¼€æ”¾ï¼Œå¼€å‘è€…ä¹Ÿä»…éœ€é‚®ä»¶ç”³è¯·å¹¶è·å¾—å®˜æ–¹å•†ç”¨è®¸å¯åï¼Œå³å¯ä»¥å…è´¹å•†ç”¨ã€‚

https://www.baichuan-ai.com/home

ç½‘é¡µç«¯å…è´¹[ChatBot](https://www.baichuan-ai.com/chat?from=%2Fhome)

7B [Baichuan2-7B](https://huggingface.co/baichuan-inc/Baichuan2-7B-Chat)

13B [Baichuan2-13B-Chat](https://huggingface.co/baichuan-inc/Baichuan2-13B-Chat)

å…¶ä½™Baichuan2æ¨¡å‹å‡ä¸æ¨è

## ChatGLM
ChatGLM æ˜¯ä¸€ä¸ªå¼€æºçš„ã€æ”¯æŒä¸­è‹±åŒè¯­é—®ç­”çš„å¯¹è¯è¯­è¨€æ¨¡å‹ï¼ŒåŸºäº [General Language Model (GLM)](https://github.com/THUDM/GLM) æ¶æ„ã€‚

ChatGLM 3
https://huggingface.co/THUDM/chatglm3-6b

### RK3588éƒ¨ç½²
å‚è€ƒã€ç¡¬ä»¶ã€‘-ã€åµŒå…¥å¼ã€‘-ã€RK3588ã€‘ä¸­ChatGLMèŠ‚


## Llama

### Llama2
#### 1.ä¸‹è½½
éœ€è¦ä¸‹è½½**hfç‰ˆæœ¬**çš„Llamaï¼Œå¦‚[Llama-2-13b-chat-hf](https://huggingface.co/meta-llama/Llama-2-13b-chat-hf)

è¿™é‡Œä»¥`Llama-2-13b-chat-hf`ä¸ºä¾‹ï¼Œå®é™…ä¸Šä¸éœ€è¦ä¸‹è½½å…¨éƒ¨æ–‡ä»¶ï¼Œå¦‚æœæ—¶é—´ç´§å¯ä»¥ä¸‹è½½å¦‚ä¸‹ï¼š
```
config.json
generation_config.json
gitattributes
LICENSE.txt
model-00001-of-00003.safetensors
model-00002-of-00003.safetensors
model-00003-of-00003.safetensors
model.safetensors.index.json
pytorch_model.bin.index.json
README.md
Responsible-Use-Guide.pdf
special_tokens_map.json
tokenizer_config.json
tokenizer.json
tokenizer.model
USE_POLICY.md
```

#### 2.è°ƒç”¨
å°†æ‰€æœ‰æ–‡ä»¶ä¸‹è½½ä¸‹æ¥æ”¾åˆ°ä¸€ä¸ªæ–‡ä»¶å¤¹é‡Œï¼Œå‘½åä¸º`Llama-2-13b-chat-hf`ï¼Œç„¶ååœ¨æ–‡ä»¶å¤¹åŒçº§ç›®å½•ä¸‹åˆ›å»ºPythonè„šæœ¬
```python
import torch
from transformers import AutoTokenizer, LlamaForCausalLM

model = LlamaForCausalLM.from_pretrained(
        "Llama-2-13b-chat-hf",
        device_map="auto",
        torch_dtype=torch.float16
        )

tokenizer = AutoTokenizer.from_pretrained("Llama-2-13b-chat-hf")

while True:
    prompt = input(">>")
    if prompt == "exit":
        break
    inputs = tokenizer(prompt, return_tensors="pt").to("cuda")
    # Generate
    generate_ids = model.generate(inputs.input_ids, max_length=30)
    ans = tokenizer.batch_decode(generate_ids, skip_special_tokens=True, clean_up_tokenization_spaces=False)[0]
    print(ans)
```

#### 3.éƒ¨ç½²

- æœåŠ¡ç«¯
```shell
pip install protobuf
```

åˆ›å»º`chat.py`å’Œ`app.py`:

::: code-group
```python [chat]                                                                                                                                           23,0-1       é¡¶ç«¯
from transformers import AutoTokenizer
import transformers
import torch

model = "Llama-2-13b-chat-hf"

tokenizer = AutoTokenizer.from_pretrained(model)
pipeline = transformers.pipeline(
    "text-generation",
    model=model,
    torch_dtype=torch.float16,
    device_map="auto",
)

def chat_API(messages):
    if messages[0]['role'] != "system" or len(messages)==1:
        system_msg = "You are AI assistant"
        dialogue = f"<s>[INST] <<SYS>> " \
                   f"{system_msg} " \
                   f"<</SYS>> " \
                f"{messages[0]['content']} [/INST]"
        dialogue += "\n"

        sequences = pipeline(
            f'{dialogue}\n',
            do_sample=True,
            top_k=10,
            num_return_sequences=1,
            eos_token_id=tokenizer.eos_token_id,
            max_length=2048,
        )

        prompt_length = len(dialogue)
        prompt_text = sequences[0]['generated_text'][:prompt_length]
        generated_part = sequences[0]['generated_text'][prompt_length:]
        generated_part = generated_part.strip()

    else:
        system_msg = messages[0]['content']
        dialogue = f"<s>[INST] <<SYS>> " \
                   f"{system_msg} " \
                   f"<</SYS>> " \
                f"{messages[1]['content']} [/INST]"
        dialogue += "\n"

        sequences = pipeline(
            f'{dialogue}\n',
            do_sample=True,
            top_k=10,
            num_return_sequences=1,
            eos_token_id=tokenizer.eos_token_id,
            max_length=2048,
        )

        prompt_length = len(dialogue)
        prompt_text = sequences[0]['generated_text'][:prompt_length]
        generated_part = sequences[0]['generated_text'][prompt_length:]
        generated_part = generated_part.strip()

    return generated_part

message = [
        {'role': "system", 'content': "ä½ æ˜¯1ä¸ªAIåŠ©æ‰‹"},
        {'role': "user", 'content': "å¸®æˆ‘è®°ä½ä¸€ä¸ªè¯ï¼ŒButton"},
        {'role': "assistant", 'content': "å¸®æˆ‘è®°ä½ä¸€ä¸ªè¯ï¼ŒButton"},
        {'role': "user", 'content': "æˆ‘è¦ä½ è®°ä½çš„è¯æ˜¯ä»€ä¹ˆ"}]


message1 = [{'role': "system", 'content': "ä½ æ˜¯1ä¸ªAIåŠ©æ‰‹, è¯·åœ¨æ‰€æœ‰å¯¹è¯ä¹‹å‰è¯´[ä½ å¥½,]"}, {'role': "user", 'content': "What is the captial of Beijing?"}]
rec = chat_API(message1)

print(message1)
print(rec)
```

```python [app]
import eventlet

eventlet.monkey_patch()
from eventlet import wsgi

from flask import Flask, jsonify, request, make_response
from flask_cors import CORS, cross_origin
import torch
from flask_sock import Sock, Server

from chat import chat_API

DEVICE = "cuda"
# DEVICE_IDS = ["0", "1", "2"]
DEVICE_IDS = ["0"]


def torch_gc():
    if torch.cuda.is_available():
        for device_id in DEVICE_IDS:
            cuda_device = f"{DEVICE}:{device_id}"
            with torch.cuda.device(cuda_device):
                torch.cuda.empty_cache()
                torch.cuda.ipc_collect()


app = Flask(__name__)
sock = Sock(app)
app.config['JSON_AS_ASCII'] = False
CORS(app)


def end_sentence(sent):
    if sent and (
        sent.endswith("......") or
        sent.endswith("ã€‚") or
        sent.endswith("ï¼") or
        sent.endswith("ï¼Ÿ")
    ):
        return True
    return False


@sock.route("/chat_stream")
def dialog_stream(ws:Server):
    msg = ws.receive()
    chat_req = eval(msg)
    if 'prompt' not in chat_req or 'userid' not in chat_req or 'faq_type' not in chat_req:
        answer = {
            "errmsg": "è¯·æ±‚å‚æ•°é”™è¯¯ï¼Œex:{'prompt':'hello','userid':'123','faq_type':0}",
        }
        ws.send(answer)
        ws.close()
        return 201

    faq_type = chat_req['faq_type']
    prompt = chat_req['prompt']
    query_faq = None
    if faq_type == 0:
        query_faq = prompt

    userid = chat_req["userid"]
    history = []

    print(history)
    history.append({"role": "user", "content": query_faq})
    history_reponse = ''

    for response in model.chat(tokenizer, history, stream=True):
        if end_sentence(response):
            response = response.replace('\n',"")
            current_response = response.replace(history_reponse, '')
            if current_response:
                history_reponse += current_response
                answear = {
                    "userid": userid,
                    "response": current_response,
                    "status": 0
                }
                print(current_response)
                ws.send(answear)
    torch_gc()
    answear = {
        "userid": userid,
        "response": "end",
        "status": 1
    }
    history = history if len(history)<5 else history[-5:]
    history.append({"role": "user", "content": prompt})
    history.append({'role': 'assisant','content': response})
    print('history-after', history)
    ws.send(answear)
    ws.close()
    return 200


@app.route("/chat", methods=['POST'])
def dialog():
    if request.content_type.startswith('application/json'):
        if 'faq_type' not in request.json:
            resp = {
                "errmsg": "no param 'faq_type'",
                "status": 201
            }
            return resp
    faq_type = request.json['faq_type']
    prompt = request.json['prompt']
    query_faq = None
    if faq_type == 0:
        query_faq = prompt

    userid = request.json['userid']
    if userid == "000":
        # msg.append({"role":"user", "content":query_faq})
        # response = model.chat(tokenizer, msg)

        #response = model.chat(tokenizer, query_faq)
        response = chat_API(query_faq)
        answear = {
            "userid": userid,
            "response": response,
            "status": faq_type
        }
    elif userid == "111":
        msg = [{"role": "user", "content": query_faq}]
       # response = model.chat(tokenizer, msg)
        response = chat_API(msg)

        answear = {
            "userid": userid,
            "response": response,
            "status": faq_type
        }
    else:
        history = []
        history.append({"role": "user", "content": query_faq})

        response = model.chat(tokenizer, history)
        answear = {
            "userid": userid,
            "response": response,
            "status": faq_type
        }
        torch_gc()
    return answear


if __name__ == '__main__':
    wsgi.server(eventlet.listen(('0.0.0.0', 3100)), app)
```
:::

- è°ƒç”¨
```python
    query = [{'role': "system", 'content': system},
             {'role': "user", 'content': prompt}]
    headers = {'Content-Type': 'application/json;charset=UTF-8'}
    payload = {'prompt': query, 'userid': "000", 'faq_type': 0}
    request = urllib.request.Request(ã€ipåœ°å€ã€‘, json.dumps(payload).encode("utf-8"), headers=headers)
    while True:
        try:
          response = urllib.request.urlopen(request, timeout=300)
          break
        except:
            print("Requesté”™è¯¯")
            time.sleep(1)
```

### Llama3
[Project](https://huggingface.co/blog/llama3)

[Llama-3-8B-Instruct](https://huggingface.co/meta-llama/Meta-Llama-3-8B-Instruct) (éœ€è¦ç”³è¯·ä½¿ç”¨ï¼Œä¸€èˆ¬ä¼šè¢«æ‹’ç»)

[å®˜æ–¹æ¨¡å‹ä¸‹è½½](https://llama.meta.com/llama-downloads/)

#### Convert the model weights using Hugging Face transformer from source
æ¨¡å‹ä¸‹è½½åä¸æ˜¯Hugging Faceæ ¼å¼ï¼Œå› æ­¤éœ€è¦è½¬æ¢ï¼ŒæŒ‰ç…§[æ•™ç¨‹](https://github.com/meta-llama/llama-recipes/blob/main/recipes/quickstart/Running_Llama3_Anywhere/Running_Llama_on_HF_transformers.ipynb)
```shell
python3 -m venv hf-convertor
source hf-convertor/bin/activate
git clone https://github.com/huggingface/transformers.git
cd transformers
pip install -e .
pip install torch tiktoken blobfile accelerate
python3 src/transformers/models/llama/convert_llama_weights_to_hf.py --input_dir ${path_to_meta_downloaded_model} --output_dir ${path_to_save_converted_hf_model} --model_size 8B --llama_version 3
```
æ‰å…‹ä¼¯æ ¼å¯çœŸèƒ½æ•´äººå•Š

ä¸€äº›æ¨¡å‹è¯´æ˜ï¼š
- è®­ç»ƒç¯å¢ƒï¼š24000ä¸ªGPU

#### FastChatç‰ˆ
éœ€è¦å…ˆå°†æ¨¡å‹è½¬æ¢è½¬æ¢ä¸ºHugging Faceç‰ˆ

ä½¿ç”¨æ–¹æ³•åŸºæœ¬å’Œ[Qwen](#_2-fastchatç‰ˆ)ç­‰å…¶ä»–å¤§æ¨¡å‹ä¸€æ ·
```shell
CUDA_VISIBLE_DEVICES=0 python3 -m fastchat.serve.model_worker --model-path Llama-3-8B-Instruct-hf --controller http://localhost:21001 --port 31000 --worker http://localhost:31000
```

## Qwen
**é€šä¹‰åƒé—®ï¼ˆQwenï¼‰** æ˜¯é˜¿é‡Œäº‘ç ”å‘çš„åŸºäºTransformerçš„å¤§è¯­è¨€æ¨¡å‹, åœ¨è¶…å¤§è§„æ¨¡çš„é¢„è®­ç»ƒæ•°æ®ä¸Šè¿›è¡Œè®­ç»ƒå¾—åˆ°ã€‚é¢„è®­ç»ƒæ•°æ®ç±»å‹å¤šæ ·ï¼Œè¦†ç›–å¹¿æ³›ï¼ŒåŒ…æ‹¬å¤§é‡ç½‘ç»œæ–‡æœ¬ã€ä¸“ä¸šä¹¦ç±ã€ä»£ç ç­‰ã€‚


### ç›¸å…³æ¨¡å‹
#### Qwen2.5 series
[Demo](https://huggingface.co/spaces/Qwen/Qwen2.5) | [é­”å¡”ç¤¾åŒº](https://www.modelscope.cn/studios/qwen/Qwen2.5)

Qwen2.5: Qwen2.5 language models, including pretrained and instruction-tuned models of 7 sizes, including 0.5B, 1.5B, 3B, 7B, 14B, 32B, and 72B.
- [Qwen2.5-0.5B](https://huggingface.co/Qwen/Qwen2.5-0.5B) | [Qwen2.5-0.5B-Instruct](https://huggingface.co/Qwen/Qwen2.5-0.5B-Instruct)
- [Qwen2.5-1.5B](https://huggingface.co/Qwen/Qwen2.5-1.5B) | [Qwen2.5-1.5B-Instruct](https://huggingface.co/Qwen/Qwen2.5-1.5B-Instruct)
- [Qwen2.5-3B](https://huggingface.co/Qwen/Qwen2.5-3B) | [Qwen2.5-3B-Instruct](https://huggingface.co/Qwen/Qwen2.5-3B-Instruct)
- [Qwen2.5-7B](https://huggingface.co/Qwen/Qwen2.5-7B) | [Qwen2.5-7B-Instruct](https://huggingface.co/Qwen/Qwen2.5-7B-Instruct)
- [Qwen2.5-14B](https://huggingface.co/Qwen/Qwen2.5-14B) | [Qwen2.5-14B-Instruct](https://huggingface.co/Qwen/Qwen2.5-14B-Instruct)
- [Qwen2.5-32B](https://huggingface.co/Qwen/Qwen2.5-32B) | [Qwen2.5-32B-Instruct](https://huggingface.co/Qwen/Qwen2.5-32B-Instruct)
- [Qwen2.5-72B](https://huggingface.co/Qwen/Qwen2.5-72B) | [Qwen2.5-72B-Instruct](https://huggingface.co/Qwen/Qwen2.5-72B-Instruct)

Qwen2.5-Coder: Code-specific model series based on Qwen2.5
- Demo: [ModelScope](https://www.modelscope.cn/studios/Qwen/Qwen2.5-Coder-demo)
- [Qwen2.5-Coder-1.5B](https://huggingface.co/Qwen/Qwen2.5-Coder-1.5B) | [Qwen2.5-Coder-1.5B-Instruct](https://huggingface.co/Qwen/Qwen2.5-Coder-1.5B-Instruct)
- [Qwen2.5-Coder-7B](https://huggingface.co/Qwen/Qwen2.5-Coder-7B) | [Qwen2.5-Coder-7B-Instruct](https://huggingface.co/Qwen/Qwen2.5-Coder-7B-Instruct)

Qwen2.5-Math: Math-specific model series based on Qwen2.5
- [Qwen2.5-Math-1.5B](https://huggingface.co/Qwen/Qwen2.5-Math-1.5B) | [Qwen2.5-Math-1.5B-Instruct](https://huggingface.co/Qwen/Qwen2.5-Math-1.5B-Instruct)
- [Qwen2.5-Math-7B](https://huggingface.co/Qwen/Qwen2.5-Math-7B)
- [Qwen2.5-Math-72B](https://huggingface.co/Qwen/Qwen2.5-Math-72B) | [Qwen2.5-Math-72B-Instruct](https://huggingface.co/Qwen/Qwen2.5-Math-72B-Instruct)

#### Qwen1 & Qwen2

[Github](https://github.com/QwenLM/Qwen1.5) | [Paper](https://arxiv.org/abs/2309.16609)

Demo [Qwen2-72B-Instruct](https://huggingface.co/spaces/Qwen/Qwen2-72B-Instruct) | [Qwen1.5-110B-Chat](https://huggingface.co/spaces/Qwen/Qwen1.5-110B-Chat-demo)

32B [Qwen1.5-32B](https://huggingface.co/Qwen/Qwen1.5-32B) | [Qwen1.5-32B-Chat](https://huggingface.co/Qwen/Qwen1.5-32B-Chat) |

14B [Qwen1.5-14B](https://huggingface.co/Qwen/Qwen1.5-14B) | [Qwen1.5-14B-Chat](https://huggingface.co/Qwen/Qwen1.5-14B-Chat)

7B [Qwen-7B-Chat](https://huggingface.co/Qwen/Qwen-7B-Chat) | [Qwen2-7B-Instruct](https://huggingface.co/Qwen/Qwen2-7B-Instruct)

[CodeQwen1.5-7b-Chat](https://huggingface.co/spaces/Qwen/CodeQwen1.5-7b-Chat-demo)

### Quick Start
#### 1. å‘½ä»¤è¡Œ
ä½¿ç”¨[FastChat](#fastchat)
```shell
python3 -m fastchat.serve.cli --model-path Qwen1.5-32B-Chat
```

####  2. Pythonè„šæœ¬

```python
from transformers import AutoModelForCausalLM, AutoTokenizer
device = "cuda" # the device to load the model onto

model = AutoModelForCausalLM.from_pretrained(
    "Qwen/Qwen1.5-32B-Chat",
    torch_dtype="auto",
    device_map="auto"
)
tokenizer = AutoTokenizer.from_pretrained("Qwen/Qwen1.5-32B-Chat")

prompt = "Give me a short introduction to large language model."
messages = [
    {"role": "system", "content": "You are a helpful assistant."},
    {"role": "user", "content": prompt}
]
text = tokenizer.apply_chat_template(
    messages,
    tokenize=False,
    add_generation_prompt=True
)
model_inputs = tokenizer([text], return_tensors="pt").to(device)

generated_ids = model.generate(
    model_inputs.input_ids,
    max_new_tokens=512
)
generated_ids = [
    output_ids[len(input_ids):] for input_ids, output_ids in zip(model_inputs.input_ids, generated_ids)
]

response = tokenizer.batch_decode(generated_ids, skip_special_tokens=True)[0]
```


### éƒ¨ç½²

32Bæ˜¾å­˜å ç”¨65G

#### 1. Flaskç‰ˆ
::: code-group
```python [chat]
from transformers import AutoModelForCausalLM, AutoTokenizer
device = "cuda" # the device to load the model onto

model = AutoModelForCausalLM.from_pretrained(
    "Qwen1.5-32B-Chat",
    torch_dtype="auto",
    device_map="auto"
)
tokenizer = AutoTokenizer.from_pretrained("Qwen1.5-32B-Chat")

def chat_API(messages):
    text = tokenizer.apply_chat_template(
        messages,
        tokenize=False,
        add_generation_prompt=True
    )
    model_inputs = tokenizer([text], return_tensors="pt").to(device)

    generated_ids = model.generate(
        model_inputs.input_ids,
        max_new_tokens=512
    )
    generated_ids = [
        output_ids[len(input_ids):] for input_ids, output_ids in zip(model_inputs.input_ids, generated_ids)
    ]

    response = tokenizer.batch_decode(generated_ids, skip_special_tokens=True)[0]

    return response

message = [
        {'role': "system", 'content': "ä½ æ˜¯1ä¸ªAIåŠ©æ‰‹"},
        {'role': "user", 'content': "å¸®æˆ‘è®°ä½ä¸€ä¸ªè¯ï¼ŒButton"},
        {'role': "assistant", 'content': "å¸®æˆ‘è®°ä½ä¸€ä¸ªè¯ï¼ŒButton"},
        {'role': "user", 'content': "æˆ‘è¦ä½ è®°ä½çš„è¯æ˜¯ä»€ä¹ˆ"}]


message1 = [{'role': "system", 'content': "ä½ æ˜¯1ä¸ªAIåŠ©æ‰‹, è¯·åœ¨æ‰€æœ‰å¯¹è¯ä¹‹å‰è¯´'ä½ å¥½,'"}, {'role': "user", 'content': "What is the captial of China?"}]
rec = chat_API(message1)

print(message1)
print(rec)
```

```python [app]
import eventlet

eventlet.monkey_patch()
from eventlet import wsgi

from flask import Flask, jsonify, request, make_response
from flask_cors import CORS, cross_origin
import torch
from flask_sock import Sock, Server

from chat import chat_API

DEVICE = "cuda"
# DEVICE_IDS = ["0", "1", "2"]
DEVICE_IDS = ["0"]


def torch_gc():
    if torch.cuda.is_available():
        for device_id in DEVICE_IDS:
            cuda_device = f"{DEVICE}:{device_id}"
            with torch.cuda.device(cuda_device):
                torch.cuda.empty_cache()
                torch.cuda.ipc_collect()


app = Flask(__name__)
sock = Sock(app)
app.config['JSON_AS_ASCII'] = False
CORS(app)


def end_sentence(sent):
    if sent and (
        sent.endswith("......") or
        sent.endswith("ã€‚") or
        sent.endswith("ï¼") or
        sent.endswith("ï¼Ÿ")
    ):
        return True
    return False


@sock.route("/chat_stream")
def dialog_stream(ws:Server):
    msg = ws.receive()
    chat_req = eval(msg)
    if 'prompt' not in chat_req or 'userid' not in chat_req or 'faq_type' not in chat_req:
        answer = {
            "errmsg": "è¯·æ±‚å‚æ•°é”™è¯¯ï¼Œex:{'prompt':'hello','userid':'123','faq_type':0}",
        }
        ws.send(answer)
        ws.close()
        return 201

    faq_type = chat_req['faq_type']
    prompt = chat_req['prompt']
    query_faq = None
    if faq_type == 0:
        query_faq = prompt

    userid = chat_req["userid"]
    history = []

    print(history)
    history.append({"role": "user", "content": query_faq})
    history_reponse = ''

    for response in model.chat(tokenizer, history, stream=True):
        if end_sentence(response):
            response = response.replace('\n',"")
            current_response = response.replace(history_reponse, '')
            if current_response:
                history_reponse += current_response
                answear = {
                    "userid": userid,
                    "response": current_response,
                    "status": 0
                }
                print(current_response)
                ws.send(answear)
    torch_gc()
    answear = {
        "userid": userid,
        "response": "end",
        "status": 1
    }
    history = history if len(history)<5 else history[-5:]
    history.append({"role": "user", "content": prompt})
    history.append({'role': 'assisant','content': response})
    print('history-after', history)
    ws.send(answear)
    ws.close()
    return 200


@app.route("/chat", methods=['POST'])
def dialog():
    if request.content_type.startswith('application/json'):
        if 'faq_type' not in request.json:
            resp = {
                "errmsg": "no param 'faq_type'",
                "status": 201
            }
            return resp
    faq_type = request.json['faq_type']
    prompt = request.json['prompt']
    query_faq = None
    if faq_type == 0:
        query_faq = prompt

    userid = request.json['userid']
    if userid == "000":
        # msg.append({"role":"user", "content":query_faq})
        # response = model.chat(tokenizer, msg)

        #response = model.chat(tokenizer, query_faq)
        response = chat_API(query_faq)
        answear = {
            "userid": userid,
            "response": response,
            "status": faq_type
        }
    elif userid == "111":
        msg = [{"role": "user", "content": query_faq}]
       # response = model.chat(tokenizer, msg)
        response = chat_API(msg)

        answear = {
            "userid": userid,
            "response": response,
            "status": faq_type
        }
    else:
        history = []
        history.append({"role": "user", "content": query_faq})

        response = model.chat(tokenizer, history)
        answear = {
            "userid": userid,
            "response": response,
            "status": faq_type
        }
        torch_gc()
    return answear


if __name__ == '__main__':
    wsgi.server(eventlet.listen(('0.0.0.0', 3100)), app)
```
:::

#### 2. FastChatç‰ˆ

http://felixzhao.cn/Articles/article/71

å¼€ä¸‰ä¸ªç»ˆç«¯
```shell
# ç¬¬ä¸€ä¸ªç»ˆç«¯
python3 -m fastchat.serve.controller

# ç¬¬äºŒä¸ªç»ˆç«¯
CUDA_VISIBLE_DEVICES=0 python3 -m fastchat.serve.model_worker --model-path Qwen1.5-32B-Chat --controller http://localhost:21001 --port 31000 --worker http://localhost:31000
# åŒå¡
CUDA_VISIBLE_DEVICES=0,1 python3 -m fastchat.serve.model_worker --model-path Qwen1.5-32B-Chat --controller http://localhost:21001 --port 31000 --worker http://localhost:31000 --num-gpus=2 --max-gpu-memory 35GiB
# Qwen2.5
python -m fastchat.serve.model_worker --model-path Qwen/Qwen2.5-3B-Instruct --controller http://localhost:21001 --port 31000 --worker http://localhost:31000
# æ˜¾å­˜ä¸å¤Ÿï¼Œéƒ¨ç½²é‡åŒ–ç‰ˆæœ¬
python -m fastchat.serve.model_worker --model-path Qwen/Qwen2.5-7B-Instruct --controller http://localhost:21001 --port 31000 --worker http://localhost:31000 --load-8bit

# ç¬¬ä¸‰ä¸ªç»ˆç«¯
## Gradio Web
python3 -m fastchat.serve.gradio_web_server

## API
python3 -m fastchat.serve.openai_api_server --host 0.0.0.0
```

å®¢æˆ·ç«¯ï¼š
```python
from openai import OpenAI

class FastChatAPI:
    def __init__(self, model, base_url=None, api_key="Empty"):
        if "gpt" not in model:
            # ä½¿ç”¨Local LLM
            self.client = OpenAI(base_url=base_url, api_key=api_key)
        else:
            # ä½¿ç”¨GPT
            self.client = OpenAI()
        self.model = model

    def completion(self, query, verbose=True):
        """Create a completion."""
        return self.completion_history(query, verbose, None)
       # response = self.client.completions.create(
       #     model=["Qwen1.5-32B-Chat"][0],
       #     prompt=query,
       #     max_tokens=768
       # )

       # return response.choices[0].text

    def completion_history(self, query, verbose=True, history=None):
        """Create a completion."""
        if history is None:
            msg = [
                {'role': 'system', 'content': f'ä½ æ˜¯ä¸€ä¸ªAIåŠ©æ‰‹ï¼Œè¯·å›ç­”.'},
                {'role': 'user', 'content': query},
            ]
        else:
            msg = history + [{'role': 'user', 'content': query}]
            # print(msg)

        response = self.client.chat.completions.create(
            model=self.model,
            messages=msg,
        )

        if verbose:
            print("ä½¿ç”¨çš„tokensï¼š", response.usage.total_tokens)
            print("é—®ï¼š", query)
            print("å›ç­”ï¼š", response.choices[0].message.content)

        new_history = msg + [{"role": "assistant", "content": response.choices[0].message.content}]
        return new_history

    def completion_stream(self, query):
        return self.completion_stream_history(query, None)

    def completion_stream_history(self, query, history=None):
        def end_sentence(sent):
            if sent and (sent.endswith("â€¦â€¦") or
                         sent.endswith("ï¼š") and len(sent) > 15 or
                         sent.endswith("ï¼Œ") and len(sent) > 15 or
                         sent.endswith("ã€‚") or
                         sent.endswith("ï¼") or
                         sent.endswith("ï¼Ÿ")):
                return True
            return False

        if history is None:
            msg = [
                {'role': 'system', 'content': f'ä½ æ˜¯ä¸€ä¸ªAIåŠ©æ‰‹ï¼Œè¯·å›ç­”.'},
                {'role': 'user', 'content': query},
            ]
        else:
            msg = history + [{'role': 'user', 'content': query}]

        response = self.client.chat.completions.create(
            model=self.model,
            messages=msg,
            temperature=0,
            stream=True
        )
        collected_messages = ''
        start_time = time.time()
        for chunk in response:
            chunk_time = time.time() - start_time  # calculate the time delay of the chunk
            chunk_content = chunk.choices[0].delta.content
            if chunk_content:
                collected_messages += chunk_content.replace("\n", "")
                if end_sentence(collected_messages):
                    print(f"Message received {chunk_time:.2f} seconds after request: {collected_messages}")
                    collected_messages = ''
        print(f"Full response received {chunk_time:.2f} seconds after request")

        return collected_messages
```

ã€æŠ¥é”™ã€‘AttributeError: module 'asyncio' has no attribute 'to_thread'ï¼š

ä»€ä¹ˆï¼Ÿä½ è¿˜åœ¨ç”¨`Python 3.9`ä¹‹å‰çš„ç‰ˆæœ¬ï¼ŒçœŸçš„æ˜¯åæ¸…å¤æ˜ï¼Œå¼€å†å²å€’è½¦ï¼[è§£å†³æ–¹æ¡ˆ](https://stackoverflow.com/questions/68523752/python-module-asyncio-has-no-attribute-to-thread)

 ã€æŠ¥é”™ã€‘ æµå¼APIé—®é¢˜

[å‚è€ƒå®‰è£…FastChat](#fastchat)

#### 3. LLM Farméƒ¨ç½²
LLM Farmæ”¯æŒå°†å¤§æ¨¡å‹éƒ¨ç½²åˆ°iPhoneå’ŒiPadç­‰è®¾å¤‡ä¸Šï¼Œå¯é€šè¿‡ä¸‹è½½GGUFæ–‡ä»¶ï¼Œå¹¶ä¼ é€åˆ°è®¾å¤‡æ–‡ä»¶å­˜å‚¨ä¸­æ¥å¯¼å…¥æ¨¡å‹ã€‚

[Qwen2.5-Coder-3B](https://www.modelscope.cn/models/Qwen/Qwen2.5-Coder-3B-Instruct-GGUF)

é…ç½®åŸºæœ¬å¯ä»¥éµå¾ªè‡ªå¸¦çš„`Phi3`çš„æ¨¡æ¿

è®¾å®šPrompt Formatä¸ºï¼š
```text
<|user|>
{{prompt}}<|end|>
<|assistant|>
```

Reverse prompts: `<|end|>`

Skip tokens: `<|assistant|>,<|user|>`

## Qwen-VL

Qwen-VL-Max [DemoğŸ¤–](https://huggingface.co/spaces/Qwen/Qwen-VL-Max) | [Qwen-VL-Chat](https://huggingface.co/Qwen/Qwen-VL-Chat) | [Qwen-VL-Chat-Int4](https://huggingface.co/Qwen/Qwen-VL-Chat-Int4)
| [Paper](https://arxiv.org/abs/2308.12966)

- æ˜¾å­˜å ç”¨ï¼š11.8G-28G (å®æµ‹çº¦ä¸º21G)
- token: 32768

ä¸€ä¸ªåŸºäºQwen APIçš„ComfyUIèŠ‚ç‚¹ï¼šhttps://github.com/ZHO-ZHO-ZHO/ComfyUI-Qwen-VL-API?tab=readme-ov-file
- é¦–å…ˆè¾“å…¥ä¸€ä¸ªå›¾åƒï¼Œç„¶åç”±Qwen-VL"describe the image"ï¼Œç„¶åå°†æ–‡å­—æè¿°é€šè¿‡CLIPä¼ ç»™SDï¼Œå®ç°image-to-image

### éƒ¨ç½²
```python
from transformers import AutoModelForCausalLM, AutoTokenizer
from transformers.generation import GenerationConfig
import torch
torch.manual_seed(1234)

# Note: The default behavior now has injection attack prevention off.
tokenizer = AutoTokenizer.from_pretrained("Qwen/Qwen-VL-Chat", trust_remote_code=True)

# use bf16
# model = AutoModelForCausalLM.from_pretrained("Qwen/Qwen-VL-Chat", device_map="auto", trust_remote_code=True, bf16=True).eval()
# use fp16
# model = AutoModelForCausalLM.from_pretrained("Qwen/Qwen-VL-Chat", device_map="auto", trust_remote_code=True, fp16=True).eval()
# use cpu only
# model = AutoModelForCausalLM.from_pretrained("Qwen/Qwen-VL-Chat", device_map="cpu", trust_remote_code=True).eval()
# use cuda device
model = AutoModelForCausalLM.from_pretrained("Qwen/Qwen-VL-Chat", device_map="cuda", trust_remote_code=True).eval()

# Specify hyperparameters for generation (No need to do this if you are using transformers>=4.32.0)
# model.generation_config = GenerationConfig.from_pretrained("Qwen/Qwen-VL-Chat", trust_remote_code=True)

# 1st dialogue turn
query = tokenizer.from_list_format([
    {'image': 'https://qianwen-res.oss-cn-beijing.aliyuncs.com/Qwen-VL/assets/demo.jpeg'},
    {'text': 'è¿™æ˜¯ä»€ä¹ˆ'},
])
response, history = model.chat(tokenizer, query=query, history=None)
print(response)
# å›¾ä¸­æ˜¯ä¸€åå¹´è½»å¥³å­åœ¨æ²™æ»©ä¸Šå’Œå¥¹çš„ç‹—ç©è€ï¼Œç‹—çš„å“ç§å¯èƒ½æ˜¯æ‹‰å¸ƒæ‹‰å¤šã€‚å¥¹ä»¬ååœ¨æ²™æ»©ä¸Šï¼Œç‹—çš„å‰è…¿æŠ¬èµ·æ¥ï¼Œä¼¼ä¹åœ¨å’Œäººç±»å‡»æŒã€‚ä¸¤äººä¹‹é—´å……æ»¡äº†ä¿¡ä»»å’Œçˆ±ã€‚

# 2nd dialogue turn
response, history = model.chat(tokenizer, 'è¾“å‡º"å‡»æŒ"çš„æ£€æµ‹æ¡†', history=history)
print(response)
# <ref>å‡»æŒ</ref><box>(517,508),(589,611)</box>
image = tokenizer.draw_bbox_on_latest_picture(response, history)
if image:
  image.save('1.jpg')
else:
  print("no box")
```

#### è¾“å…¥
æ³¨æ„åˆ°åŸå·¥ç¨‹çš„ä¸­`tokenization_qwen.py`ä¸­
```python
    def from_list_format(self, list_format: List[Dict]):
        text = ''
        num_images = 0
        for ele in list_format:
            if 'image' in ele:
                num_images += 1
                text += f'Picture {num_images}: '
                text += self.image_start_tag + ele['image'] + self.image_end_tag
                text += '\n'
            elif 'text' in ele:
                text += ele['text']
            elif 'box' in ele:
                if 'ref' in ele:
                    text += self.ref_start_tag + ele['ref'] + self.ref_end_tag
                for box in ele['box']:
                    text += self.box_start_tag + '(%d,%d),(%d,%d)' % (box[0], box[1], box[2], box[3]) + self.box_end_tag
            else:
                raise ValueError("Unsupport element: " + str(ele))
        return text
```
å¯åªé™¤äº†`'image'`å’Œ`'text'`ä¹‹å¤–ï¼Œè¿˜æ”¯æŒç¬¬ä¸‰ç§è¾“å…¥æ–¹å¼ï¼Œé‚£å°±æ˜¯`'box'`ã€‚å¯¹è¯ä¸­çš„æ£€æµ‹æ¡†å¯ä»¥è¡¨ç¤ºä¸º`<box>(x1,y1),(x2,y2)</box>`ï¼Œ
å…¶ä¸­ `(x1, y1)` å’Œ`(x2, y2)`åˆ†åˆ«å¯¹åº”å·¦ä¸Šè§’å’Œå³ä¸‹è§’çš„åæ ‡ï¼Œå¹¶ä¸”è¢«å½’ä¸€åŒ–åˆ°`[0, 1000)`çš„èŒƒå›´å†…. æ£€æµ‹æ¡†å¯¹åº”çš„æ–‡æœ¬æè¿°ä¹Ÿå¯ä»¥é€šè¿‡`<ref>text_caption</ref>`è¡¨ç¤ºã€‚

ç¬¬ä¸‰ç§ä»»åŠ¡ç¤ºä¾‹
```python
# ......å¯æ¥ä¸Šä¸€ä¸ªä»£ç 

# bboxè¾“å…¥
query = tokenizer.from_list_format([
    {'image': '00260-70362828.png'},
    {'text': 'æ£€æµ‹æ¡†ä¸­çš„äººæ˜¯ç«™ç€è¿˜æ˜¯åç€?'},
    {'box': [(272,271,526,957)]}
])
print("query:", query)

response, history = model.chat(tokenizer, query=query, history=None)
print(response)
image = tokenizer.draw_bbox_on_latest_picture("<box>(272,271),(526,957)</box>", history)
if image:
  image.save('2.jpg')
else:
  print("no box")
```

æœåŠ¡(éœ€è¦åˆ›å»ºä¸€ä¸ªç©ºæ–‡ä»¶å¤¹å‘½åä¸º`data`)

::: code-group
```python [service]
from flask import Flask, request, jsonify
import time
from flask_sock import Sock, Server
from flask_cors import CORS
from eventlet import wsgi
import eventlet
from PIL import Image
import torch
from transformers import AutoTokenizer, AutoModelForCausalLM
import json

torch.manual_seed(1234)

# Load tokenizer and model
# QianWen_VL 7B
tokenizer = AutoTokenizer.from_pretrained("Qwen/Qwen-VL-Chat", trust_remote_code=True)
model = AutoModelForCausalLM.from_pretrained("Qwen/Qwen-VL-Chat", device_map="cuda", trust_remote_code=True).eval()

app = Flask(__name__)
sock = Sock(app)
app.config['JSON_AS_ASCII'] = False
CORS(app)
img_path = "data/temp.jpg"

@app.route('/qianwen_vl', methods=['POST'])
def qianwen_vl():
    image_file = request.files['image']
    save_jpg(image_file, img_path)
    #prompt = request.form['prompt']
    print(json.loads(request.form.to_dict()['content']))
    query_list = [{'image' :img_path}] + json.loads(request.form.to_dict()['content'])
    print(query_list)
    query = tokenizer.from_list_format(query_list)
    print(query)
   # query = f'<img>{img_path}</img>{prompt}'
    start_time = time.time()
    response, _ = model.chat(tokenizer, query=query, history=None)
    end_time = time.time()

    return jsonify({'response': response, 'running_time': end_time - start_time})

def save_jpg(image_file, file_name):
    image = Image.open(image_file.stream)
    image.save(file_name, "JPEG")

if __name__ == '__main__':
    wsgi.server(eventlet.listen(('0.0.0.0', 5005)), app)
```

```python [API]
import cv2
import json
import requests

# å®šä¹‰ Flask æœåŠ¡å™¨çš„åœ°å€
server_url = 'http://192.168.xxx.xxx:5005/qianwen_vl'


def query_qwen_vl(data):
    image_path = data[0]["image"]
    content = data[1:]
    # è½¬æˆå­—ç¬¦ä¸²å†å‘é€ç»™æœåŠ¡ç«¯
    data_send = {'content': json.dumps(content, ensure_ascii=False)}
    #print(data_send)
    # æ‰“å¼€å¹¶è¯»å–å›¾åƒæ–‡ä»¶
    with open(image_path, 'rb') as f:
        # æ„é€ è¯·æ±‚æ•°æ®
        files = {'image': f}

        try:
            # å‘é€ POST è¯·æ±‚
            response = requests.post(server_url, files=files, data=data_send)

            # æ£€æŸ¥å“åº”çŠ¶æ€ç 
            if response.status_code == 200:
                # è§£æ JSON å“åº”
                response_data = response.json()
                print("é—®ï¼š", data)
                print("å¯¹è¯å“åº”:", response_data['response'])
                print("è¿è¡Œæ—¶é—´:", response_data['running_time'])
            else:
                print("è¯·æ±‚å¤±è´¥:", response.status_code)
        except requests.RequestException as e:
            print("è¯·æ±‚é”™è¯¯:", e)
    return response_data['response']


def extract_bbox(text):
    current_text = text[text.index('<box>'):]

    bboxs = []
    while '<box>' in current_text:
        current_text = current_text[current_text.index('<box>')+5:]
        current_text_list = eval("[%s]" % current_text[:current_text.index('</box>')])
        bboxs.append([current_text_list[0][0], current_text_list[0][1], current_text_list[1][0], current_text_list[1][1]])

    return bboxs


def draw_bbox_cv(img, box, text=None):
    thickness = 3  # æ¡†çš„åšåº¦
    # ç”»çŸ©å½¢æ¡†
    img1 = cv2.rectangle(img, (box[0], box[1]), (box[2], box[3]), (0, 255, 255), thickness)
    if text is not None:
        img1 = cv2.putText(img1, text, (box[0],box[1]), cv2.FONT_HERSHEY_SIMPLEX, 1, (255, 255, 0), 2)
    return img1


def test1():
    # å›¾åƒæ–‡ä»¶è·¯å¾„
    image_file = '00346-4016997344.png'
    data = [{"image": image_file}, {"text": "å›¾ä¸­æ˜¯ä»€ä¹ˆå®¶å…·ï¼Ÿ"}]
    query_qwen_vl(data)


def test2():
    # å›¾åƒæ–‡ä»¶è·¯å¾„
    image_file = '00260-70362828.png'
    data = [{"image": image_file}, {"text": "è¾“å‡ºäººçš„æ£€æµ‹æ¡†"}]
    res = query_qwen_vl(data)
    bboxs = extract_bbox(res)

    # ç”»å›¾
    image = cv2.imread(image_file)
    for i, bbox in enumerate(bboxs):
        image = draw_bbox_cv(image, bbox, str(i))
    cv2.imshow("Image", image)
    cv2.waitKey()


def test3():
    test_bbox = (267,269,526,958)
    # å›¾åƒæ–‡ä»¶è·¯å¾„
    image_file = '00260-70362828.png'
    data = [{"image": image_file}, {"text": "æ£€æµ‹æ¡†ä¸­çš„äººç©¿çš„ä»€ä¹ˆè¡£æœï¼Ÿ"}, {"box": [test_bbox]}]
    query_qwen_vl(data)

    # ç”»å›¾
    image = cv2.imread(image_file)
    image = draw_bbox_cv(image, test_bbox)
    cv2.imshow("Image", image)
    cv2.waitKey()


if __name__ == "__main__":
    test3()
```
:::

```shell
pip install tiktoken matplotlib
```

### qwen2
[Qwen2-VL-2B-Instruct](https://huggingface.co/Qwen/Qwen2-VL-2B-Instruct)

[Qwen2-VL-7B-Instruct](https://huggingface.co/Qwen/Qwen2-VL-7B-Instruct)

[Qwen2-VL-72B-Instruct](https://huggingface.co/Qwen/Qwen2-VL-72B-Instruct)

#### Install
```shell
# å‡çº§transformers
pip install -U git+https://github.com/huggingface/transformers

pip install qwen-vl-utils

# æ¨è12.4çš„cudaç¯å¢ƒå’Œtorch2.4
```

#### Demo
```python
from transformers import Qwen2VLForConditionalGeneration, AutoTokenizer, AutoProcessor
from qwen_vl_utils import process_vision_info

# default: Load the model on the available device(s)
model = Qwen2VLForConditionalGeneration.from_pretrained(
    "Qwen/Qwen2-VL-2B-Instruct", torch_dtype="auto", device_map="auto"
)

# We recommend enabling flash_attention_2 for better acceleration and memory saving, especially in multi-image and video scenarios.
# model = Qwen2VLForConditionalGeneration.from_pretrained(
#     "Qwen/Qwen2-VL-2B-Instruct",
#     torch_dtype=torch.bfloat16,
#     attn_implementation="flash_attention_2",
#     device_map="auto",
# )

# default processer
processor = AutoProcessor.from_pretrained("Qwen/Qwen2-VL-2B-Instruct")

# The default range for the number of visual tokens per image in the model is 4-16384. You can set min_pixels and max_pixels according to your needs, such as a token count range of 256-1280, to balance speed and memory usage.
# min_pixels = 256*28*28
# max_pixels = 1280*28*28
# processor = AutoProcessor.from_pretrained("Qwen/Qwen2-VL-2B-Instruct", min_pixels=min_pixels, max_pixels=max_pixels)

messages = [
    {
        "role": "user",
        "content": [
            {
                "type": "image",
                "image": "00260-70362828.png",
            },
            {"type": "text", "text": "è¿™æ˜¯ä»€ä¹ˆ?"},
        ],
    }
]

# Preparation for inference
text = processor.apply_chat_template(
    messages, tokenize=False, add_generation_prompt=True
)
image_inputs, video_inputs = process_vision_info(messages)
inputs = processor(
    text=[text],
    images=image_inputs,
    videos=video_inputs,
    padding=True,
    return_tensors="pt",
)
inputs = inputs.to("cuda")

# Inference: Generation of the output
generated_ids = model.generate(**inputs, max_new_tokens=128)
generated_ids_trimmed = [
    out_ids[len(in_ids) :] for in_ids, out_ids in zip(inputs.input_ids, generated_ids)
]
output_text = processor.batch_decode(
    generated_ids_trimmed, skip_special_tokens=True, clean_up_tokenization_spaces=False
)
print(output_text)

```

æœåŠ¡
::: code-group
```python [service]
from flask import Flask, request, jsonify
import time
from flask_sock import Sock, Server
from flask_cors import CORS
from eventlet import wsgi
import eventlet
from PIL import Image
import torch
import json
from transformers import Qwen2VLForConditionalGeneration, AutoTokenizer, AutoProcessor
from qwen_vl_utils import process_vision_info


torch.manual_seed(1234)

device = "cuda:1"
# Load tokenizer and model
model = Qwen2VLForConditionalGeneration.from_pretrained(
    "Qwen/Qwen2-VL-2B-Instruct", torch_dtype="auto", device_map=device
)
# default processer
processor = AutoProcessor.from_pretrained("Qwen/Qwen2-VL-2B-Instruct")

app = Flask(__name__)
sock = Sock(app)
app.config['JSON_AS_ASCII'] = False
CORS(app)
img_path = "data/temp.jpg"

@app.route('/qianwen_vl', methods=['POST'])
def qianwen_vl():
#    if 'image' not in request.files or 'text' not in request.form:
 #       return jsonify({'error': 'Missing image file or prompt.'}), 400

    image_file = request.files['image']
    save_jpg(image_file, img_path)
    #prompt = request.form['prompt']
    ask_content = json.loads(request.form.to_dict()['content'])
    print("Ask content:", ask_content)
    #query_list = [{'image' :img_path}] + json.loads(request.form.to_dict()['content'])
    #print(query_list)

    messages = [
        {
            "role": "user",
            "content": [
                {
                    "type": "image",
                    "image": img_path,
                }
            ],
        }
    ]
    for content_item in ask_content:
        item_type = list(content_item.keys())[0]
        messages[0]["content"].append({"type": item_type, item_type: content_item[item_type]})

    print(messages)

    # Preparation for inference
    start_time = time.time()
    text = processor.apply_chat_template(
        messages, tokenize=False, add_generation_prompt=True
    )
    image_inputs, video_inputs = process_vision_info(messages)
    inputs = processor(
        text=[text],
        images=image_inputs,
        videos=video_inputs,
        padding=True,
        return_tensors="pt",
    )
    inputs = inputs.to(device)

    # Inference: Generation of the output
    generated_ids = model.generate(**inputs, max_new_tokens=128)
    generated_ids_trimmed = [
        out_ids[len(in_ids) :] for in_ids, out_ids in zip(inputs.input_ids, generated_ids)
    ]
    output_text = processor.batch_decode(
        generated_ids_trimmed, skip_special_tokens=True, clean_up_tokenization_spaces=False
    )

   # query = f'<img>{img_path}</img>{prompt}'
   # response, _ = model.chat(tokenizer, query=query, history=None)
    end_time = time.time()

    return jsonify({'response': output_text, 'running_time': end_time - start_time})

def save_jpg(image_file, file_name):
    image = Image.open(image_file.stream)
    image.save(file_name, "JPEG")

if __name__ == '__main__':
    wsgi.server(eventlet.listen(('0.0.0.0', 5005)), app)
```

```python [API]
# APIå‘å‰å…¼å®¹
```
:::

## GLM4
[glm-4-9b-chat](https://huggingface.co/THUDM/glm-4-9b-chat)

```shell
pip install transformers==4.36.0
```

### FastChatç‰ˆ
ç›®å‰è¿˜ä¸å…¼å®¹

## EvoLLM-JP
2024.3 SakanaAIå‘å¸ƒEvoLLM-JPï¼ˆå¤§è¯­è¨€æ¨¡å‹ï¼‰ã€EvoVLM-JP(è§†è§‰è¯­è¨€æ¨¡å‹)å’ŒEvoSDXL-JP(å›¾åƒç”Ÿæˆæ¨¡å‹)ã€‚
[EvoLLM-JP-v1-10B](https://huggingface.co/SakanaAI/EvoLLM-JP-v1-10B/tree/main) | [Paper](https://arxiv.org/abs/2403.13187)

### Install
åˆ›å»ºä¸€ä¸ªè·¯å¾„å‘½ä¸º`SakanaAI/EvoLLM-v1-JP-10B`ï¼Œç„¶åæŠŠæ–‡ä»¶å­˜å…¥è¿™é‡Œï¼Œæ³¨æ„ä¸€å®šæ˜¯`EvoLLM-v1-JP-10B`ï¼Œä¸èƒ½æ˜¯å…¶ä»–çš„ï¼Œå› ä¸ºè¿™å¸®äººçš„ä»£ç ä¸æ˜¯å¾ˆå¥½ï¼Œåœ¨`config.json`ä¸­å†™æ­»äº†ã€‚

ç„¶åå®‰è£…åº“
```shell
pip install simpletransformers
pip install flash_attn
pip install transformers==4.41.2
```

ç„¶åä»£ç ä¹Ÿä¸èƒ½ç›´æ¥ç”¨HuggingFaceä¸Šçš„ï¼Œå› ä¸ºè¿™å¸®äººçš„ä»£ç ä¸æ˜¯å¾ˆå¥½
```python
import torch
from transformers import AutoModelForCausalLM, AutoTokenizer

# 1. load model
device = "cuda" if torch.cuda.is_available() else "cpu"
repo_id = "SakanaAI/EvoLLM-v1-JP-10B"
model = AutoModelForCausalLM.from_pretrained(repo_id, torch_dtype="auto", trust_remote_code=True)
tokenizer = AutoTokenizer.from_pretrained(repo_id)
model.to(device)

# 2. prepare inputs
text = "æ±äº¬ã®ãŠã„ã—ã„ã‚‚ã®ã¯ä½•ã§ã™ã‹ï¼Ÿ"
messages = [
    {"role": "system", "content": "ã‚ãªãŸã¯AIã‚¢ã‚·ã‚¹ã‚¿ãƒ³ãƒˆã§ã™"},
    {"role": "user", "content": text},
]

# æ‹¼æ¥æ¶ˆæ¯å†…å®¹
full_text = ""
for message in messages:
    if message["role"] == "system":
        full_text += f"{message['content']}\n"
    elif message["role"] == "user":
        full_text += f"{message['content']}"

# ä½¿ç”¨tokenizerè¿›è¡Œç¼–ç 
inputs = tokenizer(full_text, return_tensors="pt")

# å°†inputsç§»åŠ¨åˆ°deviceä¸Šï¼Œå¹¶è½¬æ¢ä¸ºå­—å…¸æ ¼å¼
input_ids = inputs["input_ids"].to(device)
attention_mask = inputs["attention_mask"].to(device)
input_dict = {"input_ids": input_ids, "attention_mask": attention_mask}

# 3. generate
output_ids = model.generate(**input_dict, max_new_tokens=50)
output_ids = output_ids[:, inputs.input_ids.shape[1]:]
generated_text = tokenizer.batch_decode(output_ids, skip_special_tokens=True)[0]
print("é—®:", text)
print("ç­”:", generated_text)
```

## Qwen2-Audio
[Qwen2-Audio-7B](https://huggingface.co/Qwen/Qwen2-Audio-7B) | [Qwen2-Audio-7B-Instruct](https://huggingface.co/Qwen/Qwen2-Audio-7B-Instruct)


## CyberAgentLM

[calm3-22b-chat](https://huggingface.co/cyberagent/calm3-22b-chat/tree/main) | [Demo](https://huggingface.co/spaces/cyberagent/calm3-22b-chat-demo)

[å…¬å¸ç®€ä»‹](https://www.cyberagent.co.jp/corporate/overview/)

### éƒ¨ç½²
æµ‹è¯•ä»£ç å¯ä»¥ç…§æ¬ï¼Œä¸ä¼šæœ‰æŠ¥é”™

#### FastChatç‰ˆ
èƒ½å¤Ÿå…¼å®¹FastChatï¼Œä¼°è®¡æ˜¯å› ä¸ºå’ŒLlamaå¾ˆåƒï¼Œå†å°±æ˜¯è¿™å®¶çš„ç¨‹åºæ°´å¹³é«˜ä¸€äº›ï¼Œä½¿ç”¨æ–¹æ³•åŸºæœ¬å’Œ[Qwen](#_2-fastchatç‰ˆ)ç­‰å…¶ä»–å¤§æ¨¡å‹ä¸€æ ·
```shell
# åŒå¡æ–¹æ¡ˆå‘½ä»¤ï¼ˆæ§åˆ¶ä¸¤ä¸ªGPUå„å ä¸€åŠæ˜¾å­˜ï¼‰
CUDA_VISIBLE_DEVICES=0,1 python3 -m fastchat.serve.model_worker --model-path cyberagent/calm3-22b-chat --controller http://localhost:21001 --port 31000 --worker http://localhost:31000 --num-gpus=2 --max-gpu-memory 26GiB
# 1å¡æ–¹æ¡ˆå‘½ä»¤
CUDA_VISIBLE_DEVICES=0 python3 -m fastchat.serve.model_worker --model-path cyberagent/calm3-22b-chat --controller http://localhost:21001 --port 31000 --worker http://localhost:31000
```

Mujiè¯ä¹¦é“¾æ¥ï¼šhttps://www.mojidict.com/

## é‡åŒ–
### Install AutoGPTQ
```shell
conda create -n quant python=3.10
conda activate quant
pip install torch==2.2.1+cu118 --extra-index-url https://download.pytorch.org/whl/cu118
pip install auto-gptq --no-build-isolation --extra-index-url https://huggingface.github.io/autogptq-index/whl/cu118/
```

### Quick Start
```python
from transformers import AutoTokenizer, TextGenerationPipeline
from auto_gptq import AutoGPTQForCausalLM, BaseQuantizeConfig
import logging

logging.basicConfig(
    format="%(asctime)s %(levelname)s [%(name)s] %(message)s", level=logging.INFO, datefmt="%Y-%m-%d %H:%M:%S"
)

pretrained_model_dir = "facebook/opt-125m"
quantized_model_dir = "opt-125m-4bit"

tokenizer = AutoTokenizer.from_pretrained(pretrained_model_dir, use_fast=True)
examples = [
    tokenizer(
        "auto-gptq is an easy-to-use model quantization library with user-friendly apis, based on GPTQ algorithm."
    )
]

quantize_config = BaseQuantizeConfig(
    bits=4,  # quantize model to 4-bit
    group_size=128,  # it is recommended to set the value to 128
    desc_act=False,  # set to False can significantly speed up inference but the perplexity may slightly bad
)

# load un-quantized model, by default, the model will always be loaded into CPU memory
model = AutoGPTQForCausalLM.from_pretrained(pretrained_model_dir, quantize_config)

# quantize model, the examples should be list of dict whose keys can only be "input_ids" and "attention_mask"
model.quantize(examples)

# save quantized model
model.save_quantized(quantized_model_dir)

# save quantized model using safetensors
model.save_quantized(quantized_model_dir, use_safetensors=True)

# push quantized model to Hugging Face Hub.
# to use use_auth_token=True, Login first via huggingface-cli login.
# or pass explcit token with: use_auth_token="hf_xxxxxxx"
# (uncomment the following three lines to enable this feature)
# repo_id = f"YourUserName/{quantized_model_dir}"
# commit_message = f"AutoGPTQ model for {pretrained_model_dir}: {quantize_config.bits}bits, gr{quantize_config.group_size}, desc_act={quantize_config.desc_act}"
# model.push_to_hub(repo_id, commit_message=commit_message, use_auth_token=True)

# alternatively you can save and push at the same time
# (uncomment the following three lines to enable this feature)
# repo_id = f"YourUserName/{quantized_model_dir}"
# commit_message = f"AutoGPTQ model for {pretrained_model_dir}: {quantize_config.bits}bits, gr{quantize_config.group_size}, desc_act={quantize_config.desc_act}"
# model.push_to_hub(repo_id, save_dir=quantized_model_dir, use_safetensors=True, commit_message=commit_message, use_auth_token=True)

# load quantized model to the first GPU
model = AutoGPTQForCausalLM.from_quantized(quantized_model_dir, device="cuda:0")

# download quantized model from Hugging Face Hub and load to the first GPU
# model = AutoGPTQForCausalLM.from_quantized(repo_id, device="cuda:0", use_safetensors=True, use_triton=False)

# inference with model.generate
print(tokenizer.decode(model.generate(**tokenizer("auto_gptq is", return_tensors="pt").to(model.device))[0]))

# or you can also use pipeline
pipeline = TextGenerationPipeline(model=model, tokenizer=tokenizer)
print(pipeline("auto-gptq is")[0]["generated_text"])
```

[é‡åŒ–/å¾®è°ƒæ•™ç¨‹](https://mp.weixin.qq.com/s?__biz=MzA5ODg3Mzk5NQ==&mid=2453344848&idx=1&sn=cf6274840839dde767496f20344664e6&chksm=874555b4b032dca2452b2b0391b85de94f31a7176fca0509b5d1883aa3cc12e68bcc0c1b6c66&scene=178&cur_album_id=3377603146426613767#rd)

## å¾®è°ƒ
- LoRA
- QLoRA
- å…¨å‚æ•°å¾®è°ƒ

GPUEZæ™ºèƒ½ç®—åŠ›äº‘å¹³å°: https://gpuez.com/

### è®­ç»ƒæ•°æ®
å¯ä»¥ç”¨GPTè¿›è¡Œæ‰©å……
```json
[
  {
    "question": "9.11å’Œ9.9å“ªä¸ªå¤§ï¼Ÿ",
    "answer": "9.9æ›´å¤§"
  }
]
```

## OpenGPT-4o
[Demo](https://huggingface.co/spaces/KingNish/OpenGPT-4o)

å°±æ˜¯LLava

## NVIDIA-LLM

TensorRT-LLMæ˜¯ä¸€ä¸ªå¼€æºåº“ï¼Œç”¨äºä¼˜åŒ–æœ€æ–°å¤§è¯­è¨€æ¨¡å‹åœ¨NVIDIA GPUä¸Šçš„æ¨ç†æ€§èƒ½ï¼Œå®ƒåŸºäºFastTransformerå’ŒTensorRTæ„å»º

github: NVIDIA/TensorRT-LLM

é­”å¡”ç¤¾åŒºï¼šhttps://www.modelscope.cn/organization/TensorRT-LLM?tab=model

```shell
conda install -c conda-forge mpi4py mpich
sudo nala install git-lfs
sudo nala install openmpi-bin openmpi-doc libopenmpi-dev

pip3 install tensorrt_llm==0.8.0 --extra-index-url https://pypi.nvidia.com
python3 -v -c "import tensorrt_llm"
```

## å…¶ä»–
XXXXXXX
