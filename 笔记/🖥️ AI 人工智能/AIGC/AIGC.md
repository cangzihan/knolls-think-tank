---
tags:
  - Stable Diffusion
  - AIç»˜å›¾
---

<style>
html.dark .light-mode {
  display: none;
}

html.dark .dark-mode {
  display: block;
}

html:not(.dark) .light-mode {
  display: block;
}

html:not(.dark) .dark-mode {
  display: none;
}
</style>

# AIGC

å¸¸ç”¨çš„AIç»˜å›¾æ¨¡å‹ï¼š
1. [Midjourney](https://midjourney.co/generator): åœ¨çº¿AIç»˜å›¾ç½‘ç«™ï¼Œå…è´¹ä½¿ç”¨15æ¬¡ã€‚
2. æ–‡å¿ƒä¸€æ ¼: ç”±ç™¾åº¦é£æ¡¨ã€æ–‡å¿ƒå¤§æ¨¡å‹çš„æŠ€æœ¯åˆ›æ–°æ¨å‡ºçš„â€œAI ä½œç”»â€äº§å“ï¼Œåœ¨çº¿ä½¿ç”¨ã€‚
3. Stable Diffusion: æœ‰å¼€æºæ¨¡å‹ï¼Œä¹Ÿæœ‰API
4. [DALLÂ·E 2](https://openai.com/dall-e-2): ä»˜è´¹è®¢é˜…ChatGPTåå¯ç›´æ¥ä½¿ç”¨

## å®ç”¨å·¥å…·
Stable Diffusion XL Inpainting: https://huggingface.co/spaces/diffusers/stable-diffusion-xl-inpainting

## é¡¹ç›®åœ°å€
Stable Diffusion:
https://github.com/CompVis/stable-diffusion

WebUI:
https://github.com/AUTOMATIC1111/stable-diffusion-webui

WebUIæ’ä»¶ï¼š
- LoRA: https://github.com/kohya-ss/sd-webui-additional-networks
- ControlNet: https://github.com/Mikubill/sd-webui-controlnet
- è…¾è®¯ControlNetæ¨¡å‹: [T2I-Adapter](https://huggingface.co/TencentARC/T2I-Adapter/tree/main/models) | [t2i-adapter-lineart-sdxl](https://huggingface.co/TencentARC/t2i-adapter-lineart-sdxl-1.0)
- IP2P(éControlNetç‰ˆ): https://github.com/Klace/stable-diffusion-webui-instruct-pix2pix
- AnimateDiff: https://github.com/continue-revolution/sd-webui-animatediff

ComfyUI:
https://github.com/comfyanonymous/ComfyUI/tree/master
```shell
pip install -r requirements.txt  -i https://pypi.tuna.tsinghua.edu.cn/simple
```
ComfyUI æ’ä»¶:
- æ’ä»¶ç®¡ç†å™¨: https://github.com/ltdrdata/ComfyUI-Manager
- lllyasviel/ControlNet [å®‰è£…](#controlnet-aux):
  1. https://huggingface.co/lllyasviel/Annotators/tree/5bc80eec2b4fddbb743c1e9329e3bd94b2cae14d
  2. https://huggingface.co/dhkim2810/MobileSAM/tree/main

## Stable Diffusion
Stable Diffusionæœ€åˆæ˜¯ç”±Heidelberg å¤§å­¦å’Œ[Stability AI](https://stability.ai/), [Runway](https://runwayml.com/)åˆä½œçš„å¼€æºé¡¹ç›®ã€‚

### åŸç†

<div class="theme-image">
  <img src="./assets/SD.png" alt="Light Mode Image" class="light-mode">
  <img src="./assets/dark_SD.png" alt="Dark Mode Image" class="dark-mode">
</div>

#### åˆ†è¯å™¨(tokenizer)
textå…ˆç”±CLIPè¿›è¡Œæ ‡è®°åŒ–ï¼ŒCLIPæ˜¯ç”±OpenAIå¼€å‘ï¼ˆè‹±æ–‡ç‰ˆï¼‰çš„ä¸€ç§å¤šæ¨¡æ€æ¨¡å‹ï¼Œæ—¨åœ¨ç†è§£å›¾åƒå’Œæ–‡æœ¬ä¹‹é—´çš„å…³ç³»ã€‚CLIPçš„è®­ç»ƒè¿‡ç¨‹åŒ…æ‹¬ï¼š
1. **æ•°æ®é›†**ï¼šCLIPä½¿ç”¨åŒ…å«å›¾åƒå’Œç›¸åº”æè¿°æ€§æ–‡æœ¬å¯¹çš„å¤§è§„æ¨¡æ•°æ®é›†è¿›è¡Œè®­ç»ƒã€‚
2. **å¯¹æ¯”å­¦ä¹ **ï¼šCLIPé‡‡ç”¨å¯¹æ¯”å­¦ä¹ çš„æ–¹æ³•ï¼Œé€šè¿‡æœ€å¤§åŒ–å›¾åƒå’Œå¯¹åº”æ–‡æœ¬çš„ç›¸ä¼¼æ€§ï¼ˆè€Œä¸æ˜¯ä¸éšæœºæ–‡æœ¬çš„ç›¸ä¼¼æ€§ï¼‰æ¥è®­ç»ƒæ¨¡å‹ã€‚CLIPä½¿ç”¨äº†ä¸¤ä¸ªç‹¬ç«‹çš„ç¥ç»ç½‘ç»œï¼Œä¸€ä¸ªç”¨äºå¤„ç†å›¾åƒï¼ˆå›¾åƒç¼–ç å™¨ï¼‰ï¼Œä¸€ä¸ªç”¨äºå¤„ç†æ–‡æœ¬ï¼ˆæ–‡æœ¬ç¼–ç å™¨ï¼‰ã€‚

[[Blog]](https://openai.com/blog/clip/) [[Paper]](https://arxiv.org/abs/2103.00020) [[Model Card]](model-card.md) [[Colab]](https://colab.research.google.com/github/openai/clip/blob/master/notebooks/Interacting_with_CLIP.ipynb)


åŠŸèƒ½ï¼š
- å›¾åƒ-æ–‡æœ¬ç›¸ä¼¼æ€§è¯„ä¼°ï¼šCLIPå¯ä»¥è®¡ç®—ä»»æ„å›¾åƒå’Œæ–‡æœ¬ä¹‹é—´çš„ç›¸ä¼¼æ€§ï¼Œæ‰¾åˆ°æœ€ç›¸å…³çš„å›¾åƒæˆ–æ–‡æœ¬ã€‚
- é›¶æ ·æœ¬åˆ†ç±»ï¼šé€šè¿‡å¯¹æ–‡æœ¬æè¿°è¿›è¡Œç¼–ç ï¼ŒCLIPå¯ä»¥åœ¨æ²¡æœ‰æ˜ç¡®è®­ç»ƒè¿‡çš„åˆ†ç±»ä»»åŠ¡ä¸Šè¿›è¡Œå›¾åƒåˆ†ç±»ã€‚
- å›¾åƒç”ŸæˆæŒ‡å¯¼ï¼šåœ¨ç”Ÿæˆä»»åŠ¡ä¸­ï¼ŒCLIPå¯ä»¥æä¾›ç›®æ ‡å›¾åƒçš„ç‰¹å¾æŒ‡å¯¼ï¼Œå¸®åŠ©ç”Ÿæˆé«˜è´¨é‡çš„å›¾åƒã€‚

#### ä»¤ç‰ŒåŒ–(Tokenization)
max 75ä¸ªä»¤ç‰Œ

#### åµŒå…¥/æ ‡ç­¾(Embedding)
ViT-L/14

#### VAEï¼ˆVariational Autoencoderï¼‰
VAEæ˜¯ä¸€ç§ç”Ÿæˆæ¨¡å‹ï¼Œç”¨äºå­¦ä¹ æ•°æ®çš„æ½œåœ¨è¡¨ç¤ºå¹¶ç”Ÿæˆæ–°æ•°æ®ã€‚VAEåŒ…æ‹¬ä¸¤ä¸ªä¸»è¦éƒ¨åˆ†ï¼š
1. ç¼–ç å™¨ï¼ˆEncoderï¼‰ï¼šå°†è¾“å…¥æ•°æ®ï¼ˆä¾‹å¦‚å›¾åƒï¼‰ç¼–ç ä¸ºæ½œåœ¨è¡¨ç¤ºï¼ˆé€šå¸¸æ˜¯ä¸€ä¸ªæ½œåœ¨å‘é‡ï¼‰ã€‚
2. è§£ç å™¨ï¼ˆDecoderï¼‰ï¼šä»æ½œåœ¨è¡¨ç¤ºä¸­é‡å»ºè¾“å…¥æ•°æ®ã€‚
VAEçš„è®­ç»ƒç›®æ ‡æ˜¯æœ€å¤§åŒ–å˜åˆ†ä¸‹ç•Œï¼ˆVariational Lower Boundï¼‰ï¼Œä»¥ä½¿é‡å»ºçš„å›¾åƒå°½å¯èƒ½æ¥è¿‘åŸå§‹å›¾åƒï¼Œå¹¶ä½¿æ½œåœ¨è¡¨ç¤ºçš„åˆ†å¸ƒæ¥è¿‘å…ˆéªŒåˆ†å¸ƒï¼ˆé€šå¸¸æ˜¯æ ‡å‡†æ­£æ€åˆ†å¸ƒï¼‰ã€‚

#### why VAE? not CLIP
CLIPå’ŒVAEçš„åŒºåˆ«

CLIPï¼ˆContrastive Language-Image Pre-Trainingï¼‰ï¼š

- åŠŸèƒ½ï¼šCLIPæ˜¯ä¸€ä¸ªç”¨äºç†è§£å›¾åƒå’Œæ–‡æœ¬ä¹‹é—´å…³ç³»çš„å¤šæ¨¡æ€æ¨¡å‹ã€‚å®ƒåŒ…å«ä¸¤ä¸ªéƒ¨åˆ†ï¼šå›¾åƒç¼–ç å™¨å’Œæ–‡æœ¬ç¼–ç å™¨ã€‚
- ä½œç”¨ï¼šCLIPç”¨äºè®¡ç®—å›¾åƒå’Œæ–‡æœ¬ä¹‹é—´çš„ç›¸ä¼¼æ€§ã€‚é€šè¿‡å¯¹æ¯”å­¦ä¹ ï¼ŒCLIPèƒ½å¤Ÿå°†å›¾åƒå’Œæ–‡æœ¬æ˜ å°„åˆ°åŒä¸€ä¸ªç‰¹å¾ç©ºé—´ä¸­ï¼Œä»è€Œå¯ä»¥è¿›è¡Œç›¸ä¼¼æ€§è¯„ä¼°ã€‚
é™åˆ¶ï¼šCLIPæ²¡æœ‰è§£ç å™¨éƒ¨åˆ†ï¼Œå› æ­¤æ— æ³•ç›´æ¥ç”Ÿæˆå›¾åƒã€‚å®ƒä¸»è¦ç”¨äºè¯„ä¼°å’ŒæŒ‡å¯¼ç”Ÿæˆè¿‡ç¨‹ï¼Œè€Œä¸æ˜¯ç›´æ¥å‚ä¸å›¾åƒç”Ÿæˆã€‚

VAEï¼ˆVariational Autoencoderï¼‰ï¼š

åŠŸèƒ½ï¼šVAEæ˜¯ä¸€ç§ç”Ÿæˆæ¨¡å‹ï¼Œç”¨äºå­¦ä¹ æ•°æ®çš„æ½œåœ¨è¡¨ç¤ºï¼Œå¹¶èƒ½å¤Ÿä»æ½œåœ¨è¡¨ç¤ºç”Ÿæˆæ–°æ•°æ®ã€‚VAEåŒ…å«ç¼–ç å™¨å’Œè§£ç å™¨ä¸¤éƒ¨åˆ†ã€‚
ä½œç”¨ï¼šåœ¨Stable Diffusionä¸­ï¼ŒVAEç”¨äºå°†å›¾åƒç¼–ç ä¸ºæ½œåœ¨å‘é‡ï¼ˆç¼–ç å™¨ï¼‰ï¼Œå¹¶ä»æ½œåœ¨å‘é‡ç”Ÿæˆå›¾åƒï¼ˆè§£ç å™¨ï¼‰ã€‚
ä¼˜åŠ¿ï¼šVAEçš„è§£ç å™¨éƒ¨åˆ†åœ¨å›¾åƒç”Ÿæˆè¿‡ç¨‹ä¸­èµ·å…³é”®ä½œç”¨ï¼Œèƒ½å¤Ÿä»æ‰©æ•£æ¨¡å‹ç”Ÿæˆçš„æ½œåœ¨è¡¨ç¤ºé‡å»ºå›¾åƒã€‚

ä¸ºä»€ä¹ˆä½¿ç”¨VAEè€Œä¸æ˜¯CLIPè¿›è¡Œè§£ç 

CLIPæ²¡æœ‰è§£ç å™¨éƒ¨åˆ†ï¼Œæ‰€ä»¥å®ƒä¸èƒ½ç›´æ¥ä»æ½œåœ¨è¡¨ç¤ºç”Ÿæˆå›¾åƒã€‚CLIPçš„ä¸»è¦ä½œç”¨æ˜¯æä¾›æ–‡æœ¬å’Œå›¾åƒä¹‹é—´çš„ç›¸ä¼¼æ€§æŒ‡å¯¼ã€‚ä¾‹å¦‚ï¼Œåœ¨ç”Ÿæˆè¿‡ç¨‹ä¸­ï¼ŒCLIPå¯ä»¥å¸®åŠ©ç¡®ä¿ç”Ÿæˆçš„å›¾åƒä¸ç»™å®šçš„æ–‡æœ¬æè¿°ç›¸ç¬¦ï¼Œä½†å®é™…çš„å›¾åƒç”Ÿæˆå’Œè§£ç è¿‡ç¨‹éœ€è¦ä¾èµ–å…¶ä»–æ¨¡å‹ï¼ˆå¦‚VAEï¼‰ã€‚

æ€»ç»“

åœ¨Stable Diffusionä¸­ï¼ŒVAEçš„è§£ç å™¨ç”¨äºå°†æ‰©æ•£æ¨¡å‹ç”Ÿæˆçš„æ½œåœ¨è¡¨ç¤ºè½¬åŒ–ä¸ºå›¾åƒã€‚CLIPåˆ™ç”¨äºæä¾›æ–‡æœ¬-å›¾åƒç›¸ä¼¼æ€§æŒ‡å¯¼ï¼Œç¡®ä¿ç”Ÿæˆçš„å›¾åƒç¬¦åˆæ–‡æœ¬æè¿°ã€‚ç”±äºCLIPç¼ºä¹è§£ç å™¨éƒ¨åˆ†ï¼ŒStable Diffusionä½¿ç”¨VAEæ¥å®Œæˆå›¾åƒçš„å®é™…ç”Ÿæˆã€‚

### Install

#### WebUI Linux
```shell
conda create -n AIGC python=3.10
conda activate AIGC
pip install torch==2.3.0+cu121 torchvision==0.18.0+cu121 torchaudio==2.3.0+cu121 --extra-index-url https://download.pytorch.org/whl/cu121

git clone https://github.com/AUTOMATIC1111/stable-diffusion-webui.git
#æ”¾å…¥æ¨¡å‹æ–‡ä»¶åˆ°models/StableDiffusion

cd repositories/
git clone https://github.com/AUTOMATIC1111/stable-diffusion-webui-assets.git
git clone https://github.com/Stability-AI/stablediffusion.git

cd ..
mkdir openai
mkdir openai/clip-vit-large-patch14
#æ”¾å…¥è¿™ä¸ªä»“åº“çš„æ¨¡å‹æ–‡ä»¶

CUDA_VISIBLE_DEVICES=0 python3 launch.py --listen --enable-insecure-extension-access --xformers

# å®‰è£…xformers
pip install xformers
CUDA_VISIBLE_DEVICES=0 python3 launch.py --listen --enable-insecure-extension-access --xformers


```

### ç‰ˆæœ¬
#### SD 3
[Paper](https://arxiv.org/pdf/2403.03206)

combines a [diffusion transformer](https://arxiv.org/abs/2212.09748) architecture and [flow matching](https://arxiv.org/abs/2210.02747). T5 æ˜¯ä¸€ä¸ªseq-to-seqæ¨¡å‹ã€‚

#### SDXL-Lightning
[HuggingFace](https://huggingface.co/ByteDance/SDXL-Lightning) | [Paper](https://arxiv.org/abs/2402.13929) ï¼ˆ2024.2ï¼‰

SDXL-Lightningæ˜¯ç”±å­—èŠ‚è·³åŠ¨å¼€å‘çš„ä¸€ä¸ªé¡¹ç›®ï¼Œé‡‡ç”¨äº†åˆ›æ–°çš„è’¸é¦ç­–ç•¥ï¼Œä¼˜åŒ–äº†æ‰©æ•£æ¨¡å‹ï¼Œå®ç°äº†ä»æ–‡æœ¬åˆ°é«˜åˆ†è¾¨ç‡å›¾åƒçš„å¿«é€Ÿã€é«˜è´¨é‡ç”Ÿæˆã€‚

åœ¨WebUI 1.9ç‰ˆæœ¬ä¸­ï¼Œé›†æˆäº†SDXL-Lightningä½¿ç”¨çš„sgm_uniformé‡‡æ ·å™¨ï¼ˆ2024.4ï¼‰

#### SDXL Turbo
[HuggingFace](https://huggingface.co/stabilityai/sdxl-turbo) | [Paper](https://stability.ai/research/adversarial-diffusion-distillation) (2023.12)
SD Turboçš„å¤§å·ç‰ˆ(é«˜è´¨é‡)
- å°ºå¯¸: 512x512(fix)

#### SD Turbo
[HuggingFace](https://huggingface.co/stabilityai/sd-turbo) (2023.12)
ç”±SD2.1å¾®è°ƒè€Œæ¥
- å°ºå¯¸: 512x512(fix)

#### SDXL
[stable-diffusion-xl-base-1.0](https://huggingface.co/stabilityai/stable-diffusion-xl-base-1.0)

- DreamShaper XL Alpha 2
[Model](https://civitai.com/models/112902?modelVersionId=126688)

åœ¨æ¨¡å‹é¡µé¢ä¸Šå¯ä»¥çœ‹åˆ°ï¼Œå…¶ä½¿ç”¨çš„Basemodel æ˜¯SDXL 1.0

What does it do better than SDXL1.0?
- No need for refiner. Just do highres fix (upscale+i2i)
- Better looking people
- Less blurry edges
- 75% better dragons ğŸ‰
- Better NSFW


#### SDXL 0.9
[HuggingFace](https://huggingface.co/stabilityai/stable-diffusion-xl-base-0.9) | [Paper](https://arxiv.org/abs/2307.01952)
ä½¿ç”¨ä¸åŒå°ºå¯¸çš„å›¾åƒè®­ç»ƒï¼ˆæœ€é«˜1024x1024)


<div class="theme-image">
  <img src="./assets/SDXL_pipeline.png" alt="Light Mode Image" class="light-mode">
  <img src="./assets/dark_SDXL_pipeline.png" alt="Dark Mode Image" class="dark-mode">
</div>

SDXLæ¨¡å‹æ›´å¤æ‚ä¸€äº›ï¼Œé™¤äº†Baseæ¨¡å‹ä»¥å¤–è¿˜åŒ…å«Refineræ¨¡å‹ï¼ˆä¸¤ä¸ªU-Net?ï¼‰

#### SD 2.0/2.1
[SD2.1 HuggingFace](https://huggingface.co/stabilityai/stable-diffusion-2-1) (2.1:2022.12, 2.0: 2022.11)

[SD2.1 Base HuggingFace](https://huggingface.co/stabilityai/stable-diffusion-2-1-base)
- Hardware: 32 x 8 x A100 GPUs
- å°ºå¯¸: 768x768(SD 2.1-v), 512x512(SD 2.1-base)

#### SD 1.5
[Code](https://huggingface.co/runwayml/stable-diffusion-v1-5)(2022.10)

è¿™æ˜¯SD 1æœ€åä¸€ä¸ªç‰ˆæœ¬ï¼ˆæˆªæ­¢åˆ°2024.3ï¼‰ï¼Œå¦‚æœä½ çœ‹åˆ°ä»€ä¹ˆSD1.8ï¼Œé‚£è‚¯å®šæ˜¯é‚£ä¸ªäººæ²¡åˆ†æ¸…WebUIç‰ˆæœ¬å’Œæ¨¡å‹ç‰ˆæœ¬ã€‚

#### SD 1.1-1.4
[Code](https://github.com/CompVis/stable-diffusion)(2022.8)
- å°ºå¯¸: 512x512

### é‡‡æ ·å™¨
#### åˆ†ç±»

| æ—§é‡‡æ ·å™¨                                                          | DPMé‡‡æ ·å™¨                                                                                                             | æ–°é‡‡æ ·å™¨
|---------------------------------------------------------------|--------------------------------------------------------------------------------------------------------------------|------------------|
| LMS<br>LMS Karras<br>Heun<br>Euler<br>Euler a<br>DDIM<br>PLMS | DPM2<br>DPM2a<br>DPM++2S a<br>DPM++2M<br>DPM++SDE<br>DPM++2M SDE<br>DPM++3M SDE<br>...<br>DPM fast<br>DPM adaptive | UniPC<br>Restart |

#### æ—§é‡‡æ ·å™¨
æ¨èEuler, Euler a

aä»£è¡¨ç¥–å…ˆé‡‡æ ·å™¨ï¼Œ(è¿­ä»£å™ªå£°)ä¸æ”¶æ•›

#### DPMé‡‡æ ·å™¨
- **DPM++2M Karras**
æœ€æ¨èç®—æ³•ï¼Œæ”¶æ•›ï¼Œé€Ÿåº¦å¿«ï¼Œè´¨é‡OK

- DPM++SDE Karras
éšæœºå¾®åˆ†æ–¹ç¨‹ç®—æ³•

- DPM++2M SDE Exponential(WebUI1.6 æ–°å¢)
æŒ‡æ•°ç®—æ³•ï¼Œä¸æ”¶æ•›ï¼Œç»†èŠ‚å°‘äº›ï¼Œç”»é¢æŸ”å’Œã€å¹²å‡€

- DPM++3M SDE Karas(WebUI1.6 æ–°å¢)
é€Ÿåº¦å’Œ2Mä¸€æ ·ï¼Œéœ€è¦æ›´å¤šé‡‡æ ·æ­¥æ•°ï¼Œè°ƒä½CFGï¼Œé‡‡æ ·æ­¥æ•°>30æ­¥æ•ˆæœæ›´å¥½

- DPM++3M SDE Exponential(WebUI1.6 æ–°å¢)
åŒä¸Š

#### æ–°é‡‡æ ·å™¨
- UniPC (2023)
ç»Ÿä¸€é¢„æµ‹æ ¡æ­£å™¨ï¼Œå…¼å®¹æ€§å¾ˆå¥½æ”¶æ•›ï¼Œ10æ­¥å·¦å³å°±èƒ½ç”Ÿæˆå¯ç”¨ç”»é¢ã€‚

- Restart(WebUI1.6 æ–°å¢)
æ¯æ­¥æ¸²æŸ“é•¿äº›ï¼Œä½†åªéœ€å¾ˆå°‘çš„é‡‡æ ·æ­¥æ•°

åœ¨WebUIä¸­å¯ä»¥åœ¨ã€è®¾ç½®ã€‘-ã€Sampler parametersã€‘è®¾å®šæ˜¾ç¤ºå’Œéšè—é‡‡æ ·å™¨

#### å®ä¾‹
é‡‡æ ·å™¨çš„å…·ä½“å®ç°ä»£ç å¯å‚è€ƒ https://github.com/crowsonkb/k-diffusion.git çš„`k_diffusion/sampling.py`
### Comfy UI

#### å…±äº«è·¯å¾„è®¾ç½®
ä¸ªäººä¹ æƒ¯å°†æ¨¡å‹è·¯å¾„è®¾å®šä¸ºä¸€ä¸ªç»Ÿä¸€çš„è·¯å¾„ï¼Œä½¿ä»»ä½•å¹³å°çš„WebUIå’ŒComfyUIéƒ½ç”¨åŒä¸€è·¯å¾„ä¸‹çš„æ¨¡å‹ï¼ŒèŠ‚çœç©ºé—´ã€‚

- Windowsç‰ˆ: ä¿®æ”¹`ComgyUI/folder_paths.py`
```python
import os
import time
import logging

supported_pt_extensions = set(['.ckpt', '.pt', '.bin', '.pth', '.safetensors', '.pkl'])

folder_names_and_paths = {}

base_path = os.path.dirname(os.path.realpath(__file__))
models_dir = os.path.join(base_path, "models")
share_path = "C:\Software\ShareModel" # [!code ++]
folder_names_and_paths["checkpoints"] = ([os.path.join(models_dir, "checkpoints")], supported_pt_extensions) # [!code --]
folder_names_and_paths["checkpoints"] = ([os.path.join(share_path, "stablediffusion")], supported_pt_extensions) # [!code ++]
folder_names_and_paths["configs"] = ([os.path.join(models_dir, "configs")], [".yaml"])

folder_names_and_paths["loras"] = ([os.path.join(models_dir, "loras")], supported_pt_extensions) # [!code --]
folder_names_and_paths["loras"] = ([os.path.join(share_path, "loras")], supported_pt_extensions) # [!code ++]
folder_names_and_paths["vae"] = ([os.path.join(models_dir, "vae")], supported_pt_extensions)
folder_names_and_paths["clip"] = ([os.path.join(models_dir, "clip")], supported_pt_extensions)
folder_names_and_paths["unet"] = ([os.path.join(models_dir, "unet")], supported_pt_extensions)
folder_names_and_paths["clip_vision"] = ([os.path.join(models_dir, "clip_vision")], supported_pt_extensions)
folder_names_and_paths["style_models"] = ([os.path.join(models_dir, "style_models")], supported_pt_extensions)
folder_names_and_paths["embeddings"] = ([os.path.join(models_dir, "embeddings")], supported_pt_extensions)
folder_names_and_paths["diffusers"] = ([os.path.join(models_dir, "diffusers")], ["folder"])
folder_names_and_paths["vae_approx"] = ([os.path.join(models_dir, "vae_approx")], supported_pt_extensions)

#folder_names_and_paths["controlnet"] = ([os.path.join(models_dir, "controlnet"), os.path.join(models_dir, "t2i_adapter")], supported_pt_extensions)
folder_names_and_paths["controlnet"] = ([os.path.join(share_path, "controlnet"), os.path.join(models_dir, "t2i_adapter")], supported_pt_extensions)
```

#### comfyui-various
https://github.com/jamesWalker55/comfyui-various/tree/main

`match`è¯­å¥åœ¨Python3.10ç‰ˆæœ¬ä»¥ä¸‹å¼•å‘çš„æŠ¥é”™`SyntaxError: invalid syntax`é—®é¢˜è§£å†³æ–¹æ¡ˆï¼š

åœ¨`comfyui_primitive_ops.py`çš„`line280`ä¸­
```python
    if from_right:
        splits = source.rsplit(split_by, 1)
    else:
        splits = source.split(split_by, 1)
    #match splits:
    #    case a, b:
    #        return (a, b)
    #    case a:
    #        return (a, "")
    # æ£€æŸ¥ splits æ˜¯å¦æ˜¯ä¸€ä¸ªåŒ…å«ä¸¤ä¸ªå…ƒç´ çš„åºåˆ—
    if isinstance(splits, (list, tuple)) and len(splits) == 2:
        a, b = splits
        return (a, b)
    # æ£€æŸ¥ splits æ˜¯å¦æ˜¯ä¸€ä¸ªåŒ…å«ä¸€ä¸ªå…ƒç´ çš„åºåˆ—
    elif isinstance(splits, (list, tuple)) and len(splits) == 1:
        a = splits[0]
        return (a, "")
    else:
        return ("Invalid input",)
```

#### WD14 Tagger
https://github.com/pythongosssss/ComfyUI-WD14-Tagger?tab=readme-ov-file

Waifu Diffusion 1.4 Taggerï¼Œè¿™æ˜¯ä¸€ä¸ªç”¨äºè‡ªåŠ¨åŒ–å›¾åƒæ ‡ç­¾ç”Ÿæˆçš„å·¥å…·ï¼Œä¸“é—¨ä¸ºåŠ¨æ¼«é£æ ¼å›¾åƒï¼ˆé€šå¸¸ç§°ä¸º "[waifu](https://zh.wiktionary.org/wiki/waifu)" å›¾åƒï¼‰è®¾è®¡çš„ã€‚è¿™ä¸ªå·¥å…·åŸºäºæ·±åº¦å­¦ä¹ æ¨¡å‹ï¼Œèƒ½å¤Ÿä¸ºç»™å®šçš„åŠ¨æ¼«å›¾åƒç”Ÿæˆæè¿°æ€§æ ‡ç­¾ï¼Œä»¥ä¾¿äºåˆ†ç±»ã€æœç´¢å’Œå…¶ä»–ç”¨é€”ã€‚

ä¸»è¦åŠŸèƒ½
1. è‡ªåŠ¨æ ‡ç­¾ç”Ÿæˆï¼š
   - WD 1.4 Tagger ä½¿ç”¨é¢„è®­ç»ƒçš„æ·±åº¦å­¦ä¹ æ¨¡å‹æ¥åˆ†æè¾“å…¥çš„åŠ¨æ¼«å›¾åƒï¼Œå¹¶è‡ªåŠ¨ç”Ÿæˆä¸€ç»„æè¿°æ€§æ ‡ç­¾ã€‚è¿™äº›æ ‡ç­¾å¯ä»¥åŒ…æ‹¬è§’è‰²çš„å¤–è²Œç‰¹å¾ã€æœè£…ã€åŠ¨ä½œã€èƒŒæ™¯ç­‰ã€‚
2. é«˜æ•ˆçš„åŠ¨æ¼«å›¾åƒå¤„ç†ï¼š
   - è¯¥å·¥å…·ä¸“é—¨é’ˆå¯¹åŠ¨æ¼«å›¾åƒè¿›è¡Œäº†ä¼˜åŒ–ï¼Œèƒ½å¤Ÿè¯†åˆ«å’Œç”Ÿæˆé«˜è´¨é‡çš„æ ‡ç­¾ï¼Œä½¿å¾—ç®¡ç†å’Œåˆ†ç±»å¤§é‡çš„åŠ¨æ¼«å›¾åƒå˜å¾—æ›´åŠ å®¹æ˜“ã€‚
3. é›†æˆåˆ° ComfyUIï¼š
   - åœ¨ ComfyUI ä¸­ï¼ŒWD 1.4 Tagger å¯ä»¥æ— ç¼é›†æˆåˆ°ç”¨æˆ·ç•Œé¢ä¸­ï¼Œä½¿å¾—ç”¨æˆ·èƒ½å¤Ÿè½»æ¾åœ°ä¸ºä»–ä»¬çš„å›¾åƒç”Ÿæˆæ ‡ç­¾ï¼Œå¹¶ä½¿ç”¨è¿™äº›æ ‡ç­¾è¿›è¡Œæœç´¢å’Œè¿‡æ»¤ã€‚

æ¨¡å‹ä¸‹è½½ï¼šhttps://huggingface.co/SmilingWolf

**Offline Use**

Simplest way is to use it online, interrogate an image, and the model will be downloaded and cached, however if you want to manually download the models:

- Create a `models` folder (in same folder as the `wd14tagger.py`)
- Use URLs for models from the list in `pysssss.json`
- Download `model.onnx` and name it with the model name e.g. `wd-v1-4-convnext-tagger-v2.onnx`
- Download `selected_tags.csv` and name it with the model name e.g. `wd-v1-4-convnext-tagger-v2.csv`

requirements
```text
flax
wandb
clu
```

TypeError: Descriptors cannot be created directly.

If this call came from a _pb2.py file, your generated code is out of date and must be regenerated with protoc >= 3.19.0.
```text
pip install protobuf==3.20.*
```
#### IPAdapter plus
https://github.com/cubiq/ComfyUI_IPAdapter_plus

ä¸‹è½½æ¨¡å‹ï¼š
- `/ComfyUI/models/clip_vision`
    - [CLIP-ViT-H-14-laion2B-s32B-b79K.safetensors](https://huggingface.co/h94/IP-Adapter/resolve/main/models/image_encoder/model.safetensors), download and rename
    - [CLIP-ViT-bigG-14-laion2B-39B-b160k.safetensors](https://huggingface.co/h94/IP-Adapter/resolve/main/sdxl_models/image_encoder/model.safetensors), download and rename
- `/ComfyUI/models/ipadapter`, create it if not present
    - [ip-adapter_sd15.safetensors](https://huggingface.co/h94/IP-Adapter/resolve/main/models/ip-adapter_sd15.safetensors), Basic model, average strength
    - [ip-adapter_sd15_light_v11.bin](https://huggingface.co/h94/IP-Adapter/resolve/main/models/ip-adapter_sd15_light_v11.bin), Light impact model
    - [ip-adapter-plus_sd15.safetensors](https://huggingface.co/h94/IP-Adapter/resolve/main/models/ip-adapter-plus_sd15.safetensors), Plus model, very strong
    - [ip-adapter-plus-face_sd15.safetensors](https://huggingface.co/h94/IP-Adapter/resolve/main/models/ip-adapter-plus-face_sd15.safetensors), Face model, portraits
    - [ip-adapter-full-face_sd15.safetensors](https://huggingface.co/h94/IP-Adapter/resolve/main/models/ip-adapter-full-face_sd15.safetensors), Stronger face model, not necessarily better
    - [ip-adapter_sd15_vit-G.safetensors](https://huggingface.co/h94/IP-Adapter/resolve/main/models/ip-adapter_sd15_vit-G.safetensors), Base model, **requires bigG clip vision encoder**
    - [ip-adapter_sdxl_vit-h.safetensors](https://huggingface.co/h94/IP-Adapter/resolve/main/sdxl_models/ip-adapter_sdxl_vit-h.safetensors), SDXL model
    - [ip-adapter-plus_sdxl_vit-h.safetensors](https://huggingface.co/h94/IP-Adapter/resolve/main/sdxl_models/ip-adapter-plus_sdxl_vit-h.safetensors), SDXL plus model
    - [ip-adapter-plus-face_sdxl_vit-h.safetensors](https://huggingface.co/h94/IP-Adapter/resolve/main/sdxl_models/ip-adapter-plus-face_sdxl_vit-h.safetensors), SDXL face model
    - [ip-adapter_sdxl.safetensors](https://huggingface.co/h94/IP-Adapter/resolve/main/sdxl_models/ip-adapter_sdxl.safetensors), vit-G SDXL model, **requires bigG clip vision encoder**
    - **Deprecated** [ip-adapter_sd15_light.safetensors](https://huggingface.co/h94/IP-Adapter/resolve/main/models/ip-adapter_sd15_light.safetensors), v1.0 Light impact model


#### controlnet-aux
ä¸‹è½½æ¨¡å‹ https://huggingface.co/lllyasviel/Annotators/tree/main åˆ°ï¼š
`ComfyUI/custom_nodes/comfyui_controlnet_aux/ckpts/lllyasviel/Annotators`

### ä¼˜åŒ–åŠ é€Ÿ
Xformerså®‰è£…ï¼š https://post.smzdm.com/p/axzmd56d/
bash webui.sh --xformers
or
 CUDA_VISIBLE_DEVICES=7 python3 launch.py --listen --enable-insecure-extension-access --xformers

åŠ é€Ÿæ•ˆæœ
OneFlow > [TensorRT](https://docs.nvidia.com/deeplearning/tensorrt/archives/index.html) > Aitemplate > Xformers

LLM: https://latent-consistency-models.github.io/

#### æµ‹è¯•
ç”Ÿæˆå›¾ç‰‡çš„å¤§å°ä¸º`(450,800)`ï¼Œåœ¨ä½¿ç”¨TensorRTæ—¶ï¼Œç”Ÿæˆå›¾ç‰‡çš„å¤§å°ä¸º`(768,450)`

| ç¯å¢ƒ | é¢éƒ¨ä¿®å¤ | LoRA | é€Ÿåº¦ | +Xformersé€Ÿåº¦ | +TensorRTé€Ÿåº¦ |
| --- |:---:|:---:| --- | --- | --- |
| 3060(Notebook) | &#x2713; |   | 3.65it/s | 4.80it/s ||
| 4090 | &#x2713; |   | 7.80it/s | 9.63it/s |32.83it/s|
| 3060(Notebook) | &#x2713;  | &#x2713;  |	3.16it/s | 3.96it/s ||
| 4090 | &#x2713;  | &#x2713;  | 5.54it/s | 6.25it/s |32.87it/s|
| 4090 |  |   | 8.14it/s | 10.06it/s |37.25it/s|

### Embeddings
- ConfyUI:
https://comfyanonymous.github.io/ComfyUI_examples/textual_inversion_embeddings/

To use an embedding put the file in the `models/embeddings` folder then use it in your prompt like I used the SDA768.pt embedding in the previous picture.

Note that you can omit the filename extension so these two are equivalent:

`embedding:SDA768.pt`

`embedding:SDA768`

You can also set the strength of the embedding just like regular words in the prompt:

`(embedding:SDA768:1.2)`

- WebUI
https://zhuanlan.zhihu.com/p/627500143

å°†embeddingæ–‡ä»¶ä¸‹è½½ï¼Œæ‹·è´è‡³æ ¹ç›®å½•ä¸‹çš„embeddingç›®å½•é‡Œ

ä½¿ç”¨ï¼šç›´æ¥åœ¨prompté‡Œè¾“å…¥embeddingçš„åå­—å³å¯ï¼Œä¸éœ€è¦å†™åç¼€ã€‚æ–°ç‰ˆæœ¬çš„WebUIä¼šè‡ªåŠ¨è¯†åˆ«embeddingï¼Œé€‰æ‹©å¯è‡ªåŠ¨å¡«å……prompt

### Controlnet

#### Openpose
è¿™é‡Œçš„Openposeæ˜¯æŒ‡å€ŸåŠ©å®ƒæå–keypointç‰¹å¾ï¼Œè€Œä½¿ç”¨[Openpose Editor](https://github.com/fkunn1326/openpose-editor)ç¼–è¾‘å‡ºæ¥çš„éª¨æ¶å¦‚æœæ²¡æœ‰è¾“å…¥å›¾åƒå‚è€ƒï¼Œåˆ™æ²¡æœ‰ç”¨åˆ°Openpose

åœ¨ https://huggingface.co/lllyasviel/Annotators/tree/main ä¸­ä¸‹è½½3ä¸ªæ¨¡å‹æ”¾å…¥`extensions/sd-webui-controlnet/annotator/downloads/openpose`ä¸­ï¼š

- `body_pose_model.pth`
- `facenet.pth`
- `hand_pose_model.pth`

### è®­ç»ƒ

#### LoRA

1. å‡†å¤‡æ•°æ®: å‡†å¤‡è‡³å°‘10å¼ å›¾åƒï¼Œå¦‚æœæ˜¯äººï¼Œé‚£ä¹ˆèƒŒæ™¯å°½é‡ä¸ºç™½è‰²ï¼Œä¸ç„¶ä¼šè¢«AIå­¦ä¹ åˆ°èƒŒæ™¯ã€‚æ”¾å…¥ã€æ–‡ä»¶å¤¹Aã€‘

2. æ‰“tag: ä½¿ç”¨SD WebUIï¼Œç‚¹ã€è®­ç»ƒã€‘-ã€é¢„å¤„ç†ã€‘

   - å…¶ä¸­æºç›®å½•è¾“å…¥ã€æ–‡ä»¶å¤¹Aã€‘ï¼Œåˆ›å»ºä¸€ä¸ªæ–°ç›®å½•ã€æ–‡ä»¶å¤¹Bã€‘è®¾ä¸ºç›®æ ‡ç›®å½•

   - è‡ªåŠ¨ç„¦ç‚¹è£åˆ‡ï¼Œä½¿ç”¨deepbooruç”Ÿæˆè¯´æ˜æ–‡å­—(tags)

   - è®¾ç½®å®Œåç‚¹ã€è¾“å‡ºã€‘

   - æ‰‹åŠ¨ä¿®æ”¹ä¸æ­£ç¡®çš„æ ‡ç­¾ï¼ˆåœ¨ã€æ–‡ä»¶å¤¹Bã€‘çš„`.txt`æ–‡ä»¶ä¸­ï¼Œå¯ä½¿ç”¨[GUIå·¥å…·](https://github.com/cangzihan/sd_lazy_editor/blob/main/webui.py)ï¼‰

3. æ•°æ®é›†æ ¼å¼ï¼šåˆ›å»ºä¸€ä¸ªæ–°æ–‡ä»¶å¤¹ã€æ–‡ä»¶å¤¹Cã€‘ï¼Œç„¶ååœ¨é‡Œé¢å†åˆ›å»ºä¸€ä¸ªã€æ–‡ä»¶å¤¹Dã€‘å‘½åä¸º"æ•°å­—_åç§°"ï¼Œå¦‚â€œ10_faceâ€ã€‚å…¶ä¸­æ•°å­—ä»£è¡¨è®­ç»ƒæ¬¡æ•°ã€‚
ç„¶åæŠŠã€æ–‡ä»¶å¤¹Bã€‘ä¸­çš„æ‰€æœ‰æ–‡ä»¶æ”¾è¿›å»ã€‚

4. è®­ç»ƒ
```shell
git clone https://github.com/Akegarasu/lora-scripts.git
```

ä¿®æ”¹`train.ps1`ä¸­çš„å†…å®¹ï¼ˆä»£ç æ³¨é‡Šå·²ç»å¾ˆæ¸…æ¥šäº†ï¼‰

| å˜é‡                  | è¯´æ˜   |
|---------------------|------|
| `pretrained_model`  |      |
| `train_data_dir`    | æ”¹ä¸ºã€æ–‡ä»¶å¤¹Cã€‘ |
| `max_train_epoches` | æ”¹ä¸º14 |
| `output_name`       |      |

ç„¶åè¿è¡Œå®ƒ
```shell
# chmod a+x train.ps1
./train.ps1
```

### LCM
[Project](https://latent-consistency-models.github.io/) | [Paper](https://arxiv.org/abs/2310.04378) | [Code](https://github.com/luosiallen/latent-consistency-model) | [Model](https://huggingface.co/SimianLuo/LCM_Dreamshaper_v7)

LCM-LoRA: [Paper](https://arxiv.org/abs/2311.05556) | [Model](https://huggingface.co/latent-consistency/lcm-lora-sdv1-5)

LCM(Latent Consistency Models)æ˜¯æ¸…åå¤§å­¦æå‡ºçš„å¿«é€Ÿæ–‡ç”Ÿå›¾æ¨¡å‹ï¼Œ LCMå¯ä»¥è®©è¿­ä»£æ­¥æ•°è¿›ä¸€æ­¥å‡å°‘åˆ°7

#### ä½¿ç”¨æ–¹æ³•
- WebUI: https://www.bilibili.com/video/BV1Q94y1E7uc
- ComfyUI: https://www.bilibili.com/video/BV1D94y1P7FM

å‚æ•°ï¼šstep:4 cfg:1.5 é‡‡æ ·å™¨:lcm

å¯ä»¥é…åˆAnimateDiffä½¿ç”¨

### SDXL-Lightning

#### ä½¿ç”¨æ–¹æ³•
åœ¨å®˜æ–¹Model Cardä¸Šæœ‰ComfyUI workflowä¾›ä¸‹è½½

1. ç›´æ¥ä½¿ç”¨å¤§æ¨¡å‹
Checkpoint: sdxl_lighting_4step.safetensors
Latent Image: 1024, 1024
steps: 4
cfg: 1.0
sampler: euler
scheduler: sgm_uniform

2. ä½¿ç”¨LoRA(å¦‚æœä½¿ç”¨çš„æ˜¯non-SDXL base models)

3. 1-stepæ¨¡å‹ï¼ˆä¸ç¨³å®šï¼‰

(æ¨¡å‹åé¢çš„1-step, 2-step, 4-step, and 8-step æ ‡æ³¨ä»£è¡¨å®ƒèƒ½åœ¨è¿™äº›æ­¥éª¤ç”»å›¾)


- DreamShaper XL - Lightning DPM++ SDE

[Model](https://civitai.com/models/112902?modelVersionId=354657)

DreamShaper is a general purpose SD model that aims at doing everything well, photos, art, anime, manga. It's designed to go against other general purpose models and pipelines like Midjourney and DALL-E.

åœ¨æ¨¡å‹é¡µé¢ä¸Šå¯ä»¥çœ‹åˆ°ï¼Œå…¶ä½¿ç”¨çš„Basemodel æ˜¯SDXL lighting

### SDXL Inpainting
```shell
pip install diffusers
```

#### Deploy
ä¸‹è½½diffusers/stable-diffusion-xl-1.0-inpainting-0.1çš„å…¨éƒ¨æ–‡ä»¶ï¼Œå’Œstabilityai/stable-diffusion-xl-base-1.0çš„`scheduler`æ–‡ä»¶å¤¹é‡Œçš„æ–‡ä»¶å³å¯ï¼Œåˆ†åˆ«æ”¾å…¥ä¸¤ä¸ªè·¯å¾„ä¸‹
```python
from diffusers import AutoPipelineForInpainting
import diffusers
import torch

from PIL import Image

prompt = "gray laptop on the table"
guidance_scale = 7.5
steps = 20
strength = 0.99

#print(type(image))  # PIL Image


device = "cuda" if torch.cuda.is_available() else "cpu"
print("Using device", device)
pipe = AutoPipelineForInpainting.from_pretrained("models/diffusers/stable-diffusion-xl-1.0-inpainting-0.1", torch_dtype=torch.float16, variant="fp16").to(device)


def sdxl_inpaint(image, mask, negative_prompt, scheduler):
    if negative_prompt == "":
        negative_prompt = None
    scheduler_class_name = scheduler.split("-")[0]

    add_kwargs = {}
    if len(scheduler.split("-")) > 1:
        add_kwargs["use_karras"] = True
    if len(scheduler.split("-")) > 2:
        add_kwargs["algorithm_type"] = "sde-dpmsolver++"

    scheduler = getattr(diffusers, scheduler_class_name)
    pipe.scheduler = scheduler.from_pretrained("models/stabilityai/stable-diffusion-xl-base-1.0", subfolder="scheduler", **add_kwargs)

    init_image = image.convert("RGB").resize((1024, 1024))
    mask = mask.convert("RGB").resize((1024, 1024))

    output = pipe(prompt=prompt, negative_prompt=negative_prompt, image=init_image, mask_image=mask,
                  guidance_scale=guidance_scale, num_inference_steps=int(steps), strength=strength)

    out_pil = output.images[0]

    # è·å–å›¾åƒçš„å®½åº¦å’Œé«˜åº¦
    width, height = image.size
    resized_image = out_pil.resize((width, height))
    return resized_image


if __name__ == "__main__":
    image = Image.open("IMG_1024_540.png")
    mask = Image.open("mask.png")
    negative_prompt = "QR code"
    scheduler = "EulerDiscreteScheduler"
    image_new = sdxl_inpaint(image, mask, negative_prompt, scheduler)
    image_new.save('edit.png', format='PNG')

```

### ä»£ç åˆ†C

#### WebUI
è¿™é‡Œä»¥1.9.4ç‰ˆæœ¬ä¸ºå‡†

UIéƒ¨åˆ†åœ¨`modules/ui.py`
- create_ui()
  - with gr.Blocks(analytics_enabled=False) as txt2img_interface: ......
  - with gr.Blocks(analytics_enabled=False) as img2img_interface: ......
  - with gr.Blocks(analytics_enabled=False) as pnginfo_interface: ......
  - with gr.Blocks(analytics_enabled=False) as train_interface: ......
  - ã€æ±‰åŒ–ã€‘interfaces = [(txt2img_interface, "txt2img", "txt2img")......]
  - interfaces += script_callbacks.ui_tabs_callback()
  - ã€ç½‘é¡µæ ‡é¢˜ã€‘with gr.Blocks(theme, ..., title="Stable Diffusion") as demo: ......

éƒ¨åˆ†ç»„ä»¶åœ¨`modules/ui_toprow.py`ä¸­

æ–‡ç”Ÿå›¾éƒ¨åˆ†åœ¨`modules/txt2img.py`çš„`txt2img()`å‡½æ•°ä¸­ï¼Œ`modules.scripts.scripts_txt2img.run`å®é™…ä¸Šæ˜¯è¿è¡Œäº†`txt2img_create_processing`

::: details `from contextlib import closing`æ˜¯åšä»€ä¹ˆçš„?

from contextlib import closingæ˜¯Pythonæ ‡å‡†åº“ä¸­çš„ä¸€ä¸ªæ¨¡å—ï¼Œå®ƒæä¾›äº†ä¸€ä¸ªä¸Šä¸‹æ–‡ç®¡ç†å™¨ï¼Œç”¨äºç¡®ä¿æŸä¸ªèµ„æºçš„æ­£ç¡®æ‰“å¼€å’Œå…³é—­ã€‚

å½“ä½¿ç”¨closing()å‡½æ•°æ—¶ï¼Œæˆ‘ä»¬å¯ä»¥å°†éœ€è¦æ‰“å¼€ã€ä½¿ç”¨åéœ€è¦å…³é—­çš„èµ„æºä½œä¸ºå‚æ•°ä¼ é€’ç»™å®ƒã€‚è¿™æ ·ï¼Œå½“ä»£ç å—æ‰§è¡Œå®Œæ¯•åï¼Œclosing()å‡½æ•°ä¼šè‡ªåŠ¨è°ƒç”¨è¯¥èµ„æºçš„å…³é—­æ–¹æ³•ï¼Œç¡®ä¿èµ„æºå¾—åˆ°æ­£ç¡®åœ°é‡Šæ”¾ã€‚

è¿™ä¸ªæ¨¡å—çš„å¥½å¤„æ˜¯ï¼Œå®ƒå¯ä»¥ç¡®ä¿åœ¨æ‰“å¼€èµ„æºåï¼Œæ— è®ºä»£ç æ‰§è¡Œæ˜¯å¦æ­£å¸¸ï¼Œéƒ½èƒ½ç¡®ä¿èµ„æºè¢«æ­£ç¡®åœ°å…³é—­ã€‚ä¾‹å¦‚ï¼Œåœ¨æ‰“å¼€æ–‡ä»¶æ—¶ï¼Œå¦‚æœæ–‡ä»¶æ“ä½œå‡ºé”™ï¼Œé‚£ä¹ˆclosing()å‡½æ•°å°±ä¼šç¡®ä¿æ–‡ä»¶è¢«å…³é—­ã€‚

ä»¥ä¸‹æ˜¯ä¸€ä¸ªä½¿ç”¨`closing()`å‡½æ•°çš„ä¾‹å­ï¼š

```python
from contextlib import closing

with closing(open('example.txt', 'r')) as f:
    data = f.read()
    # ä½¿ç”¨æ–‡ä»¶æ•°æ®...
```
åœ¨è¿™ä¸ªä¾‹å­ä¸­ï¼Œ`open()`å‡½æ•°æ‰“å¼€äº†ä¸€ä¸ªæ–‡ä»¶ï¼Œå¹¶è¿”å›äº†ä¸€ä¸ªæ–‡ä»¶å¯¹è±¡ã€‚ç„¶åï¼Œ`closing()`å‡½æ•°æ¥æ”¶è¿™ä¸ªæ–‡ä»¶å¯¹è±¡ä½œä¸ºå‚æ•°ï¼Œå¹¶å°†å…¶ä½œä¸ºä¸€ä¸ªä¸Šä¸‹æ–‡ç®¡ç†å™¨ã€‚åœ¨è¿™ä¸ªä¸Šä¸‹æ–‡ç®¡ç†å™¨ä¸­ï¼Œæˆ‘ä»¬è¯»å…¥äº†æ–‡ä»¶çš„æ•°æ®ï¼Œå¹¶ä½¿ç”¨è¿™äº›æ•°æ®è¿›è¡Œä¸€äº›æ“ä½œã€‚å½“ä»£ç å—æ‰§è¡Œå®Œæ¯•åï¼Œclosing()å‡½æ•°ä¼šè‡ªåŠ¨è°ƒç”¨f.close()æ–¹æ³•ï¼Œç¡®ä¿æ–‡ä»¶è¢«æ­£ç¡®åœ°å…³é—­ã€‚

```python
    p = txt2img_create_processing(id_task, request, *args)

    with closing(p):
        processed = modules.scripts.scripts_txt2img.run(p, *p.script_args)

        if processed is None:
            processed = processing.process_images(p)
```
åœ¨è¿™é‡Œï¼Œ`p`æ˜¯ä¸€ä¸ªå¯èƒ½éœ€è¦å…³é—­çš„èµ„æºã€‚ä½¿ç”¨with closing(p)å¯ä»¥ç¡®ä¿åœ¨ä»£ç å—æ‰§è¡Œå®Œæ¯•åï¼Œä¼šè°ƒç”¨pçš„close()æ–¹æ³•ã€‚è¿™å¯¹äºæ–‡ä»¶ã€ç½‘ç»œè¿æ¥ã€æ•°æ®åº“è¿æ¥ç­‰èµ„æºç®¡ç†éå¸¸æœ‰ç”¨ã€‚

ä»¥ä¸‹æ˜¯`closing`çš„å®ç°ï¼Œå¯ä»¥å¸®åŠ©ä½ æ›´å¥½åœ°ç†è§£å®ƒçš„å·¥ä½œåŸç†ï¼š
```python
from contextlib import closing

class ClosingExample:
    def close(self):
        print("Resource has been closed")

# ä½¿ç”¨ç¤ºä¾‹
example = ClosingExample()

with closing(example):
    print("Using the resource")
# è¾“å‡º: Resource has been closed
```
:::

`txt2img()`çš„è¾“å…¥å‚æ•°`*args`ï¼ˆé™¤æ­¤ä¹‹å¤–å‰é¢è¿˜æœ‰ä¸¤ä¸ªå‚æ•°ï¼‰
```shell
[1]: bottle (<class 'str'>)
[2]:  (<class 'str'>)
[3]: [] (<class 'list'>)
[4]: 1 (<class 'int'>)
[5]: 1 (<class 'int'>)
[6]: 7 (<class 'int'>)
[7]: 512 (<class 'int'>)
[8]: 512 (<class 'int'>)
[9]: False (<class 'bool'>)
[10]: 0.7 (<class 'float'>)
[11]: 2 (<class 'int'>)
[12]: Latent (<class 'str'>)
[13]: 0 (<class 'int'>)
[14]: 0 (<class 'int'>)
[15]: 0 (<class 'int'>)
[16]: Use same checkpoint (<class 'str'>)
[17]: Use same sampler (<class 'str'>)
[18]: Use same scheduler (<class 'str'>)
[19]:  (<class 'str'>)
[20]:  (<class 'str'>)
[21]: [] (<class 'list'>)
[22]: 0 (<class 'int'>)
[23]: 20 (<class 'int'>)
[24]: DPM++ 2M (<class 'str'>)
[25]: Automatic (<class 'str'>)
[26]: False (<class 'bool'>)
[27]:  (<class 'str'>)
[28]: 0.8 (<class 'float'>)
[29]: -1 (<class 'int'>)
[30]: False (<class 'bool'>)
[31]: -1 (<class 'int'>)
[32]: 0 (<class 'int'>)
[33]: 0 (<class 'int'>)
[34]: 0 (<class 'int'>)
[35]: False (<class 'bool'>)
[36]: False (<class 'bool'>)
[37]: LoRA (<class 'str'>)
[38]: None (<class 'str'>)
[39]: 1 (<class 'int'>)
[40]: 1 (<class 'int'>)
[41]: LoRA (<class 'str'>)
[42]: None (<class 'str'>)
[43]: 1 (<class 'int'>)
[44]: 1 (<class 'int'>)
[45]: LoRA (<class 'str'>)
[46]: None (<class 'str'>)
[47]: 1 (<class 'int'>)
[48]: 1 (<class 'int'>)
[49]: LoRA (<class 'str'>)
[50]: None (<class 'str'>)
[51]: 1 (<class 'int'>)
[52]: 1 (<class 'int'>)
[53]: LoRA (<class 'str'>)
[54]: None (<class 'str'>)
[55]: 1 (<class 'int'>)
[56]: 1 (<class 'int'>)
[57]: None (<class 'NoneType'>)
[58]: Refresh models (<class 'str'>)
[59]: False (<class 'bool'>)
[60]: False (<class 'bool'>)
[61]: positive (<class 'str'>)
[62]: comma (<class 'str'>)
[63]: 0 (<class 'int'>)
[64]: False (<class 'bool'>)
[65]: False (<class 'bool'>)
[66]: start (<class 'str'>)
[67]:  (<class 'str'>)
[68]: 1 (<class 'int'>)
[69]:  (<class 'str'>)
[70]: [] (<class 'list'>)
[71]: 0 (<class 'int'>)
[72]:  (<class 'str'>)
[73]: [] (<class 'list'>)
[74]: 0 (<class 'int'>)
[75]:  (<class 'str'>)
[76]: [] (<class 'list'>)
[77]: True (<class 'bool'>)
[78]: False (<class 'bool'>)
[79]: False (<class 'bool'>)
[80]: False (<class 'bool'>)
[81]: False (<class 'bool'>)
[82]: False (<class 'bool'>)
[83]: False (<class 'bool'>)
[84]: 0 (<class 'int'>)
[85]: False (<class 'bool'>)
```

æ–‡ç”Ÿå›¾æœ€åæ‰§è¡Œäº†`modules/processing.py`çš„`line 861`å¤„`process_images_inner`å‡½æ•°ã€‚
å…¶ä¸­çš„sampleè¿‡ç¨‹æœ€ç»ˆæ˜¯è¿›å…¥åˆ°äº†[å¤–éƒ¨åº“](https://github.com/crowsonkb/k-diffusion.git)`repositories/k-diffusion/k_diffusion/sampling.py`ä¸­çš„å„ä¸ªé‡‡æ ·å™¨ï¼Œå¦‚`sample_dpmpp_2m()`å‡½æ•°ã€‚

`KDiffusionSampler`ä¹Ÿæ˜¯ç»§æ‰¿äº†`sd_samplers_common.Sampler`

::: details å…³äº`@property`
`@property`æ˜¯Pythonä¸­ä¸€ä¸ªè£…é¥°å™¨ï¼Œä¸»è¦ç”¨äºå°†ä¸€ä¸ªæ–¹æ³•å˜æˆä¸€ä¸ªåªè¯»å±æ€§ã€‚å®ƒä½¿å¾—å¯¹å±æ€§å€¼çš„ä¿®æ”¹å˜å¾—éå¸¸è‡ªç„¶å’Œç›´è§‚ï¼Œå°¤å…¶æ˜¯åœ¨å¯¹è±¡å±æ€§ä¸Šè¿›è¡Œå„ç§è®¡ç®—æ—¶ã€‚

ä½¿ç”¨`@property`è£…é¥°å™¨å¯ä»¥å®šä¹‰ä¸€ä¸ªæ–¹æ³•ï¼Œè¯¥æ–¹æ³•åœ¨è°ƒç”¨æ—¶ä¼šè‡ªåŠ¨æ‰§è¡Œï¼Œè€Œä¸”å¯ä»¥ä½œä¸ºå±æ€§æ¥ä½¿ç”¨ã€‚å½“å¯¹è±¡å®ä¾‹é€šè¿‡ç‚¹è¯­æ³•è®¿é—®è¯¥å±æ€§çš„å€¼æ—¶ï¼Œä¼šè‡ªåŠ¨è°ƒç”¨è¯¥æ–¹æ³•ï¼Œæ‰§è¡Œæ–¹æ³•å†…çš„é€»è¾‘ï¼Œç„¶åè¿”å›å…¶è¿”å›å€¼ã€‚

ä¸‹é¢æ˜¯ä¸€ä¸ªç®€å•çš„ä¾‹å­ï¼š
```python
class Person:
    def __init__(self, name, age):
        self.name = name
        self.age = age

    @property
    def age_status(self):
        if self.age < 18:
            return "Minor"
        elif self.age < 65:
            return "Adult"
        else:
            return "Senior"

# åˆ›å»ºPersonå®ä¾‹
person = Person("Alice", 30)

# ç›´æ¥é€šè¿‡ç‚¹è¯­æ³•è®¿é—®å±æ€§ï¼Œä¼šè‡ªåŠ¨è°ƒç”¨@propertyè£…é¥°å™¨å®šä¹‰çš„getteræ–¹æ³•
print(person.age_status)  # è¾“å‡º "Adult"
```
åœ¨è¿™ä¸ªä¾‹å­ä¸­ï¼Œ`age_status`æ˜¯ä¸€ä¸ªé€šè¿‡`@property`è£…é¥°å™¨å®šä¹‰çš„å±æ€§ï¼Œä½†å®ƒèƒŒåæ˜¯ä¸€ä¸ªæ–¹æ³•ã€‚
å½“æˆ‘ä»¬å°è¯•é€šè¿‡`person.age_status`æ¥è·å–è¿™ä¸ªå±æ€§å€¼æ—¶ï¼Œå®é™…ä¸Šæ˜¯åœ¨è°ƒç”¨`age_status`æ–¹æ³•å¹¶è¿”å›å…¶è¿”å›å€¼ã€‚

åœ¨`modules/sd_samplers_kdiffusion.py`ä¸­
```python
class CFGDenoiserKDiffusion(sd_samplers_cfg_denoiser.CFGDenoiser):
    @property
    def inner_model(self):
        if self.model_wrap is None:
            denoiser = k_diffusion.external.CompVisVDenoiser if shared.sd_model.parameterization == "v" else k_diffusion.external.CompVisDenoiser
            self.model_wrap = denoiser(shared.sd_model, quantize=shared.opts.enable_quantization)

        return self.model_wrap
```
è¿™æ®µä»£ç å®šä¹‰äº†ä¸€ä¸ªåä¸º`CFGDenoiserKDiffusion`çš„ç±»ï¼Œå®ƒç»§æ‰¿è‡ª`sd_samplers_cfg_denoiser.CFGDenoiser`ç±»ã€‚åœ¨è¿™ä¸ªç±»ä¸­ï¼Œæœ‰ä¸€ä¸ªåä¸º`inner_model`çš„å±æ€§ï¼Œé€šè¿‡`@property`è£…é¥°å™¨å®šä¹‰ã€‚

å½“è®¿é—®`inner_model`å±æ€§æ—¶ï¼ŒPythonä¼šè‡ªåŠ¨è°ƒç”¨`inner_model`æ–¹æ³•ã€‚åœ¨è¿™ä¸ªæ–¹æ³•å†…éƒ¨ï¼Œé¦–å…ˆæ£€æŸ¥`self.model_wrap`æ˜¯å¦ä¸º`None`ã€‚å¦‚æœä¸æ˜¯`None`ï¼Œåˆ™ç›´æ¥è¿”å›`self.model_wrap`çš„å€¼ã€‚
å¦‚æœ`self.model_wrapä¸ºNone`ï¼Œåˆ™æ‰§è¡Œä»¥ä¸‹é€»è¾‘ï¼š
1. æ ¹æ®`shared.sd_model.parameterization`çš„å€¼æ¥é€‰æ‹©ä½¿ç”¨`k_diffusion.external.CompVisVDenoiser`è¿˜æ˜¯`k_diffusion.external.CompVisDenoiser`ç±»ã€‚
2. ä½¿ç”¨ä¸Šé¢é€‰å®šçš„ç±»åˆ›å»ºä¸€ä¸ªæ–°çš„å¯¹è±¡ï¼Œä¼ å…¥`shared.sd_model`ä½œä¸ºå‚æ•°ï¼Œå¹¶è®¾ç½®`quantize`å‚æ•°ä¸º`shared.opts.enable_quantization`çš„å€¼ã€‚
3. å°†æ–°åˆ›å»ºçš„å¯¹è±¡èµ‹å€¼ç»™`self.model_wrap`ã€‚
æœ€åï¼Œæ–¹æ³•è¿”å›`self.model_wrap`çš„å€¼ï¼Œå³`self.model_wrap`å¯¹è±¡ï¼Œè¿™ä¸ªå¯¹è±¡åœ¨ç¬¬ä¸€æ¬¡è°ƒç”¨`inner_model`å±æ€§æ—¶è¢«åˆ›å»ºå¹¶è¿”å›ã€‚

å¯ä»¥æŸ¥åˆ°ï¼Œå®ƒç»§æ‰¿çš„ç±»ä¸­ä¹Ÿæœ‰`inner_model`çš„å‡½æ•°ï¼Œé»˜è®¤ä½¿ç”¨æ—¶ä¼šå¼•å‘`NotImplementedError()`
```python
    @property
    def inner_model(self):
        raise NotImplementedError()
```
:::

::: details å…³äº`@property` å’Œ `.setter`
å…ˆæŠ›å‡ºé—®é¢˜ï¼Œåœ¨`modules/processing.py`ä¸­ï¼š
```python
@dataclass(repr=False)
class StableDiffusionProcessing:
    @property
    def script_args(self):
        return self.script_args_value

    @script_args.setter
    def script_args(self, value):
        self.script_args_value = value

        if self.scripts_value and self.script_args_value and not self.scripts_setup_complete:
            self.setup_scripts()
```
å…³äº`@dataclass`è£…é¥°å™¨ä¼šåœ¨åé¢è¯´æ˜ï¼Œåœ¨ Python ä¸­ï¼Œå¯ä»¥ä½¿ç”¨`@property`è£…é¥°å™¨æ¥å®šä¹‰å±æ€§ï¼ŒåŒæ—¶ä½¿ç”¨åŒåçš„`@property`è£…é¥°å™¨çš„`.setter`æ–¹æ³•æ¥å®šä¹‰è¯¥å±æ€§çš„è®¾ç½®æ–¹æ³•ã€‚è¿™ä½¿å¾—ä½ å¯ä»¥åˆ›å»ºå…·æœ‰æ›´å¤æ‚è¡Œä¸ºçš„å±æ€§ã€‚ä¾‹å¦‚ï¼Œå¯ä»¥æ§åˆ¶å±æ€§çš„è·å–å’Œè®¾ç½®æ“ä½œï¼Œæ·»åŠ é¢å¤–çš„é€»è¾‘æˆ–éªŒè¯ã€‚
```python
class Person:
    def __init__(self, name):
        self._name = name
        self._age = 0  # é»˜è®¤å¹´é¾„ä¸º0

    @property
    def age(self):
        return self._age

    @age.setter
    def age(self, value):
        if value < 0:
            raise ValueError("Age cannot be negative")
        self._age = value

    @property
    def name(self):
        return self._name

    @name.setter
    def name(self, value):
        if not value:
            raise ValueError("Name cannot be empty")
        self._name = value

# ä½¿ç”¨ç¤ºä¾‹
try:
    p = Person("Alice")
    print(p.age)  # è¾“å‡º: 0

    p.age = 30
    print(p.age)  # è¾“å‡º: 30

    p.age = -5  # è¿™å°†å¼•å‘ ValueError
except ValueError as e:
    print(e)

# è¾“å‡º:
# 0
# 30
# Age cannot be negative

# å°è¯•è®¾ç½®ç©ºåå­—
try:
    p.name = ""  # è¿™å°†å¼•å‘ ValueError
except ValueError as e:
    print(e)
```
:::

::: details å…³äº`model`ä¸ºä»€ä¹ˆå¯ä»¥åƒå‡½æ•°ä¸€æ ·ä½¿ç”¨
åœ¨PyTorchä¸­ï¼Œç»§æ‰¿è‡ªtorch.nn.Moduleçš„ç±»å¯ä»¥åƒå‡½æ•°ä¸€æ ·ä½¿ç”¨ï¼Œè¿™ä¸»è¦æ˜¯å› ä¸ºtorch.nn.Moduleç±»å®ç°äº†ç‰¹æ®Šæ–¹æ³•`__call__`ã€‚å½“ä½ åˆ›å»ºä¸€ä¸ªè‡ªå®šä¹‰çš„ç¥ç»ç½‘ç»œç±»å¹¶ç»§æ‰¿è‡ªtorch.nn.Moduleæ—¶ï¼Œ`__call__`æ–¹æ³•ä¼šè‡ªåŠ¨è°ƒç”¨ä½ åœ¨å­ç±»ä¸­å®šä¹‰çš„forwardæ–¹æ³•ã€‚

`repositories/k-diffusion/k_diffusion/sampling.py`
```python
    for i in trange(len(sigmas) - 1, disable=disable):
        denoised = model(x, sigmas[i] * s_in, **extra_args)
```
`sd_samplers_cfg_denoiser.py`
```python
class CFGDenoiser(torch.nn.Module):
    @property
    def inner_model(self):
        raise NotImplementedError()

    def forward(self, x, sigma, uncond, cond, cond_scale, s_min_uncond, image_cond):
        ......
        return denoised
```
:::

::: details Python æ ‡å‡†åº“`dataclasses`
`dataclass` å’Œ `field` æ˜¯`dataclasses`æ¨¡å—ä¸­çš„ä¸¤ä¸ªç»„ä»¶ã€‚`dataclasses`æ¨¡å—ç”¨äºç®€åŒ–æ•°æ®ç±»çš„åˆ›å»ºå’Œç®¡ç†ã€‚æ•°æ®ç±»æ˜¯ä¸€ç§ç‰¹æ®Šçš„ç±»ï¼Œä¸»è¦ç”¨äºå­˜å‚¨æ•°æ®è€Œä¸éœ€è¦å®šä¹‰å¾ˆå¤šæ ·æ¿ä»£ç ã€‚

ä»¥ä¸‹æ˜¯ä¸€ä¸ªç®€å•çš„ç¤ºä¾‹ï¼Œå±•ç¤ºäº†å¦‚ä½•ä½¿ç”¨ `@dataclass` å’Œ `field`ï¼š
```python
from dataclasses import dataclass, field
from typing import List

@dataclass
class Person:
    name: str
    age: int
    friends: List[str] = field(default_factory=list)

# åˆ›å»ºä¸€ä¸ª Person å®ä¾‹
john = Person(name="John Doe", age=30)

print(john)  # è¾“å‡º: Person(name='John Doe', age=30, friends=[])
```

`@dataclass(repr=False)`çš„å«ä¹‰ï¼š
å¦‚æœä½ ä¸å¸Œæœ›æ•°æ®ç±»è‡ªåŠ¨ç”Ÿæˆ`__repr__`æ–¹æ³•ï¼Œå¯ä»¥å°†`repr`å‚æ•°è®¾ç½®ä¸º`False`ã€‚è¿™æ„å‘³ç€ä½ éœ€è¦æ‰‹åŠ¨å®šä¹‰`__repr__`æ–¹æ³•ï¼Œæˆ–è€…æ ¹æœ¬ä¸å®šä¹‰ã€‚
```python
from dataclasses import dataclass

@dataclass(repr=False)
class Person:
    name: str
    age: int

# æ‰‹åŠ¨å®šä¹‰ __repr__ æ–¹æ³•
@dataclass(repr=False)
class PersonWithCustomRepr:
    name: str
    age: int

    def __repr__(self):
        return f'Person(name={self.name})'

# åˆ›å»ºå®ä¾‹
person = Person(name="John Doe", age=30)
person_with_custom_repr = PersonWithCustomRepr(name="Jane Doe", age=25)

print(repr(person))  # è¾“å‡º: <__main__.Person object at 0x...>
print(repr(person_with_custom_repr))  # è¾“å‡º: Person(name=Jane Doe)
```
:::

ldmæ˜¯`repositories/stable-diffusion-stability-ai`é‡Œçš„ä¸€ä¸ªæ–‡ä»¶å¤¹ã€‚`No module 'xformers'. Proceeding without it.`æ˜¯åœ¨
`repositories/stable-diffusion-stability-ai/ldm/modules/diffusionmodules/model.py`ä¸­åˆå§‹åŒ–ã€‚
```python
try:
    import xformers
    import xformers.ops
    XFORMERS_IS_AVAILBLE = True
except:
    XFORMERS_IS_AVAILBLE = False
    print("No module 'xformers'. Proceeding without it.")
```
è¿™é‡Œæ˜¯åˆ¤æ–­èƒ½å¦æˆåŠŸå¯¼å…¥`xformers`åº“æ¥å†³å®šæ˜¯å¦ä½¿ç”¨xformersï¼Œä½†æ˜¯åœ¨å®é™…ä½¿ç”¨ä¸­ï¼Œç”¨è¿‡Linuxç‰ˆçš„éƒ½çŸ¥é“å³ä½¿ç¯å¢ƒæœ‰è¿™ä¸ªåº“ï¼Œå¦‚æœåœ¨ä½¿ç”¨æ—¶ä¸æ·»åŠ `--xfomers`
è¿™ä¸€é¡¹ï¼Œä¾ç„¶ä¸ä¼šè½½å…¥`xformers`ï¼Œå› æ­¤å¯çŸ¥ï¼Œä¹‹å‰æœ‰ä»£ç ä¼šå½±å“è¿™ä¸€æ­¥å¯¼å…¥åº“æ˜¯å¦æˆåŠŸã€‚åœ¨`modules/import_hook.py`ä¸­æœ‰ï¼š

```python
import sys

# this will break any attempt to import xformers which will prevent stability diffusion repo from trying to use it
if "--xformers" not in "".join(sys.argv):
    sys.modules["xformers"] = None
```

è‡³äºä¸ºä»€ä¹ˆè¦è¿™ä¹ˆåˆ¤æ–­ï¼Œæˆ‘æƒ³åº”è¯¥æ˜¯è¿™ä¸¤ä¸ªä»£ç ä¸å±äºåŒä¸€ä¸ªä»“åº“ï¼Œä½†æ˜¯éœ€è¦ä¸€ä¸ªæ–¹æ³•å…±äº«ç›¸åŒçš„ä¿¡æ¯ï¼Œç›´æ¥è¿™æ ·æœ€æ–¹ä¾¿


## SD & 3D Model

### è´´å›¾ç”Ÿæˆ
#### æ— ç¼è´´å›¾
æ•™ç¨‹: https://www.bilibili.com/video/BV1Kp42117Mv
::: code-group
```json [æ— ç¼è´´å›¾-AIç”Ÿæˆv3.1]
{
  "last_node_id": 329, "last_link_id": 695,
  "nodes": [
    {
      "id": 172, "type": "PrimitiveNode", "pos": [1468.0768300781262, 350], "size": {"0": 210, "1": 80}, "flags": {}, "order": 0, "mode": 0,
      "outputs": [
        {"name": "INT", "type": "INT", "links": [282, 289], "slot_index": 0, "widget": {"name": "x_offset"}, "label": "INT"}
      ],
      "properties": {"Run widget replace on values": false}, "widgets_values": [1020, "fixed"]
    },
    {"id": 227, "type": "Reroute", "pos": [1538.0768300781262, 500], "size": [75, 26], "flags": {}, "order": 29, "mode": 0, "inputs": [{"name": "", "type": "*", "link": 695, "label": ""}], "outputs": [{"name": "", "type": "IMAGE", "links": [434, 435, 436, 437], "slot_index": 0, "label": ""}], "properties": {"showOutputText": false, "horizontal": false}},
    {"id": 6, "type": "CLIPTextEncode", "pos": [630, 210], "size": {"0": 210, "1": 90}, "flags": {"collapsed": true}, "order": 18, "mode": 0, "inputs": [{"name": "clip", "type": "CLIP", "link": 224, "label": "CLIP"}, {"name": "text", "type": "STRING", "link": 98, "widget": {"name": "text"}, "label": "\u6587\u672c"}], "outputs": [{"name": "CONDITIONING", "type": "CONDITIONING", "links": [48], "slot_index": 0, "label": "\u6761\u4ef6"}], "properties": {"Node name for S&R": "CLIPTextEncode"}, "widgets_values": ["1girl"], "color": "#232", "bgcolor": "#353"},
    {"id": 7, "type": "CLIPTextEncode", "pos": [640, 260], "size": {"0": 210, "1": 100}, "flags": {"collapsed": true}, "order": 21, "mode": 0, "inputs": [{"name": "clip", "type": "CLIP", "link": 225, "label": "CLIP"}, {"name": "text", "type": "STRING", "link": 100, "widget": {"name": "text"}, "label": "\u6587\u672c", "slot_index": 1}], "outputs": [{"name": "CONDITIONING", "type": "CONDITIONING", "links": [6], "slot_index": 0, "label": "\u6761\u4ef6"}], "properties": {"Node name for S&R": "CLIPTextEncode"}, "widgets_values": ["text, watermark"], "color": "#322", "bgcolor": "#533"},
    {"id": 245, "type": "CLIPTextEncode", "pos": [310, 1110], "size": {"0": 210, "1": 90}, "flags": {"collapsed": true}, "order": 20, "mode": 0, "inputs": [{"name": "clip", "type": "CLIP", "link": 457, "label": "CLIP"}, {"name": "text", "type": "STRING", "link": 464, "widget": {"name": "text"}, "label": "\u6587\u672c", "slot_index": 1}], "outputs": [{"name": "CONDITIONING", "type": "CONDITIONING", "links": [459], "slot_index": 0, "label": "\u6761\u4ef6"}], "properties": {"Node name for S&R": "CLIPTextEncode"}, "widgets_values": ["1girl"], "color": "#232", "bgcolor": "#353"},
    {"id": 3, "type": "KSampler", "pos": [810, 220], "size": {"0": 260, "1": 470}, "flags": {}, "order": 25, "mode": 0, "inputs": [{"name": "model", "type": "MODEL", "link": 223, "label": "\u6a21\u578b"}, {"name": "positive", "type": "CONDITIONING", "link": 48, "label": "\u6b63\u9762\u6761\u4ef6"}, {"name": "negative", "type": "CONDITIONING", "link": 6, "label": "\u8d1f\u9762\u6761\u4ef6"}, {"name": "latent_image", "type": "LATENT", "link": 431, "label": "Latent"}, {"name": "seed", "type": "INT", "link": 506, "widget": {"name": "seed"}, "label": "\u968f\u673a\u79cd", "slot_index": 4}, {"name": "steps", "type": "INT", "link": 503, "widget": {"name": "steps"}, "label": "\u6b65\u6570"}, {"name": "cfg", "type": "FLOAT", "link": 502, "widget": {"name": "cfg"}, "label": "CFG"}, {"name": "sampler_name", "type": "COMBO", "link": 504, "widget": {"name": "sampler_name"}, "label": "\u91c7\u6837\u5668", "slot_index": 7}, {"name": "scheduler", "type": "COMBO", "link": 505, "widget": {"name": "scheduler"}, "label": "\u8c03\u5ea6\u5668", "slot_index": 8}], "outputs": [{"name": "LATENT", "type": "LATENT", "links": [7], "slot_index": 0, "label": "Latent"}], "properties": {"Node name for S&R": "KSampler"}, "widgets_values": [386213472092672, "randomize", 8, 1, "euler", "sgm_uniform", 1]}, {"id": 65, "type": "SDXLPromptStyler", "pos": [560, 330], "size": {"0": 230, "1": 170}, "flags": {"collapsed": false}, "order": 15, "mode": 0, "inputs": [{"name": "text_positive", "type": "STRING", "link": 204, "widget": {"name": "text_positive"}, "label": "\u6b63\u9762\u6761\u4ef6"}, {"name": "text_negative", "type": "STRING", "link": 205, "widget": {"name": "text_negative"}, "label": "\u8d1f\u9762\u6761\u4ef6"}], "outputs": [{"name": "positive_prompt_text_g", "type": "STRING", "links": [98, 134, 464], "shape": 3, "label": "positive_prompt_text_g", "slot_index": 0}, {"name": "negative_prompt_text_g", "type": "STRING", "links": [100, 135, 465], "shape": 3, "label": "negative_prompt_text_g", "slot_index": 1}], "properties": {"Node name for S&R": "SDXLPromptStyler"}, "widgets_values": ["1 girl, long hair, dress, 3/4 profile, ", "text, watermark", "sai-texture", true, true, true], "color": "#232", "bgcolor": "#353"},
    {"id": 211, "type": "PreviewImage", "pos": [1988.0768300781262, 250], "size": {"0": 210, "1": 250}, "flags": {}, "order": 33, "mode": 0, "inputs": [{"name": "images", "type": "IMAGE", "link": 361, "label": "\u56fe\u50cf"}], "properties": {"Node name for S&R": "PreviewImage"}}, {"id": 268, "type": "PrimitiveNode", "pos": [570, 820], "size": {"0": 210, "1": 80}, "flags": {}, "order": 1, "mode": 0, "outputs": [{"name": "FLOAT", "type": "FLOAT", "links": [497, 502], "slot_index": 0, "widget": {"name": "cfg"}, "label": "FLOAT"}], "properties": {"Run widget replace on values": false}, "widgets_values": [1, "fixed"]}, {"id": 274, "type": "PrimitiveNode", "pos": [570, 680], "size": {"0": 210, "1": 80}, "flags": {}, "order": 2, "mode": 0, "outputs": [{"name": "INT", "type": "INT", "links": [501, 503], "slot_index": 0, "widget": {"name": "steps"}, "label": "INT"}], "properties": {"Run widget replace on values": false}, "widgets_values": [8, "fixed"]}, {"id": 91, "type": "CheckpointLoader|pysssss", "pos": [230, 230], "size": {"0": 210, "1": 122}, "flags": {}, "order": 3, "mode": 0, "outputs": [{"name": "MODEL", "type": "MODEL", "links": [223, 454], "shape": 3, "label": "\u6a21\u578b", "slot_index": 0}, {"name": "CLIP", "type": "CLIP", "links": [224, 225, 457, 458], "shape": 3, "label": "CLIP", "slot_index": 1}, {"name": "VAE", "type": "VAE", "links": [609], "shape": 3, "label": "VAE", "slot_index": 2}], "properties": {"Node name for S&R": "CheckpointLoader|pysssss"}, "widgets_values": [{"content": "SDXL-lightning/sdxl_lightning_8step.safetensors", "image": null}, "[none]"]}, {"id": 306, "type": "Reroute", "pos": [1140, 150], "size": [75, 26], "flags": {}, "order": 13, "mode": 0, "inputs": [{"name": "", "type": "*", "link": 609}], "outputs": [{"name": "", "type": "VAE", "links": [613, 617], "slot_index": 0}], "properties": {"showOutputText": false, "horizontal": false}}, {"id": 176, "type": "PreviewImage", "pos": [1988.0768300781262, 630], "size": {"0": 200, "1": 250}, "flags": {}, "order": 31, "mode": 0, "inputs": [{"name": "images", "type": "IMAGE", "link": 290, "label": "\u56fe\u50cf"}], "properties": {"Node name for S&R": "PreviewImage"}}, {"id": 181, "type": "Image Overlay", "pos": [1748.0768300781262, 630], "size": {"0": 210, "1": 290}, "flags": {}, "order": 30, "mode": 0, "inputs": [{"name": "base_image", "type": "IMAGE", "link": 693, "label": "\u57fa\u7840\u56fe\u50cf", "slot_index": 0}, {"name": "overlay_image", "type": "IMAGE", "link": 434, "label": "\u8986\u76d6\u56fe\u50cf", "slot_index": 1}, {"name": "optional_mask", "type": "MASK", "link": null, "label": "\u906e\u7f69", "slot_index": 2}, {"name": "y_offset", "type": "INT", "link": 289, "widget": {"name": "y_offset"}, "label": "Y\u504f\u79fb", "slot_index": 4}], "outputs": [{"name": "IMAGE", "type": "IMAGE", "links": [290, 405], "shape": 3, "label": "\u56fe\u50cf", "slot_index": 0}], "properties": {"Node name for S&R": "Image Overlay"}, "widgets_values": ["None", "nearest-exact", 1, 0, 0, 0, 1020, 0, 0], "color": "#222233", "bgcolor": "#333355", "shape": 1},
    {"id": 264, "type": "PreviewImage", "pos": [1090, 370], "size": {"0": 290, "1": 320}, "flags": {}, "order": 28, "mode": 0, "inputs": [{"name": "images", "type": "IMAGE", "link": 489, "label": "\u56fe\u50cf"}], "properties": {"Node name for S&R": "PreviewImage"}}, {"id": 270, "type": "PrimitiveNode", "pos": [810, 810], "size": {"0": 210, "1": 110}, "flags": {}, "order": 4, "mode": 0, "outputs": [{"name": "COMBO", "type": "COMBO", "links": [498, 504], "slot_index": 0, "widget": {"name": "sampler_name"}, "label": "COMBO"}], "properties": {"Run widget replace on values": false}, "widgets_values": ["euler", "fixed", ""]},
    {"id": 271, "type": "PrimitiveNode", "pos": [1060, 810], "size": {"0": 210, "1": 110}, "flags": {}, "order": 5, "mode": 0, "outputs": [{"name": "COMBO", "type": "COMBO", "links": [499, 505], "slot_index": 0, "widget": {"name": "scheduler"}, "label": "COMBO"}], "properties": {"Run widget replace on values": false}, "widgets_values": ["sgm_uniform", "fixed", ""]}, {"id": 307, "type": "Reroute", "pos": [109.90929193787348, 1038.4640011941774], "size": [75, 26], "flags": {}, "order": 16, "mode": 0, "inputs": [{"name": "", "type": "*", "link": 613}], "outputs": [{"name": "", "type": "VAE", "links": [614, 615], "slot_index": 0}], "properties": {"showOutputText": false, "horizontal": false}}, {"id": 290, "type": "SaveImage", "pos": [2414.773395543872, 1209.6649475057209], "size": {"0": 380, "1": 480}, "flags": {}, "order": 53, "mode": 2, "inputs": [{"name": "images", "type": "IMAGE", "link": 659, "label": "\u56fe\u50cf"}], "properties": {}, "widgets_values": ["normal-map"]},
    {"id": 291, "type": "SaveImage", "pos": [2840.6181026141826, 1216.2230412752522], "size": {"0": 400, "1": 480}, "flags": {}, "order": 54, "mode": 2, "inputs": [{"name": "images", "type": "IMAGE", "link": 660, "label": "\u56fe\u50cf"}], "properties": {}, "widgets_values": ["bump-map"]}, {"id": 288, "type": "AIO_Preprocessor", "pos": [2840.6181026141826, 1076.2230412752522], "size": {"0": 310, "1": 82}, "flags": {}, "order": 51, "mode": 2, "inputs": [{"name": "image", "type": "IMAGE", "link": 691, "label": "\u56fe\u50cf", "slot_index": 0}], "outputs": [{"name": "IMAGE", "type": "IMAGE", "links": [660], "shape": 3, "label": "\u56fe\u50cf", "slot_index": 0}], "properties": {"Node name for S&R": "AIO_Preprocessor"}, "widgets_values": ["MiDaS-DepthMapPreprocessor", 1024]},
    {"id": 286, "type": "AIO_Preprocessor", "pos": [2428.2527168593847, 1080.2547984062505], "size": {"0": 310, "1": 82}, "flags": {}, "order": 50, "mode": 2, "inputs": [{"name": "image", "type": "IMAGE", "link": 690, "label": "\u56fe\u50cf", "slot_index": 0}], "outputs": [{"name": "IMAGE", "type": "IMAGE", "links": [659], "shape": 3, "label": "\u56fe\u50cf", "slot_index": 0}], "properties": {"Node name for S&R": "AIO_Preprocessor"}, "widgets_values": ["BAE-NormalMapPreprocessor", 1024]}, {"id": 179, "type": "Image Overlay", "pos": [1750, 220], "size": {"0": 210, "1": 290}, "flags": {}, "order": 32, "mode": 0, "inputs": [{"name": "base_image", "type": "IMAGE", "link": 405, "label": "\u57fa\u7840\u56fe\u50cf", "slot_index": 0}, {"name": "overlay_image", "type": "IMAGE", "link": 435, "label": "\u8986\u76d6\u56fe\u50cf", "slot_index": 1}, {"name": "optional_mask", "type": "MASK", "link": null, "label": "\u906e\u7f69", "slot_index": 2}, {"name": "x_offset", "type": "INT", "link": 282, "widget": {"name": "x_offset"}, "label": "X\u504f\u79fb", "slot_index": 3}], "outputs": [{"name": "IMAGE", "type": "IMAGE", "links": [361, 669], "shape": 3, "label": "\u56fe\u50cf", "slot_index": 0}], "properties": {"Node name for S&R": "Image Overlay"}, "widgets_values": ["None", "nearest-exact", 1, 0, 0, 1020, 0, 0, 0], "color": "#222233", "bgcolor": "#333355", "shape": 1}, {"id": 222, "type": "Image Overlay", "pos": [2230, 220], "size": {"0": 210, "1": 290}, "flags": {}, "order": 34, "mode": 0, "inputs": [{"name": "base_image", "type": "IMAGE", "link": 669, "label": "\u57fa\u7840\u56fe\u50cf", "slot_index": 0}, {"name": "overlay_image", "type": "IMAGE", "link": 437, "label": "\u8986\u76d6\u56fe\u50cf", "slot_index": 1}, {"name": "optional_mask", "type": "MASK", "link": null, "label": "\u906e\u7f69", "slot_index": 2}], "outputs": [{"name": "IMAGE", "type": "IMAGE", "links": [422, 670], "shape": 3, "label": "\u56fe\u50cf", "slot_index": 0}], "properties": {"Node name for S&R": "Image Overlay"}, "widgets_values": ["None", "nearest-exact", 1, 0, 0, 1020, 1020, 0, 0], "color": "#222233", "bgcolor": "#333355", "shape": 1}, {"id": 220, "type": "Image Overlay", "pos": [2230, 620], "size": {"0": 210, "1": 290}, "flags": {}, "order": 36, "mode": 0, "inputs": [{"name": "base_image", "type": "IMAGE", "link": 670, "label": "\u57fa\u7840\u56fe\u50cf", "slot_index": 0}, {"name": "overlay_image", "type": "IMAGE", "link": 436, "label": "\u8986\u76d6\u56fe\u50cf", "slot_index": 1}, {"name": "optional_mask", "type": "MASK", "link": null, "label": "\u906e\u7f69", "slot_index": 2}], "outputs": [{"name": "IMAGE", "type": "IMAGE", "links": [416, 671], "shape": 3, "label": "\u56fe\u50cf", "slot_index": 0}], "properties": {"Node name for S&R": "Image Overlay"}, "widgets_values": ["None", "nearest-exact", 1, 0, 0, 0, 0, 0, 0], "color": "#222233", "bgcolor": "#333355", "shape": 1}, {"id": 223, "type": "PreviewImage", "pos": [2460, 240], "size": {"0": 210, "1": 250}, "flags": {}, "order": 35, "mode": 0, "inputs": [{"name": "images", "type": "IMAGE", "link": 422, "label": "\u56fe\u50cf"}], "properties": {"Node name for S&R": "PreviewImage"}}, {"id": 221, "type": "PreviewImage", "pos": [2460, 620], "size": {"0": 210, "1": 250}, "flags": {}, "order": 37, "mode": 0, "inputs": [{"name": "images", "type": "IMAGE", "link": 416, "label": "\u56fe\u50cf"}], "properties": {"Node name for S&R": "PreviewImage"}},
    {"id": 317, "type": "Reroute", "pos": [1133.988006132812, 1061.076388007812], "size": [75, 26], "flags": {}, "order": 43, "mode": 0, "inputs": [{"name": "", "type": "*", "link": 687, "label": ""}], "outputs": [{"name": "", "type": "IMAGE", "links": [633, 634, 637, 643], "slot_index": 0}], "properties": {"showOutputText": false, "horizontal": false}}, {"id": 324, "type": "ImageToMask", "pos": [1113.988006132812, 1631.076388007812], "size": {"0": 210, "1": 60}, "flags": {}, "order": 26, "mode": 0, "inputs": [{"name": "image", "type": "IMAGE", "link": 677, "label": "\u56fe\u50cf"}], "outputs": [{"name": "MASK", "type": "MASK", "links": [683, 684, 685], "shape": 3, "label": "\u906e\u7f69", "slot_index": 0}], "properties": {"Node name for S&R": "ImageToMask"}, "widgets_values": ["red"]}, {"id": 313, "type": "Image Overlay", "pos": [1383.988006132812, 1441.076388007812], "size": {"0": 210, "1": 290}, "flags": {}, "order": 45, "mode": 0, "inputs": [{"name": "base_image", "type": "IMAGE", "link": 636, "label": "\u57fa\u7840\u56fe\u50cf", "slot_index": 0}, {"name": "overlay_image", "type": "IMAGE", "link": 637, "label": "\u8986\u76d6\u56fe\u50cf", "slot_index": 1}, {"name": "optional_mask", "type": "MASK", "link": 684, "label": "\u906e\u7f69", "slot_index": 2}, {"name": "y_offset", "type": "INT", "link": 638, "widget": {"name": "y_offset"}, "label": "Y\u504f\u79fb"}], "outputs": [{"name": "IMAGE", "type": "IMAGE", "links": [642], "shape": 3, "label": "\u56fe\u50cf", "slot_index": 0}], "properties": {"Node name for S&R": "Image Overlay"}, "widgets_values": ["None", "nearest-exact", 1, 1280, 256, 0, -1020, 0, 0], "color": "#222233", "bgcolor": "#333355", "shape": 1},
    {"id": 318, "type": "Image Overlay", "pos": [1623.9880061328115, 1311.076388007812], "size": {"0": 210, "1": 290}, "flags": {}, "order": 46, "mode": 0, "inputs": [{"name": "base_image", "type": "IMAGE", "link": 642, "label": "\u57fa\u7840\u56fe\u50cf", "slot_index": 0}, {"name": "overlay_image", "type": "IMAGE", "link": 643, "label": "\u8986\u76d6\u56fe\u50cf", "slot_index": 1}, {"name": "optional_mask", "type": "MASK", "link": 685, "label": "\u906e\u7f69", "slot_index": 2}, {"name": "x_offset", "type": "INT", "link": 644, "widget": {"name": "x_offset"}, "label": "X\u504f\u79fb", "slot_index": 3}, {"name": "y_offset", "type": "INT", "link": 645, "widget": {"name": "y_offset"}, "label": "Y\u504f\u79fb", "slot_index": 4}], "outputs": [{"name": "IMAGE", "type": "IMAGE", "links": [639], "shape": 3, "label": "\u56fe\u50cf", "slot_index": 0}], "properties": {"Node name for S&R": "Image Overlay"}, "widgets_values": ["None", "nearest-exact", 1, 256, 256, -1020, -1020, 0, 0], "color": "#222233", "bgcolor": "#333355", "shape": 1}, {"id": 315, "type": "SaveImage", "pos": [1853.9880061328115, 1261.076388007812], "size": {"0": 460, "1": 480}, "flags": {}, "order": 49, "mode": 0, "inputs": [{"name": "images", "type": "IMAGE", "link": 689, "label": "\u56fe\u50cf"}], "properties": {}, "widgets_values": ["color-map"]}, {"id": 322, "type": "ImageScale", "pos": [1113.988006132812, 1271.076388007812], "size": {"0": 210, "1": 130}, "flags": {}, "order": 17, "mode": 0, "inputs": [{"name": "image", "type": "IMAGE", "link": 676, "label": "\u56fe\u50cf"}], "outputs": [{"name": "IMAGE", "type": "IMAGE", "links": [675], "shape": 3, "label": "\u56fe\u50cf", "slot_index": 0}], "properties": {"Node name for S&R": "ImageScale"}, "widgets_values": ["bilinear", 1590, 1590, "disabled"]}, {"id": 323, "type": "ImageCrop", "pos": [1113.988006132812, 1451.076388007812], "size": {"0": 210, "1": 130}, "flags": {}, "order": 24, "mode": 0, "inputs": [{"name": "image", "type": "IMAGE", "link": 675, "label": "\u56fe\u50cf"}], "outputs": [{"name": "IMAGE", "type": "IMAGE", "links": [677], "shape": 3, "label": "\u56fe\u50cf", "slot_index": 0}], "properties": {"Node name for S&R": "ImageCrop"}, "widgets_values": [1276, 1276, 0, 0]}, {"id": 129, "type": "VAEEncode", "pos": [110, 1110], "size": {"0": 140, "1": 46}, "flags": {}, "order": 38, "mode": 0, "inputs": [{"name": "pixels", "type": "IMAGE", "link": 671, "label": "\u56fe\u50cf"}, {"name": "vae", "type": "VAE", "link": 614, "label": "VAE", "slot_index": 1}], "outputs": [{"name": "LATENT", "type": "LATENT", "links": [208], "shape": 3, "label": "Latent", "slot_index": 0}], "properties": {"Node name for S&R": "VAEEncode"}},
    {"id": 249, "type": "KSampler", "pos": [400, 1220], "size": {"0": 260, "1": 470}, "flags": {}, "order": 40, "mode": 0, "inputs": [{"name": "model", "type": "MODEL", "link": 454, "label": "\u6a21\u578b"}, {"name": "positive", "type": "CONDITIONING", "link": 459, "label": "\u6b63\u9762\u6761\u4ef6", "slot_index": 1}, {"name": "negative", "type": "CONDITIONING", "link": 460, "label": "\u8d1f\u9762\u6761\u4ef6", "slot_index": 2}, {"name": "latent_image", "type": "LATENT", "link": 466, "label": "Latent", "slot_index": 3}, {"name": "cfg", "type": "FLOAT", "link": 497, "widget": {"name": "cfg"}, "label": "CFG"}, {"name": "sampler_name", "type": "COMBO", "link": 498, "widget": {"name": "sampler_name"}, "label": "\u91c7\u6837\u5668"}, {"name": "scheduler", "type": "COMBO", "link": 499, "widget": {"name": "scheduler"}, "label": "\u8c03\u5ea6\u5668"}, {"name": "seed", "type": "INT", "link": 500, "widget": {"name": "seed"}, "label": "\u968f\u673a\u79cd"}, {"name": "steps", "type": "INT", "link": 501, "widget": {"name": "steps"}, "label": "\u6b65\u6570"}], "outputs": [{"name": "LATENT", "type": "LATENT", "links": [461], "slot_index": 0, "label": "Latent"}], "properties": {"Node name for S&R": "KSampler"}, "widgets_values": [1065189257968362, "randomize", 8, 1, "euler", "sgm_uniform", 1]},
    {"id": 132, "type": "SetLatentNoiseMask", "pos": [110, 1220], "size": {"0": 176.39999389648438, "1": 46}, "flags": {}, "order": 39, "mode": 0, "inputs": [{"name": "samples", "type": "LATENT", "link": 208, "label": "Latent"}, {"name": "mask", "type": "MASK", "link": 491, "label": "\u906e\u7f69", "slot_index": 1}], "outputs": [{"name": "LATENT", "type": "LATENT", "links": [466], "shape": 3, "label": "Latent", "slot_index": 0}], "properties": {"Node name for S&R": "SetLatentNoiseMask"}}, {"id": 246, "type": "CLIPTextEncode", "pos": [310, 1160], "size": {"0": 210, "1": 100}, "flags": {"collapsed": true}, "order": 23, "mode": 0, "inputs": [{"name": "clip", "type": "CLIP", "link": 458, "label": "CLIP"}, {"name": "text", "type": "STRING", "link": 465, "widget": {"name": "text"}, "label": "\u6587\u672c", "slot_index": 1}], "outputs": [{"name": "CONDITIONING", "type": "CONDITIONING", "links": [460], "slot_index": 0, "label": "\u6761\u4ef6"}], "properties": {"Node name for S&R": "CLIPTextEncode"}, "widgets_values": ["text, watermark"], "color": "#322", "bgcolor": "#533"}, {"id": 263, "type": "PreviewImage", "pos": [690, 1230], "size": {"0": 310, "1": 350}, "flags": {}, "order": 42, "mode": 0, "inputs": [{"name": "images", "type": "IMAGE", "link": 487, "label": "\u56fe\u50cf"}], "properties": {"Node name for S&R": "PreviewImage"}}, {"id": 312, "type": "Image Overlay", "pos": [1384.784878124999, 1092.8564836914059], "size": {"0": 210, "1": 290}, "flags": {}, "order": 44, "mode": 0, "inputs": [{"name": "base_image", "type": "IMAGE", "link": 633, "label": "\u57fa\u7840\u56fe\u50cf", "slot_index": 0}, {"name": "overlay_image", "type": "IMAGE", "link": 634, "label": "\u8986\u76d6\u56fe\u50cf", "slot_index": 1}, {"name": "optional_mask", "type": "MASK", "link": 683, "label": "\u906e\u7f69", "slot_index": 2}, {"name": "x_offset", "type": "INT", "link": 635, "widget": {"name": "x_offset"}, "label": "X\u504f\u79fb"}], "outputs": [{"name": "IMAGE", "type": "IMAGE", "links": [636], "shape": 3, "label": "\u56fe\u50cf", "slot_index": 0}], "properties": {"Node name for S&R": "Image Overlay"}, "widgets_values": ["None", "nearest-exact", 1, 256, 1280, -1020, 0, 0, 0], "color": "#222233", "bgcolor": "#333355", "shape": 1}, {"id": 316, "type": "PrimitiveNode", "pos": [1113.988006132812, 1141.076388007812], "size": {"0": 210, "1": 80}, "flags": {}, "order": 6, "mode": 0, "outputs": [{"name": "INT", "type": "INT", "links": [635, 638, 644, 645], "slot_index": 0, "widget": {"name": "x_offset"}, "label": "INT"}], "properties": {"Run widget replace on values": false}, "widgets_values": [-1020, "fixed"]}, {"id": 206, "type": "LoadImageMask", "pos": [110, 1340], "size": {"0": 250, "1": 320}, "flags": {}, "order": 7, "mode": 0, "outputs": [{"name": "MASK", "type": "MASK", "links": [491, 493], "shape": 3, "label": "\u906e\u7f69", "slot_index": 0}], "properties": {"Node name for S&R": "LoadImageMask"}, "widgets_values": ["alpha \u8d34\u56fe.png", "alpha", "image"]}, {"id": 266, "type": "MaskToImage", "pos": [850, 1150], "size": {"0": 140, "1": 30}, "flags": {}, "order": 14, "mode": 0, "inputs": [{"name": "mask", "type": "MASK", "link": 493, "label": "\u906e\u7f69"}], "outputs": [{"name": "IMAGE", "type": "IMAGE", "links": [676], "shape": 3, "label": "\u56fe\u50cf", "slot_index": 0}], "properties": {"Node name for S&R": "MaskToImage"}}, {"id": 248, "type": "VAEDecode", "pos": [680, 1120], "size": {"0": 140, "1": 50}, "flags": {}, "order": 41, "mode": 0, "inputs": [{"name": "samples", "type": "LATENT", "link": 461, "label": "Latent"}, {"name": "vae", "type": "VAE", "link": 615, "label": "VAE", "slot_index": 1}], "outputs": [{"name": "IMAGE", "type": "IMAGE", "links": [487, 687], "slot_index": 0, "label": "\u56fe\u50cf"}], "properties": {"Node name for S&R": "VAEDecode"}}, {"id": 314, "type": "ImageCrop", "pos": [1613.9880061328115, 1081.076388007812], "size": {"0": 210, "1": 130}, "flags": {}, "order": 47, "mode": 0, "inputs": [{"name": "image", "type": "IMAGE", "link": 639, "label": "\u56fe\u50cf", "slot_index": 0}], "outputs": [{"name": "IMAGE", "type": "IMAGE", "links": [688], "shape": 3, "label": "\u56fe\u50cf", "slot_index": 0}], "properties": {"Node name for S&R": "ImageCrop"}, "widgets_values": [1020, 1020, 0, 0]}, {"id": 319, "type": "PlaySound|pysssss", "pos": [2090, 1090], "size": {"0": 210, "1": 110}, "flags": {}, "order": 52, "mode": 0, "inputs": [{"name": "any", "type": "*", "link": 692, "label": "\u8f93\u5165"}], "outputs": [{"name": "*", "type": "*", "links": null, "shape": 6}], "properties": {"Node name for S&R": "PlaySound|pysssss"}, "widgets_values": ["always", 1, "notify.mp3"]}, {"id": 327, "type": "ImageScale", "pos": [1854.7848781249986, 1082.8564836914059], "size": {"0": 210, "1": 130}, "flags": {}, "order": 48, "mode": 0, "inputs": [{"name": "image", "type": "IMAGE", "link": 688, "label": "\u56fe\u50cf"}], "outputs": [{"name": "IMAGE", "type": "IMAGE", "links": [689, 690, 691, 692], "shape": 3, "label": "\u56fe\u50cf", "slot_index": 0}], "properties": {"Node name for S&R": "ImageScale"}, "widgets_values": ["bilinear", 1024, 1024, "disabled"]},
    {"id": 328, "type": "EmptyImage", "pos": [1480, 710], "size": [210, 130], "flags": {}, "order": 8, "mode": 0, "outputs": [{"name": "IMAGE", "type": "IMAGE", "links": [693], "shape": 3, "label": "\u56fe\u50cf", "slot_index": 0}], "properties": {"Node name for S&R": "EmptyImage"}, "widgets_values": [1276, 1276, 1, 999999]}, {"id": 8, "type": "VAEDecode", "pos": [1110, 240], "size": {"0": 140, "1": 50}, "flags": {}, "order": 27, "mode": 0, "inputs": [{"name": "samples", "type": "LATENT", "link": 7, "label": "Latent"}, {"name": "vae", "type": "VAE", "link": 617, "label": "VAE", "slot_index": 1}], "outputs": [{"name": "IMAGE", "type": "IMAGE", "links": [489, 695], "slot_index": 0, "label": "\u56fe\u50cf"}], "properties": {"Node name for S&R": "VAEDecode"}}, {"id": 126, "type": "DeepTranslatorTextNode", "pos": [90, 400], "size": [230, 250], "flags": {}, "order": 9, "mode": 0, "outputs": [{"name": "text", "type": "STRING", "links": [204], "shape": 3, "label": "\u6587\u672c", "slot_index": 0}], "properties": {"Node name for S&R": "DeepTranslatorTextNode"}, "widgets_values": ["auto", "english", "disable", "", "", "GoogleTranslator [free]", "rock", "proxy_hide", "authorization_hide"], "color": "#232", "bgcolor": "#353"}, {"id": 127, "type": "DeepTranslatorTextNode", "pos": [90, 690], "size": [230, 250], "flags": {}, "order": 10, "mode": 0, "outputs": [{"name": "text", "type": "STRING", "links": [205], "shape": 3, "label": "\u6587\u672c", "slot_index": 0}], "properties": {"Node name for S&R": "DeepTranslatorTextNode"}, "widgets_values": ["auto", "english", "disable", "", "", "GoogleTranslator [free]", "", "proxy_hide", "authorization_hide"], "color": "#322", "bgcolor": "#533"},
    {"id": 92, "type": "ShowText|pysssss", "pos": [330, 410], "size": [220, 160], "flags": {"collapsed": false}, "order": 19, "mode": 0, "inputs": [{"name": "text", "type": "STRING", "link": 134, "widget": {"name": "text"}, "label": "\u6587\u672c"}], "outputs": [{"name": "STRING", "type": "STRING", "links": null, "shape": 6, "label": "\u5b57\u7b26\u4e32"}], "properties": {"Node name for S&R": "ShowText|pysssss"}, "widgets_values": [["ethereal fantasy concept art of  psychedelic style 1 girl, \u9648\u5c0f\u7ead, long hair, dress, looking back,   . vibrant colors, swirling patterns, abstract forms, surreal, trippy . magnificent, celestial, ethereal, painterly, epic, majestic, magical, fantasy art, cover art, dreamy"], "ethereal fantasy concept art of  psychedelic style 1 girl, \u9648\u5c0f\u7ead, long hair, dress, looking back,   . vibrant colors, swirling patterns, abstract forms, surreal, trippy . magnificent, celestial, ethereal, painterly, epic, majestic, magical, fantasy art, cover art, dreamy", "texture rock top down close-up"], "color": "#232", "bgcolor": "#353"},
    {"id": 272, "type": "PrimitiveNode", "pos": [570, 550], "size": {"0": 210, "1": 80}, "flags": {}, "order": 11, "mode": 0, "outputs": [{"name": "INT", "type": "INT", "links": [500, 506], "slot_index": 0, "widget": {"name": "seed"}, "label": "INT"}], "properties": {"Run widget replace on values": false}, "widgets_values": [386213472092672, "randomize"]},
    {"id": 93, "type": "ShowText|pysssss", "pos": [330, 610], "size": [220, 130], "flags": {"collapsed": false}, "order": 22, "mode": 0, "inputs": [{"name": "text", "type": "STRING", "link": 135, "widget": {"name": "text"}, "label": "\u6587\u672c"}], "outputs": [{"name": "STRING", "type": "STRING", "links": null, "shape": 6, "label": "\u5b57\u7b26\u4e32"}], "properties": {"Node name for S&R": "ShowText|pysssss"}, "widgets_values": [["photographic, realistic, realism, 35mm film, dslr, cropped, frame, text, deformed, glitch, noise, noisy, off-center, deformed, cross-eyed, closed eyes, bad anatomy, ugly, disfigured, sloppy, duplicate, mutated, black and white, monochrome, black and white, low contrast, realistic, photorealistic, plain, simple, text, watermark"], "photographic, realistic, realism, 35mm film, dslr, cropped, frame, text, deformed, glitch, noise, noisy, off-center, deformed, cross-eyed, closed eyes, bad anatomy, ugly, disfigured, sloppy, duplicate, mutated, black and white, monochrome, black and white, low contrast, realistic, photorealistic, plain, simple, text, watermark", "ugly, deformed, noisy, blurry"], "color": "#322", "bgcolor": "#533"},
    {"id": 5, "type": "EmptyLatentImage", "pos": [340, 800], "size": {"0": 210, "1": 110}, "flags": {}, "order": 12, "mode": 0, "outputs": [{"name": "LATENT", "type": "LATENT", "links": [431], "slot_index": 0, "label": "Latent"}], "properties": {"Node name for S&R": "EmptyLatentImage"}, "widgets_values": [1024, 1024, 1]}
  ],
  "links": [
    [6, 7, 0, 3, 2, "CONDITIONING"], [7, 3, 0, 8, 0, "LATENT"], [48, 6, 0, 3, 1, "CONDITIONING"], [98, 65, 0, 6, 1, "STRING"], [100, 65, 1, 7, 1, "STRING"], [134, 65, 0, 92, 0, "STRING"], [135, 65, 1, 93, 0, "STRING"], [204, 126, 0, 65, 0, "STRING"], [205, 127, 0, 65, 1, "STRING"], [208, 129, 0, 132, 0, "LATENT"], [223, 91, 0, 3, 0, "MODEL"], [224, 91, 1, 6, 0, "CLIP"], [225, 91, 1, 7, 0, "CLIP"], [282, 172, 0, 179, 3, "INT"], [289, 172, 0, 181, 3, "INT"], [290, 181, 0, 176, 0, "IMAGE"], [361, 179, 0, 211, 0, "IMAGE"], [405, 181, 0, 179, 0, "IMAGE"], [416, 220, 0, 221, 0, "IMAGE"], [422, 222, 0, 223, 0, "IMAGE"], [431, 5, 0, 3, 3, "LATENT"], [434, 227, 0, 181, 1, "IMAGE"], [435, 227, 0, 179, 1, "IMAGE"], [436, 227, 0, 220, 1, "IMAGE"], [437, 227, 0, 222, 1, "IMAGE"], [454, 91, 0, 249, 0, "MODEL"], [457, 91, 1, 245, 0, "CLIP"], [458, 91, 1, 246, 0, "CLIP"], [459, 245, 0, 249, 1, "CONDITIONING"], [460, 246, 0, 249, 2, "CONDITIONING"], [461, 249, 0, 248, 0, "LATENT"], [464, 65, 0, 245, 1, "STRING"], [465, 65, 1, 246, 1, "STRING"], [466, 132, 0, 249, 3, "LATENT"], [487, 248, 0, 263, 0, "IMAGE"], [489, 8, 0, 264, 0, "IMAGE"], [491, 206, 0, 132, 1, "MASK"], [493, 206, 0, 266, 0, "MASK"], [497, 268, 0, 249, 4, "FLOAT"], [498, 270, 0, 249, 5, "COMBO"], [499, 271, 0, 249, 6, "COMBO"], [500, 272, 0, 249, 7, "INT"], [501, 274, 0, 249, 8, "INT"], [502, 268, 0, 3, 6, "FLOAT"], [503, 274, 0, 3, 5, "INT"], [504, 270, 0, 3, 7, "COMBO"], [505, 271, 0, 3, 8, "COMBO"], [506, 272, 0, 3, 4, "INT"], [609, 91, 2, 306, 0, "*"], [613, 306, 0, 307, 0, "*"], [614, 307, 0, 129, 1, "VAE"], [615, 307, 0, 248, 1, "VAE"], [617, 306, 0, 8, 1, "VAE"], [633, 317, 0, 312, 0, "IMAGE"], [634, 317, 0, 312, 1, "IMAGE"], [635, 316, 0, 312, 3, "INT"], [636, 312, 0, 313, 0, "IMAGE"], [637, 317, 0, 313, 1, "IMAGE"], [638, 316, 0, 313, 3, "INT"], [639, 318, 0, 314, 0, "IMAGE"], [642, 313, 0, 318, 0, "IMAGE"], [643, 317, 0, 318, 1, "IMAGE"], [644, 316, 0, 318, 3, "INT"], [645, 316, 0, 318, 4, "INT"], [659, 286, 0, 290, 0, "IMAGE"], [660, 288, 0, 291, 0, "IMAGE"], [669, 179, 0, 222, 0, "IMAGE"], [670, 222, 0, 220, 0, "IMAGE"], [671, 220, 0, 129, 0, "IMAGE"], [675, 322, 0, 323, 0, "IMAGE"], [676, 266, 0, 322, 0, "IMAGE"], [677, 323, 0, 324, 0, "IMAGE"], [683, 324, 0, 312, 2, "MASK"], [684, 324, 0, 313, 2, "MASK"], [685, 324, 0, 318, 2, "MASK"], [687, 248, 0, 317, 0, "*"], [688, 314, 0, 327, 0, "IMAGE"], [689, 327, 0, 315, 0, "IMAGE"], [690, 327, 0, 286, 0, "IMAGE"], [691, 327, 0, 288, 0, "IMAGE"], [692, 327, 0, 319, 0, "*"], [693, 328, 0, 181, 0, "IMAGE"], [695, 8, 0, 227, 0, "*"]
  ],
  "groups": [
    {"title": "\u7b2c\u4e00\u6b65\uff1a\u51fa\u56fe", "bounding": [77, 93, 1313, 862], "color": "#3f789e", "font_size": 24, "locked": false},
    {"title": "\u7b2c\u4e8c\u6b65\uff1a\u62fc\u56fe", "bounding": [1428, 94, 1269, 862], "color": "#3f789e", "font_size": 24, "locked": false},
    {"title": "\u7b2c\u4e09\u6b65\uff1a\u6d88\u9664\u63a5\u7f1d", "bounding": [78, 992, 958, 771], "color": "#3f789e", "font_size": 24, "locked": false},
    {"title": "\u7b2c\u56db\u6b65\uff1a\u8f93\u51fa\u989c\u8272\u8d34\u56fe", "bounding": [1076, 993, 1266, 775], "color": "#3f789e", "font_size": 24, "locked": false},
    {"title": "\u7b2c\u4e94\u6b65\uff1a\u8f93\u51fa\u6cd5\u7ebf\u548c\u6df1\u5ea6\u8d34\u56fe", "bounding": [2371, 990, 932, 773], "color": "#3f789e", "font_size": 24, "locked": false}
  ],
  "config": {}, "extra": {}, "version": 0.4}
```

```json [æ— ç¼è´´å›¾-AIç”Ÿæˆv3.1]
{"last_node_id": 365, "last_link_id": 769, "nodes": [{"id": 245, "type": "CLIPTextEncode", "pos": [2670, 210], "size": {"0": 210, "1": 90}, "flags": {"collapsed": true}, "order": 14, "mode": 0, "inputs": [{"name": "clip", "type": "CLIP", "link": 457, "label": "CLIP"}, {"name": "text", "type": "STRING", "link": 464, "widget": {"name": "text"}, "label": "\u6587\u672c", "slot_index": 1}], "outputs": [{"name": "CONDITIONING", "type": "CONDITIONING", "links": [459], "slot_index": 0, "label": "\u6761\u4ef6"}], "properties": {"Node name for S&R": "CLIPTextEncode"}, "widgets_values": ["1girl"], "color": "#232", "bgcolor": "#353"}, {"id": 290, "type": "SaveImage", "pos": [1622.586564967992, 1231.5679835842157], "size": {"0": 390, "1": 460}, "flags": {}, "order": 45, "mode": 2, "inputs": [{"name": "images", "type": "IMAGE", "link": 650, "label": "\u56fe\u50cf"}], "properties": {}, "widgets_values": ["normal-map"]}, {"id": 291, "type": "SaveImage", "pos": [2072.5865649680004, 1221.5679835842157], "size": {"0": 380, "1": 460}, "flags": {}, "order": 46, "mode": 2, "inputs": [{"name": "images", "type": "IMAGE", "link": 659, "label": "\u56fe\u50cf"}], "properties": {}, "widgets_values": ["bump-map"]}, {"id": 288, "type": "AIO_Preprocessor", "pos": [2082.586564968002, 1081.567983584216], "size": {"0": 310, "1": 82}, "flags": {}, "order": 43, "mode": 2, "inputs": [{"name": "image", "type": "IMAGE", "link": 761, "label": "\u56fe\u50cf", "slot_index": 0}], "outputs": [{"name": "IMAGE", "type": "IMAGE", "links": [659], "shape": 3, "label": "\u56fe\u50cf", "slot_index": 0}], "properties": {"Node name for S&R": "AIO_Preprocessor"}, "widgets_values": ["MiDaS-DepthMapPreprocessor", 1024]}, {"id": 315, "type": "ImageScale", "pos": [594.8780563964839, 168.33799871826173], "size": {"0": 210, "1": 130}, "flags": {}, "order": 11, "mode": 0, "inputs": [{"name": "image", "type": "IMAGE", "link": 665, "label": "\u56fe\u50cf"}], "outputs": [{"name": "IMAGE", "type": "IMAGE", "links": [693], "shape": 3, "label": "\u56fe\u50cf", "slot_index": 0}], "properties": {"Node name for S&R": "ImageScale"}, "widgets_values": ["bilinear", 1024, 1024, "center"]}, {"id": 286, "type": "AIO_Preprocessor", "pos": [1632.586564967992, 1091.567983584216], "size": {"0": 310, "1": 82}, "flags": {}, "order": 42, "mode": 2, "inputs": [{"name": "image", "type": "IMAGE", "link": 760, "label": "\u56fe\u50cf", "slot_index": 0}], "outputs": [{"name": "IMAGE", "type": "IMAGE", "links": [650], "shape": 3, "label": "\u56fe\u50cf", "slot_index": 0}], "properties": {"Node name for S&R": "AIO_Preprocessor"}, "widgets_values": ["BAE-NormalMapPreprocessor", 1024]}, {"id": 337, "type": "Reroute", "pos": [840, 170], "size": [75, 26], "flags": {}, "order": 17, "mode": 0, "inputs": [{"name": "", "type": "*", "link": 693, "label": ""}], "outputs": [{"name": "", "type": "IMAGE", "links": [674, 677, 681, 684, 767], "slot_index": 0}], "properties": {"showOutputText": false, "horizontal": false}}, {"id": 330, "type": "Image Overlay", "pos": [590, 350], "size": {"0": 210, "1": 290}, "flags": {}, "order": 19, "mode": 0, "inputs": [{"name": "base_image", "type": "IMAGE", "link": 763, "label": "\u57fa\u7840\u56fe\u50cf", "slot_index": 0}, {"name": "overlay_image", "type": "IMAGE", "link": 677, "label": "\u8986\u76d6\u56fe\u50cf", "slot_index": 1}, {"name": "optional_mask", "type": "MASK", "link": null, "label": "\u906e\u7f69", "slot_index": 2}, {"name": "y_offset", "type": "INT", "link": 678, "widget": {"name": "y_offset"}, "label": "Y\u504f\u79fb", "slot_index": 4}], "outputs": [{"name": "IMAGE", "type": "IMAGE", "links": [672, 673], "shape": 3, "label": "\u56fe\u50cf", "slot_index": 0}], "properties": {"Node name for S&R": "Image Overlay"}, "widgets_values": ["None", "nearest-exact", 1, 0, 0, 0, 1020, 0, 0], "color": "#222233", "bgcolor": "#333355", "shape": 1}, {"id": 328, "type": "PreviewImage", "pos": [590, 680], "size": {"0": 210, "1": 250}, "flags": {}, "order": 22, "mode": 0, "inputs": [{"name": "images", "type": "IMAGE", "link": 672, "label": "\u56fe\u50cf"}], "properties": {"Node name for S&R": "PreviewImage"}}, {"id": 332, "type": "PreviewImage", "pos": [830, 680], "size": {"0": 210, "1": 250}, "flags": {}, "order": 25, "mode": 0, "inputs": [{"name": "images", "type": "IMAGE", "link": 679, "label": "\u56fe\u50cf"}], "properties": {"Node name for S&R": "PreviewImage"}}, {"id": 329, "type": "Image Overlay", "pos": [830, 300], "size": {"0": 210, "1": 290}, "flags": {}, "order": 23, "mode": 0, "inputs": [{"name": "base_image", "type": "IMAGE", "link": 673, "label": "\u57fa\u7840\u56fe\u50cf", "slot_index": 0}, {"name": "overlay_image", "type": "IMAGE", "link": 674, "label": "\u8986\u76d6\u56fe\u50cf", "slot_index": 1}, {"name": "optional_mask", "type": "MASK", "link": null, "label": "\u906e\u7f69", "slot_index": 2}, {"name": "x_offset", "type": "INT", "link": 675, "widget": {"name": "x_offset"}, "label": "X\u504f\u79fb", "slot_index": 3}], "outputs": [{"name": "IMAGE", "type": "IMAGE", "links": [679, 683], "shape": 3, "label": "\u56fe\u50cf", "slot_index": 0}], "properties": {"Node name for S&R": "Image Overlay"}, "widgets_values": ["None", "nearest-exact", 1, 0, 0, 1020, 0, 0, 0], "color": "#222233", "bgcolor": "#333355", "shape": 1}, {"id": 335, "type": "Image Overlay", "pos": [1060, 300], "size": {"0": 210, "1": 290}, "flags": {}, "order": 26, "mode": 0, "inputs": [{"name": "base_image", "type": "IMAGE", "link": 683, "label": "\u57fa\u7840\u56fe\u50cf", "slot_index": 0}, {"name": "overlay_image", "type": "IMAGE", "link": 684, "label": "\u8986\u76d6\u56fe\u50cf", "slot_index": 1}, {"name": "optional_mask", "type": "MASK", "link": null, "label": "\u906e\u7f69", "slot_index": 2}], "outputs": [{"name": "IMAGE", "type": "IMAGE", "links": [680, 685], "shape": 3, "label": "\u56fe\u50cf", "slot_index": 0}], "properties": {"Node name for S&R": "Image Overlay"}, "widgets_values": ["None", "nearest-exact", 1, 0, 0, 1020, 1020, 0, 0], "color": "#222233", "bgcolor": "#333355", "shape": 1}, {"id": 333, "type": "Image Overlay", "pos": [1300, 300], "size": {"0": 210, "1": 290}, "flags": {}, "order": 27, "mode": 0, "inputs": [{"name": "base_image", "type": "IMAGE", "link": 680, "label": "\u57fa\u7840\u56fe\u50cf", "slot_index": 0}, {"name": "overlay_image", "type": "IMAGE", "link": 681, "label": "\u8986\u76d6\u56fe\u50cf", "slot_index": 1}, {"name": "optional_mask", "type": "MASK", "link": null, "label": "\u906e\u7f69", "slot_index": 2}], "outputs": [{"name": "IMAGE", "type": "IMAGE", "links": [682, 689], "shape": 3, "label": "\u56fe\u50cf", "slot_index": 0}], "properties": {"Node name for S&R": "Image Overlay"}, "widgets_values": ["None", "nearest-exact", 1, 0, 0, 0, 0, 0, 0], "color": "#222233", "bgcolor": "#333355", "shape": 1}, {"id": 336, "type": "PreviewImage", "pos": [1070, 680], "size": {"0": 210, "1": 250}, "flags": {}, "order": 28, "mode": 0, "inputs": [{"name": "images", "type": "IMAGE", "link": 685, "label": "\u56fe\u50cf"}], "properties": {"Node name for S&R": "PreviewImage"}}, {"id": 334, "type": "PreviewImage", "pos": [1310, 680], "size": {"0": 210, "1": 250}, "flags": {}, "order": 29, "mode": 0, "inputs": [{"name": "images", "type": "IMAGE", "link": 682, "label": "\u56fe\u50cf"}], "properties": {"Node name for S&R": "PreviewImage"}}, {"id": 255, "type": "Image Overlay", "pos": [590, 1430], "size": {"0": 210, "1": 290}, "flags": {}, "order": 37, "mode": 0, "inputs": [{"name": "base_image", "type": "IMAGE", "link": 471, "label": "\u57fa\u7840\u56fe\u50cf", "slot_index": 0}, {"name": "overlay_image", "type": "IMAGE", "link": 619, "label": "\u8986\u76d6\u56fe\u50cf", "slot_index": 1}, {"name": "optional_mask", "type": "MASK", "link": 755, "label": "\u906e\u7f69", "slot_index": 2}, {"name": "y_offset", "type": "INT", "link": 495, "widget": {"name": "y_offset"}, "label": "Y\u504f\u79fb"}], "outputs": [{"name": "IMAGE", "type": "IMAGE", "links": [668], "shape": 3, "label": "\u56fe\u50cf", "slot_index": 0}], "properties": {"Node name for S&R": "Image Overlay"}, "widgets_values": ["None", "nearest-exact", 1, 1280, 256, 0, -1020, 0, 0], "color": "#222233", "bgcolor": "#333355", "shape": 1}, {"id": 265, "type": "SaveImage", "pos": [1070, 1260], "size": {"0": 440, "1": 460}, "flags": {}, "order": 44, "mode": 0, "inputs": [{"name": "images", "type": "IMAGE", "link": 762, "label": "\u56fe\u50cf"}], "properties": {}, "widgets_values": ["color-map"]}, {"id": 251, "type": "Image Overlay", "pos": [600, 1070], "size": {"0": 210, "1": 290}, "flags": {}, "order": 36, "mode": 0, "inputs": [{"name": "base_image", "type": "IMAGE", "link": 753, "label": "\u57fa\u7840\u56fe\u50cf", "slot_index": 0}, {"name": "overlay_image", "type": "IMAGE", "link": 618, "label": "\u8986\u76d6\u56fe\u50cf", "slot_index": 1}, {"name": "optional_mask", "type": "MASK", "link": 756, "label": "\u906e\u7f69", "slot_index": 2}, {"name": "x_offset", "type": "INT", "link": 496, "widget": {"name": "x_offset"}, "label": "X\u504f\u79fb"}], "outputs": [{"name": "IMAGE", "type": "IMAGE", "links": [471], "shape": 3, "label": "\u56fe\u50cf", "slot_index": 0}], "properties": {"Node name for S&R": "Image Overlay"}, "widgets_values": ["None", "nearest-exact", 1, 256, 1280, -1020, 0, 0, 0], "color": "#222233", "bgcolor": "#333355", "shape": 1}, {"id": 280, "type": "Reroute", "pos": [470, 1030], "size": [75, 26], "flags": {}, "order": 35, "mode": 0, "inputs": [{"name": "", "type": "*", "link": 754, "label": ""}], "outputs": [{"name": "", "type": "IMAGE", "links": [560, 618, 619, 753], "slot_index": 0, "label": ""}], "properties": {"showOutputText": false, "horizontal": false}}, {"id": 282, "type": "Image Overlay", "pos": [840, 1300], "size": {"0": 210, "1": 290}, "flags": {}, "order": 38, "mode": 0, "inputs": [{"name": "base_image", "type": "IMAGE", "link": 668, "label": "\u57fa\u7840\u56fe\u50cf", "slot_index": 0}, {"name": "overlay_image", "type": "IMAGE", "link": 560, "label": "\u8986\u76d6\u56fe\u50cf", "slot_index": 1}, {"name": "optional_mask", "type": "MASK", "link": 757, "label": "\u906e\u7f69", "slot_index": 2}, {"name": "x_offset", "type": "INT", "link": 550, "widget": {"name": "x_offset"}, "label": "X\u504f\u79fb", "slot_index": 3}, {"name": "y_offset", "type": "INT", "link": 551, "widget": {"name": "y_offset"}, "label": "Y\u504f\u79fb", "slot_index": 4}], "outputs": [{"name": "IMAGE", "type": "IMAGE", "links": [563], "shape": 3, "label": "\u56fe\u50cf", "slot_index": 0}], "properties": {"Node name for S&R": "Image Overlay"}, "widgets_values": ["None", "nearest-exact", 1, 256, 256, -1020, -1020, 0, 0], "color": "#222233", "bgcolor": "#333355", "shape": 1}, {"id": 260, "type": "ImageCrop", "pos": [840, 1080], "size": {"0": 210, "1": 130}, "flags": {}, "order": 39, "mode": 0, "inputs": [{"name": "image", "type": "IMAGE", "link": 563, "label": "\u56fe\u50cf", "slot_index": 0}], "outputs": [{"name": "IMAGE", "type": "IMAGE", "links": [758], "shape": 3, "label": "\u56fe\u50cf", "slot_index": 0}], "properties": {"Node name for S&R": "ImageCrop"}, "widgets_values": [1020, 1020, 0, 0]}, {"id": 308, "type": "PlaySound|pysssss", "pos": [1310, 1090], "size": {"0": 210, "1": 110}, "flags": {}, "order": 41, "mode": 0, "inputs": [{"name": "any", "type": "*", "link": 759, "label": "\u8f93\u5165"}], "outputs": [{"name": "*", "type": "*", "links": null, "shape": 6}], "properties": {"Node name for S&R": "PlaySound|pysssss"}, "widgets_values": ["always", 1, "notify.mp3"]}, {"id": 360, "type": "ImageToMask", "pos": [340, 1690], "size": {"0": 210, "1": 60}, "flags": {}, "order": 24, "mode": 0, "inputs": [{"name": "image", "type": "IMAGE", "link": 750, "label": "\u56fe\u50cf"}], "outputs": [{"name": "MASK", "type": "MASK", "links": [755, 756, 757], "shape": 3, "label": "\u906e\u7f69", "slot_index": 0}], "properties": {"Node name for S&R": "ImageToMask"}, "widgets_values": ["red"]}, {"id": 359, "type": "ImageCrop", "pos": [340, 1510], "size": {"0": 210, "1": 130}, "flags": {}, "order": 21, "mode": 0, "inputs": [{"name": "image", "type": "IMAGE", "link": 748, "label": "\u56fe\u50cf"}], "outputs": [{"name": "IMAGE", "type": "IMAGE", "links": [750], "shape": 3, "label": "\u56fe\u50cf", "slot_index": 0}], "properties": {"Node name for S&R": "ImageCrop"}, "widgets_values": [1276, 1276, 0, 0]}, {"id": 357, "type": "ImageScale", "pos": [340, 1330], "size": {"0": 210, "1": 130}, "flags": {}, "order": 18, "mode": 0, "inputs": [{"name": "image", "type": "IMAGE", "link": 746, "label": "\u56fe\u50cf"}], "outputs": [{"name": "IMAGE", "type": "IMAGE", "links": [748], "shape": 3, "label": "\u56fe\u50cf", "slot_index": 0}], "properties": {"Node name for S&R": "ImageScale"}, "widgets_values": ["bilinear", 1590, 1590, "disabled"]}, {"id": 267, "type": "PrimitiveNode", "pos": [340, 1100], "size": {"0": 210, "1": 80}, "flags": {}, "order": 0, "mode": 0, "outputs": [{"name": "INT", "type": "INT", "links": [495, 496, 550, 551], "slot_index": 0, "widget": {"name": "y_offset"}, "label": "INT"}], "properties": {"Run widget replace on values": false}, "widgets_values": [-1020, "fixed"]}, {"id": 266, "type": "MaskToImage", "pos": [400, 1240], "size": {"0": 140, "1": 30}, "flags": {}, "order": 12, "mode": 0, "inputs": [{"name": "mask", "type": "MASK", "link": 493, "label": "\u906e\u7f69"}], "outputs": [{"name": "IMAGE", "type": "IMAGE", "links": [746], "shape": 3, "label": "\u56fe\u50cf", "slot_index": 0}], "properties": {"Node name for S&R": "MaskToImage"}}, {"id": 361, "type": "ImageScale", "pos": [1080, 1080], "size": {"0": 210, "1": 130}, "flags": {}, "order": 40, "mode": 0, "inputs": [{"name": "image", "type": "IMAGE", "link": 758, "label": "\u56fe\u50cf"}], "outputs": [{"name": "IMAGE", "type": "IMAGE", "links": [759, 760, 761, 762], "shape": 3, "label": "\u56fe\u50cf", "slot_index": 0}], "properties": {"Node name for S&R": "ImageScale"}, "widgets_values": ["bilinear", 1024, 1024, "disabled"]}, {"id": 126, "type": "DeepTranslatorTextNode", "pos": [1890, 190], "size": [220, 260], "flags": {}, "order": 1, "mode": 0, "outputs": [{"name": "text", "type": "STRING", "links": [204], "shape": 3, "label": "\u6587\u672c", "slot_index": 0}], "properties": {"Node name for S&R": "DeepTranslatorTextNode"}, "widgets_values": ["auto", "english", "disable", "", "", "GoogleTranslator [free]", "rock", "proxy_hide", "authorization_hide"], "color": "#232", "bgcolor": "#353"}, {"id": 249, "type": "KSampler", "pos": [2370, 430], "size": {"0": 260, "1": 470}, "flags": {}, "order": 32, "mode": 0, "inputs": [{"name": "model", "type": "MODEL", "link": 768, "label": "\u6a21\u578b"}, {"name": "positive", "type": "CONDITIONING", "link": 459, "label": "\u6b63\u9762\u6761\u4ef6", "slot_index": 1}, {"name": "negative", "type": "CONDITIONING", "link": 460, "label": "\u8d1f\u9762\u6761\u4ef6", "slot_index": 2}, {"name": "latent_image", "type": "LATENT", "link": 466, "label": "Latent", "slot_index": 3}], "outputs": [{"name": "LATENT", "type": "LATENT", "links": [461], "slot_index": 0, "label": "Latent"}], "properties": {"Node name for S&R": "KSampler"}, "widgets_values": [1086404413043726, "randomize", 8, 1, "euler", "sgm_uniform", 1]}, {"id": 246, "type": "CLIPTextEncode", "pos": [2680, 250], "size": {"0": 210, "1": 100}, "flags": {"collapsed": true}, "order": 16, "mode": 0, "inputs": [{"name": "clip", "type": "CLIP", "link": 458, "label": "CLIP"}, {"name": "text", "type": "STRING", "link": 465, "widget": {"name": "text"}, "label": "\u6587\u672c", "slot_index": 1}], "outputs": [{"name": "CONDITIONING", "type": "CONDITIONING", "links": [460], "slot_index": 0, "label": "\u6761\u4ef6"}], "properties": {"Node name for S&R": "CLIPTextEncode"}, "widgets_values": ["text, watermark"], "color": "#322", "bgcolor": "#533"}, {"id": 65, "type": "SDXLPromptStyler", "pos": [2380, 180], "size": {"0": 230, "1": 170}, "flags": {"collapsed": false}, "order": 8, "mode": 0, "inputs": [{"name": "text_positive", "type": "STRING", "link": 204, "widget": {"name": "text_positive"}, "label": "\u6b63\u9762\u6761\u4ef6"}, {"name": "text_negative", "type": "STRING", "link": 205, "widget": {"name": "text_negative"}, "label": "\u8d1f\u9762\u6761\u4ef6"}], "outputs": [{"name": "positive_prompt_text_g", "type": "STRING", "links": [134, 464], "shape": 3, "label": "positive_prompt_text_g", "slot_index": 0}, {"name": "negative_prompt_text_g", "type": "STRING", "links": [135, 465], "shape": 3, "label": "negative_prompt_text_g", "slot_index": 1}], "properties": {"Node name for S&R": "SDXLPromptStyler"}, "widgets_values": ["1 girl, long hair, dress, 3/4 profile, ", "text, watermark", "sai-texture", true, true, true], "color": "#232", "bgcolor": "#353"}, {"id": 127, "type": "DeepTranslatorTextNode", "pos": [2130, 190], "size": [210, 260], "flags": {}, "order": 2, "mode": 0, "outputs": [{"name": "text", "type": "STRING", "links": [205], "shape": 3, "label": "\u6587\u672c", "slot_index": 0}], "properties": {"Node name for S&R": "DeepTranslatorTextNode"}, "widgets_values": ["auto", "english", "disable", "", "", "GoogleTranslator [free]", "", "proxy_hide", "authorization_hide"], "color": "#322", "bgcolor": "#533"}, {"id": 248, "type": "VAEDecode", "pos": [2680, 330], "size": {"0": 140, "1": 50}, "flags": {}, "order": 33, "mode": 0, "inputs": [{"name": "samples", "type": "LATENT", "link": 461, "label": "Latent"}, {"name": "vae", "type": "VAE", "link": 615, "label": "VAE", "slot_index": 1}], "outputs": [{"name": "IMAGE", "type": "IMAGE", "links": [487, 754], "slot_index": 0, "label": "\u56fe\u50cf"}], "properties": {"Node name for S&R": "VAEDecode"}}, {"id": 263, "type": "PreviewImage", "pos": [2660, 450], "size": {"0": 310, "1": 350}, "flags": {}, "order": 34, "mode": 0, "inputs": [{"name": "images", "type": "IMAGE", "link": 487, "label": "\u56fe\u50cf"}], "properties": {"Node name for S&R": "PreviewImage"}}, {"id": 363, "type": "EmptyImage", "pos": [350, 540], "size": {"0": 210, "1": 130}, "flags": {}, "order": 3, "mode": 0, "outputs": [{"name": "IMAGE", "type": "IMAGE", "links": [763], "shape": 3, "label": "\u56fe\u50cf", "slot_index": 0}], "properties": {"Node name for S&R": "EmptyImage"}, "widgets_values": [1276, 1276, 1, 999999]}, {"id": 327, "type": "PrimitiveNode", "pos": [350, 750], "size": {"0": 210, "1": 80}, "flags": {}, "order": 4, "mode": 0, "outputs": [{"name": "INT", "type": "INT", "links": [675, 678], "slot_index": 0, "widget": {"name": "x_offset"}, "label": "INT"}], "properties": {"Run widget replace on values": false}, "widgets_values": [1020, "fixed"]}, {"id": 91, "type": "CheckpointLoader|pysssss", "pos": [1590, 200], "size": {"0": 210, "1": 122}, "flags": {}, "order": 5, "mode": 0, "outputs": [{"name": "MODEL", "type": "MODEL", "links": [766], "shape": 3, "label": "\u6a21\u578b", "slot_index": 0}, {"name": "CLIP", "type": "CLIP", "links": [457, 458], "shape": 3, "label": "CLIP", "slot_index": 1}, {"name": "VAE", "type": "VAE", "links": [634], "shape": 3, "label": "VAE", "slot_index": 2}], "properties": {"Node name for S&R": "CheckpointLoader|pysssss"}, "widgets_values": [{"content": "SDXL-lightning/sdxl_lightning_8step.safetensors", "image": null}, "[none]"]}, {"id": 326, "type": "LoadImage", "pos": [344.878056396484, 178.33799871826173], "size": [220, 310], "flags": {}, "order": 6, "mode": 0, "outputs": [{"name": "IMAGE", "type": "IMAGE", "links": [665], "shape": 3, "label": "\u56fe\u50cf", "slot_index": 0}, {"name": "MASK", "type": "MASK", "links": null, "shape": 3, "label": "\u906e\u7f69"}], "properties": {"Node name for S&R": "LoadImage"}, "widgets_values": ["u=3985828024,2950789954&fm=193.jpeg", "image"]}, {"id": 206, "type": "LoadImageMask", "pos": [1600, 600], "size": {"0": 220, "1": 320}, "flags": {}, "order": 7, "mode": 0, "outputs": [{"name": "MASK", "type": "MASK", "links": [491, 493], "shape": 3, "label": "\u906e\u7f69", "slot_index": 0}], "properties": {"Node name for S&R": "LoadImageMask"}, "widgets_values": ["alpha \u8d34\u56fe.png", "alpha", "image"]}, {"id": 129, "type": "VAEEncode", "pos": [1650, 450], "size": {"0": 140, "1": 46}, "flags": {}, "order": 30, "mode": 0, "inputs": [{"name": "pixels", "type": "IMAGE", "link": 689, "label": "\u56fe\u50cf"}, {"name": "vae", "type": "VAE", "link": 614, "label": "VAE", "slot_index": 1}], "outputs": [{"name": "LATENT", "type": "LATENT", "links": [208], "shape": 3, "label": "Latent", "slot_index": 0}], "properties": {"Node name for S&R": "VAEEncode"}}, {"id": 92, "type": "ShowText|pysssss", "pos": [1890, 500], "size": {"0": 210, "1": 130}, "flags": {"collapsed": false}, "order": 13, "mode": 0, "inputs": [{"name": "text", "type": "STRING", "link": 134, "widget": {"name": "text"}, "label": "\u6587\u672c"}], "outputs": [{"name": "STRING", "type": "STRING", "links": null, "shape": 6, "label": "\u5b57\u7b26\u4e32"}], "properties": {"Node name for S&R": "ShowText|pysssss"}, "widgets_values": [["ethereal fantasy concept art of  psychedelic style 1 girl, \u9648\u5c0f\u7ead, long hair, dress, looking back,   . vibrant colors, swirling patterns, abstract forms, surreal, trippy . magnificent, celestial, ethereal, painterly, epic, majestic, magical, fantasy art, cover art, dreamy"], "ethereal fantasy concept art of  psychedelic style 1 girl, \u9648\u5c0f\u7ead, long hair, dress, looking back,   . vibrant colors, swirling patterns, abstract forms, surreal, trippy . magnificent, celestial, ethereal, painterly, epic, majestic, magical, fantasy art, cover art, dreamy", "texture rock top down close-up"], "color": "#232", "bgcolor": "#353"}, {"id": 93, "type": "ShowText|pysssss", "pos": [2120, 510], "size": {"0": 220, "1": 130}, "flags": {"collapsed": false}, "order": 15, "mode": 0, "inputs": [{"name": "text", "type": "STRING", "link": 135, "widget": {"name": "text"}, "label": "\u6587\u672c"}], "outputs": [{"name": "STRING", "type": "STRING", "links": null, "shape": 6, "label": "\u5b57\u7b26\u4e32"}], "properties": {"Node name for S&R": "ShowText|pysssss"}, "widgets_values": [["photographic, realistic, realism, 35mm film, dslr, cropped, frame, text, deformed, glitch, noise, noisy, off-center, deformed, cross-eyed, closed eyes, bad anatomy, ugly, disfigured, sloppy, duplicate, mutated, black and white, monochrome, black and white, low contrast, realistic, photorealistic, plain, simple, text, watermark"], "photographic, realistic, realism, 35mm film, dslr, cropped, frame, text, deformed, glitch, noise, noisy, off-center, deformed, cross-eyed, closed eyes, bad anatomy, ugly, disfigured, sloppy, duplicate, mutated, black and white, monochrome, black and white, low contrast, realistic, photorealistic, plain, simple, text, watermark", "ugly, deformed, noisy, blurry"], "color": "#322", "bgcolor": "#533"}, {"id": 365, "type": "IPAdapterUnifiedLoader", "pos": [1870, 730], "size": {"0": 240, "1": 80}, "flags": {"collapsed": false}, "order": 9, "mode": 0, "inputs": [{"name": "model", "type": "MODEL", "link": 766, "label": "model", "slot_index": 0}, {"name": "ipadapter", "type": "IPADAPTER", "link": null, "label": "ipadapter"}], "outputs": [{"name": "model", "type": "MODEL", "links": [769], "shape": 3, "label": "model"}, {"name": "ipadapter", "type": "IPADAPTER", "links": [764], "shape": 3, "label": "ipadapter"}], "properties": {"Node name for S&R": "IPAdapterUnifiedLoader"}, "widgets_values": ["PLUS (high strength)"]}, {"id": 364, "type": "IPAdapter", "pos": [2130, 730], "size": {"0": 210, "1": 170}, "flags": {}, "order": 20, "mode": 0, "inputs": [{"name": "model", "type": "MODEL", "link": 769, "label": "\u6a21\u578b", "slot_index": 0}, {"name": "ipadapter", "type": "IPADAPTER", "link": 764, "slot_index": 1}, {"name": "image", "type": "IMAGE", "link": 767, "label": "\u56fe\u50cf", "slot_index": 2}, {"name": "attn_mask", "type": "MASK", "link": null}], "outputs": [{"name": "MODEL", "type": "MODEL", "links": [768], "shape": 3, "label": "\u6a21\u578b", "slot_index": 0}], "properties": {"Node name for S&R": "IPAdapter"}, "widgets_values": [0.8, 0, 1]}, {"id": 132, "type": "SetLatentNoiseMask", "pos": [1880, 870], "size": {"0": 140, "1": 50}, "flags": {}, "order": 31, "mode": 0, "inputs": [{"name": "samples", "type": "LATENT", "link": 208, "label": "Latent"}, {"name": "mask", "type": "MASK", "link": 491, "label": "\u906e\u7f69", "slot_index": 1}], "outputs": [{"name": "LATENT", "type": "LATENT", "links": [466], "shape": 3, "label": "Latent", "slot_index": 0}], "properties": {"Node name for S&R": "SetLatentNoiseMask"}}, {"id": 307, "type": "Reroute", "pos": [1710, 360], "size": [75, 26], "flags": {}, "order": 10, "mode": 0, "inputs": [{"name": "", "type": "*", "link": 634}], "outputs": [{"name": "", "type": "VAE", "links": [614, 615], "slot_index": 0}], "properties": {"showOutputText": false, "horizontal": false}}], "links": [[134, 65, 0, 92, 0, "STRING"], [135, 65, 1, 93, 0, "STRING"], [204, 126, 0, 65, 0, "STRING"], [205, 127, 0, 65, 1, "STRING"], [208, 129, 0, 132, 0, "LATENT"], [457, 91, 1, 245, 0, "CLIP"], [458, 91, 1, 246, 0, "CLIP"], [459, 245, 0, 249, 1, "CONDITIONING"], [460, 246, 0, 249, 2, "CONDITIONING"], [461, 249, 0, 248, 0, "LATENT"], [464, 65, 0, 245, 1, "STRING"], [465, 65, 1, 246, 1, "STRING"], [466, 132, 0, 249, 3, "LATENT"], [471, 251, 0, 255, 0, "IMAGE"], [487, 248, 0, 263, 0, "IMAGE"], [491, 206, 0, 132, 1, "MASK"], [493, 206, 0, 266, 0, "MASK"], [495, 267, 0, 255, 3, "INT"], [496, 267, 0, 251, 3, "INT"], [550, 267, 0, 282, 3, "INT"], [551, 267, 0, 282, 4, "INT"], [560, 280, 0, 282, 1, "IMAGE"], [563, 282, 0, 260, 0, "IMAGE"], [614, 307, 0, 129, 1, "VAE"], [615, 307, 0, 248, 1, "VAE"], [618, 280, 0, 251, 1, "IMAGE"], [619, 280, 0, 255, 1, "IMAGE"], [634, 91, 2, 307, 0, "*"], [650, 286, 0, 290, 0, "IMAGE"], [659, 288, 0, 291, 0, "IMAGE"], [665, 326, 0, 315, 0, "IMAGE"], [668, 255, 0, 282, 0, "IMAGE"], [672, 330, 0, 328, 0, "IMAGE"], [673, 330, 0, 329, 0, "IMAGE"], [674, 337, 0, 329, 1, "IMAGE"], [675, 327, 0, 329, 3, "INT"], [677, 337, 0, 330, 1, "IMAGE"], [678, 327, 0, 330, 3, "INT"], [679, 329, 0, 332, 0, "IMAGE"], [680, 335, 0, 333, 0, "IMAGE"], [681, 337, 0, 333, 1, "IMAGE"], [682, 333, 0, 334, 0, "IMAGE"], [683, 329, 0, 335, 0, "IMAGE"], [684, 337, 0, 335, 1, "IMAGE"], [685, 335, 0, 336, 0, "IMAGE"], [689, 333, 0, 129, 0, "IMAGE"], [693, 315, 0, 337, 0, "*"], [746, 266, 0, 357, 0, "IMAGE"], [748, 357, 0, 359, 0, "IMAGE"], [750, 359, 0, 360, 0, "IMAGE"], [753, 280, 0, 251, 0, "IMAGE"], [754, 248, 0, 280, 0, "*"], [755, 360, 0, 255, 2, "MASK"], [756, 360, 0, 251, 2, "MASK"], [757, 360, 0, 282, 2, "MASK"], [758, 260, 0, 361, 0, "IMAGE"], [759, 361, 0, 308, 0, "*"], [760, 361, 0, 286, 0, "IMAGE"], [761, 361, 0, 288, 0, "IMAGE"], [762, 361, 0, 265, 0, "IMAGE"], [763, 363, 0, 330, 0, "IMAGE"], [764, 365, 1, 364, 1, "IPADAPTER"], [766, 91, 0, 365, 0, "MODEL"], [767, 337, 0, 364, 2, "IMAGE"], [768, 364, 0, 249, 0, "MODEL"], [769, 365, 0, 364, 0, "MODEL"]], "groups": [{"title": "\u7b2c\u4e00\u6b65\uff1a\u4e0a\u4f20\u7d20\u6750+\u62fc\u56fe", "bounding": [305, 85, 1242, 869], "color": "#3f789e", "font_size": 24, "locked": false}, {"title": "\u7b2c\u4e8c\u6b65\uff1a\u6d88\u9664\u63a5\u7f1d", "bounding": [1574, 86, 1419, 865], "color": "#3f789e", "font_size": 24, "locked": false}, {"title": "\u7b2c\u4e09\u6b65\uff1a\u8f93\u51fa\u989c\u8272\u8d34\u56fe", "bounding": [301, 977, 1246, 793], "color": "#3f789e", "font_size": 24, "locked": false}, {"title": "\u7b2c\u56db\u6b65\uff1a\u8f93\u51fa\u6df1\u5ea6\u56fe+\u6cd5\u7ebf\u56fe", "bounding": [1576, 981, 907, 791], "color": "#3f789e", "font_size": 24, "locked": false}], "config": {}, "extra": {}, "version": 0.4}
```
:::

æ¶‰åŠå·¥å…·/èŠ‚ç‚¹ï¼š
- ipadapteråœ°å€ï¼šhttps://github.com/cubiq/ComfyUI_IPAdapter_plus
- æ•ˆç‡èŠ‚ç‚¹åœ°å€ï¼šhttps://github.com/jags111/efficiency-nodes-comfyui
- SDXLé£æ ¼é€‰æ‹©æ’ä»¶ï¼ˆæ±‰åŒ–ç‰ˆï¼‰https://github.com/ZHO-ZHO-ZHO/sdxl_prompt_styler-Zh-Chinese
- https://github.com/bash-j/mikey_nodes

Model:
- https://huggingface.co/ByteDance/SDXL-Lightning

ç»“åˆComfyUI_IPAdapter_plusé¡¹ç›®çš„`IPAdapterPlus.py`å’Œ`utils.py`(2024.5)
- å°†`CLIP-ViT-H-14-laion2B-s32B-b79K.safetensors`æ”¾å…¥`models\clip_vision`ä¸­
- å°†`ip-adapter-plus_sdxl_vit-h.safetensors`æ”¾å…¥`models\ipadapter`ä¸­(if folder not exist, create one)


## æ··å…ƒ-DiT

[Project](https://dit.hunyuan.tencent.com/) | [Paper](https://arxiv.org/abs/2405.08748) | [Model](https://huggingface.co/Tencent-Hunyuan/HunyuanDiT) | [Code](https://github.com/tencent/HunyuanDiT)

æ··å…ƒ-DiTæ˜¯è…¾è®¯æå‡ºçš„ä¸€ä¸ªæ”¯æŒä¸­è‹±æ–‡ç”Ÿæˆå›¾ç‰‡çš„æ¨¡å‹ã€‚

### CLIP model
ä½¿ç”¨äº†ä¸€ç§bilingual CLIPã€‚

åœ¨`hydit/inference.py`ä¸­ï¼Œæœ‰ï¼š
```python
#...
from transformers import BertModel, BertTokenizer
# ...
from .diffusion.pipeline import StableDiffusionPipeline
# ...

def get_pipeline(args, vae, text_encoder, tokenizer, model, device, rank,
                 embedder_t5, infer_mode, sampler=None):
    #...
    pipeline = StableDiffusionPipeline(vae=vae,
                                       text_encoder=text_encoder,
                                       tokenizer=tokenizer,
                                       unet=model,
                                       scheduler=scheduler,
                                       feature_extractor=None,
                                       safety_checker=None,
                                       requires_safety_checker=False,
                                       progress_bar_config=progress_bar_config,
                                       embedder_t5=embedder_t5,
                                       infer_mode=infer_mode,
                                       )

    pipeline = pipeline.to(device)

    return pipeline, sampler
# ...
class End2End(object):
    def __init__(self, args, models_root_path):
        # ...
        # ========================================================================
        logger.info(f"Loading CLIP Text Encoder...")
        text_encoder_path = self.root / "clip_text_encoder"
        self.clip_text_encoder = BertModel.from_pretrained(str(text_encoder_path), False, revision=None).to(self.device)
        logger.info(f"Loading CLIP Text Encoder finished")

        # ========================================================================
        logger.info(f"Loading CLIP Tokenizer...")
        tokenizer_path = self.root / "tokenizer"
        self.tokenizer = BertTokenizer.from_pretrained(str(tokenizer_path))
        logger.info(f"Loading CLIP Tokenizer finished")
        # ...
        self.pipeline, self.sampler = self.load_sampler()
        # ...

    def load_sampler(self, sampler=None):
        pipeline, sampler = get_pipeline(self.args,
                                         self.vae,
                                         self.clip_text_encoder,
                                         self.tokenizer,
                                         self.model,
                                         device=self.device,
                                         rank=0,
                                         embedder_t5=self.embedder_t5,
                                         infer_mode=self.infer_mode,
                                         sampler=sampler,
                                         )
        return pipeline, sampler
    # ...
    def predict(self,
                user_prompt,
                height=1024,
                width=1024,
                seed=None,
                enhanced_prompt=None,
                negative_prompt=None,
                infer_steps=100,
                guidance_scale=6,
                batch_size=1,
                src_size_cond=(1024, 1024),
                sampler=None,
                ):
        # ...

        samples = self.pipeline(
            height=target_height,
            width=target_width,
            prompt=prompt,
            negative_prompt=negative_prompt,
            num_images_per_prompt=batch_size,
            guidance_scale=guidance_scale,
            num_inference_steps=infer_steps,
            image_meta_size=image_meta_size,
            style=style,
            return_dict=False,
            generator=generator,
            freqs_cis_img=freqs_cis_img,
            use_fp16=self.args.use_fp16,
            learn_sigma=self.args.learn_sigma,
        )[0]
        gen_time = time.time() - start_time
        logger.debug(f"Success, time: {gen_time}")

        return {
            'images': samples,
            'seed': seed,
        }
```
åœ¨`hydit/diffusion/pipeline.py`ä¸­ï¼Œæœ‰ï¼š
```python
# ...
from diffusers.pipelines.pipeline_utils import DiffusionPipeline
# ...
class StableDiffusionPipeline(DiffusionPipeline, TextualInversionLoaderMixin, LoraLoaderMixin, FromSingleFileMixin):
    # ...
    def __init__(
        self,
        vae: AutoencoderKL,
        text_encoder: Union[BertModel, CLIPTextModel],
        tokenizer: Union[BertTokenizer, CLIPTokenizer],
        unet: Union[HunYuanDiT, UNet2DConditionModel],
        scheduler: KarrasDiffusionSchedulers,
        safety_checker: StableDiffusionSafetyChecker,
        feature_extractor: CLIPImageProcessor,
        requires_safety_checker: bool = True,
        progress_bar_config: Dict[str, Any] = None,
        embedder_t5=None,
        infer_mode='torch',
    ):
        super().__init__()
        # ...
        self.register_modules(
            vae=vae,
            text_encoder=text_encoder,
            tokenizer=tokenizer,
            unet=unet,
            scheduler=scheduler,
            safety_checker=safety_checker,
            feature_extractor=feature_extractor,
        )
        # ...
    @torch.no_grad()
    @replace_example_docstring(EXAMPLE_DOC_STRING)
    def __call__(
            self,
            height: int,
            width: int,
            prompt: Union[str, List[str]] = None,
            num_inference_steps: Optional[int] = 50,
            guidance_scale: Optional[float] = 7.5,
            negative_prompt: Optional[Union[str, List[str]]] = None,
            num_images_per_prompt: Optional[int] = 1,
            eta: Optional[float] = 0.0,
            generator: Optional[Union[torch.Generator, List[torch.Generator]]] = None,
            latents: Optional[torch.FloatTensor] = None,
            prompt_embeds: Optional[torch.FloatTensor] = None,
            prompt_embeds_t5: Optional[torch.FloatTensor] = None,
            negative_prompt_embeds: Optional[torch.FloatTensor] = None,
            negative_prompt_embeds_t5: Optional[torch.FloatTensor] = None,
            output_type: Optional[str] = "pil",
            return_dict: bool = True,
            callback: Optional[Callable[[int, int, torch.FloatTensor, torch.FloatTensor], None]] = None,
            callback_steps: int = 1,
            cross_attention_kwargs: Optional[Dict[str, Any]] = None,
            guidance_rescale: float = 0.0,
            image_meta_size: Optional[torch.LongTensor] = None,
            style: Optional[torch.LongTensor] = None,
            progress: bool = True,
            use_fp16: bool = False,
            freqs_cis_img: Optional[tuple] = None,
            learn_sigma: bool = True,
    ):
        # ...
```

å†åé¢æŸ¥ä¸‹å»å°±åˆ°diffuserå†…éƒ¨åº“äº†ï¼ˆç”¨äº†å¾ˆå¤šç»§æ‰¿çš„å˜é‡å’Œæ–¹æ³•ï¼‰ï¼Œç»¼ä¸Šä¸€é¡¿çåˆ†æå¯çŸ¥ï¼Œå¦‚æœæƒ³å•ç‹¬ä½¿ç”¨â€œCLIPâ€æ¨¡å‹ï¼Œå…ˆä¸‹è½½[Model](https://huggingface.co/Tencent-Hunyuan/HunyuanDiT) ä¸­`clip_text_encoder`å’Œ`tokenizer`çš„æ¨¡å‹ï¼š
```python
import torch
from transformers import BertTokenizer, BertModel

# è®¾ç½®æ¨¡å‹å’ŒTokenizerçš„è·¯å¾„
model_dir = "hunyuanDiT/clip_text_encoder"  # åŒ…å«config.jsonå’Œpytorch_model.binçš„ç›®å½•
tokenizer_dir = "hunyuanDiT/tokenizer"  # åŒ…å«tokenizeræ–‡ä»¶çš„ç›®å½•

# åŠ è½½Tokenizerå’Œæ¨¡å‹
tokenizer = BertTokenizer.from_pretrained(tokenizer_dir)
model = BertModel.from_pretrained(model_dir)

# å‡†å¤‡è¾“å…¥æ–‡æœ¬
text = "ä½ å¥½ï¼Œä¸–ç•Œï¼"
inputs = tokenizer(text, return_tensors="pt", max_length=512, truncation=True, padding="max_length")

# è¿›è¡Œæ¨ç†
with torch.no_grad():
    outputs = model(**inputs)

# è¾“å‡ºç»“æœ
last_hidden_state = outputs.last_hidden_state
print(last_hidden_state)
print("Shape:", last_hidden_state.shape)


```

åœ¨`dialoggen/llava/model/multimodal_encoder/clip_encoder.py`ä¸­
```python
from transformers import CLIPVisionModel, CLIPImageProcessor, CLIPVisionConfig

```

## å¹³é¢è®¾è®¡
[ArchiGAN](https://developer.nvidia.com/blog/archigan-generative-stack-apartment-building-design/?linkId=70968833)

å›¾ç¥ç»ç½‘ç»œæ–¹æ³•
[News](https://baijiahao.baidu.com/s?id=1678104857914261902) | [Paper](https://link.springer.com/chapter/10.1007/978-3-030-66823-5_27)(ECCV 2020)

GANï¼š
https://blog.csdn.net/qq_28941587/article/details/104104823

## Motion

- MDM: [Project](https://guytevet.github.io/mdm-page/) | [Paper](https://arxiv.org/abs/2209.14916) | [Code](https://github.com/GuyTevet/motion-diffusion-model)
- MLD: [Paper](https://arxiv.org/abs/2212.04048) | [Code](https://github.com/ChenFengYe/motion-latent-diffusion)
- T2M-GPT: [Project](https://mael-zys.github.io/T2M-GPT/) | [Paper](https://arxiv.org/abs/2301.06052) | [Code](https://github.com/Mael-zys/T2M-GPT)
- **MoMask**:
[Project](https://ericguo5513.github.io/momask) | [Paper](https://arxiv.org/abs/2312.00063) | [Github](https://github.com/EricGuo5513/momask-codes/tree/main)

Application
- Text-to-motion: is the task of generating motion given an input text prompt.
- Action-to-motion: is the task of generating motion given an input action class, represented by a scalar.
- Motion Editing
  1. Body Part Editing: fix the joints we donâ€™t want to edit and leave the model to generate the rest.
  2. Motion In-Betweening: fix the first and last 25% of the motion, leaving the model to generate the remaining
50% in the middle.

## 3D

- Depth Map
- 3D-GPT: [Project](https://chuny1.github.io/3DGPT/3dgpt.html) | [Paper](https://arxiv.org/abs/2310.12945) | [Code](https://github.com/Chuny1/3DGPT)
- DreamScene: [Project](https://dreamscene-project.github.io/) | [Paper](https://arxiv.org/abs/2404.03575) | [Code](https://github.com/DreamScene-Project/DreamScene)
- DreamScene360: [Paper](https://arxiv.org/abs/2404.06903)
- Text2Room: [Project](https://lukashoel.github.io/text-to-room/) | [Code](https://github.com/lukasHoel/text2room)
- Text2NeRF: [Project](https://eckertzhang.github.io/Text2NeRF.github.io/) | [Code](https://github.com/eckertzhang/Text2NeRF)
- GaussianCube: [Project](https://gaussiancube.github.io/) | [Paper](https://arxiv.org/abs/2403.19655) | [Code](https://github.com/GaussianCube/GaussianCube)
- ReconFusion: [Project](https://reconfusion.github.io/)
- TripoSR: [Demo](https://huggingface.co/spaces/stabilityai/TripoSR)
- Stable Fast 3D: [Project](https://stable-fast-3d.github.io) | [Paper](https://arxiv.org/abs/2408.00653) | [Demo](https://huggingface.co/spaces/stabilityai/stable-fast-3d)
- GRM: [Project](https://justimyhxu.github.io/projects/grm/) | [Demoç›®å‰æ— æ•ˆ](https://huggingface.co/spaces/GRM-demo/GRM)
- InstantMesh: [Project](https://github.com/TencentARC/InstantMesh?tab=readme-ov-file) | [Demo](https://huggingface.co/spaces/TencentARC/InstantMesh)
  - å¯ä»¥ç”Ÿæˆ`.obj`å’Œ`.glb`æ¨¡å‹ã€‚
- VFUsion3D: [Demo](https://huggingface.co/spaces/facebook/VFusion3D)

### 3D Editing
[GaussCtrl](https://gaussctrl.active.vision/)

### æ¨¡å‹é‡ä¼˜åŒ–
InFusion: [Project](https://johanan528.github.io/Infusion/) | [Code](https://github.com/ali-vilab/infusion) | [Model](https://huggingface.co/Johanan0528/Infusion/tree/main)

é¦–å…ˆè®­ç»ƒä¸€ä¸ªé«˜æ–¯æ¨¡å‹ï¼ˆå¾…ä¼˜åŒ–ï¼‰ï¼Œç„¶åæŠŠè¿™ä¸ªæ¨¡å‹æ¸²æŸ“å‡ºæ¥ã€‚ç„¶åæŒ‘é€‰ä¸€å¼ æ¸²æŸ“å›¾ç‰‡ï¼ŒæŠŠè¦ç¼–è¾‘çš„éƒ¨ä½ç”»å‡ºæ¥ï¼ŒåšæˆMaskå›¾åƒï¼Œé€šè¿‡SDXL-Inpaintingè¿™å¼ å›¾åƒã€‚å¾—åˆ°inpaintåçš„å›¾åƒï¼Œç„¶åè®­ç»ƒæ¨¡å‹ï¼Œæœ€åå¾®è°ƒæ¨¡å‹ã€‚

å¦‚æœèƒ½é€šè¿‡UNetè‡ªåŠ¨æ£€æµ‹ç¼ºé™·éƒ¨ä½ï¼Œç”ŸæˆMaskï¼Œé‚£ä¹ˆå°±å¯ä»¥è‡ªåŠ¨åŒ–ã€‚

### CAT3D
[Project](https://cat3d.github.io/) | [Paper](https://arxiv.org/abs/2405.10314)

ä¹‹å‰çš„å·¥ä½œæ˜¯ä¾§é‡äºå¦‚ä½•æ›´å¥½åœ°é‡å»ºæ¨¡å‹/æå‡å•å›¾é‡å»ºæ¨¡å‹çš„è´¨é‡ï¼Œä½†è¿™ç¯‡æ–‡ç« çš„ä¾§é‡ç‚¹æ˜¯å¦‚ä½•é€šè¿‡diffusion model äº§ç”Ÿæ›´å¤šè§†è§’çš„å›¾åƒï¼Œè§£å†³æœ€å¤§çš„ç—›ç‚¹ã€‚

<div class="theme-image">
  <img src="./assets/CAT3D.png" alt="Light Mode Image" class="light-mode">
  <img src="./assets/dark_CAT3D.png" alt="Dark Mode Image" class="dark-mode">
</div>

CAT3D has two stages:

(1) generate a large set of synthetic views from a **multi-view latent diffusion model** conditioned on the input views alongside
the camera poses of target views;

(2) run a **robust 3D reconstruction pipeline** on the observed and
generated views to learn a NeRF representation.

CAT3Dæœ€ç»ˆå¯ä»¥é€šè¿‡å¤šå¼ å›¾åƒã€å•å¼ å›¾åƒæˆ–çº¯æ–‡æœ¬ç”Ÿæˆ3Dæ¨¡å‹ã€‚

### LGM
[Project](https://me.kiui.moe/lgm/) | [Paper](https://arxiv.org/abs/2402.05054) | [Demo](https://huggingface.co/spaces/ashawkey/LGM)

ç”Ÿæˆé«˜æ–¯æ¨¡å‹

æ¨èç¯å¢ƒï¼šCUDA 11.8ä»¥ä¸Š

![img](assets/LGM.png)

#### Run
```shell
CUDA_VISIBLE_DEVICES=0 python3 app.py big --resume pretrained/model_fp16_fixrot.safetensors
```

#### ä»£ç åˆ†C
- å¤šè§†è§’å›¾ç‰‡

ä»£ç ä¸­å·²ç»å¾ˆæ˜ç¡®æ ‡æ³¨äº†å„ä¸ªTensorçš„å°ºå¯¸ï¼Œæ–‡æœ¬ç”Ÿæˆå’Œå›¾åƒç”Ÿæˆåˆ†åˆ«å¯¹åº”äº`pipe_text`å’Œ`pipe_image`ï¼ˆMVDreamå’ŒImageDreamï¼‰

å¯¹äºå›¾ç”Ÿå¤šè§†è§’å›¾ç‰‡ï¼Œè¿™é‡Œé¦–å…ˆä½¿ç”¨äº†`rembg`åº“ï¼ˆè‡ªåŠ¨ä¼šä¸‹è½½ä¸€ä¸ªUNetï¼‰è·å–èƒŒæ™¯çš„maskï¼Œç„¶åé€šè¿‡[`kiui.op.recenter()`](https://kit.kiui.moe/ops/#kiui.op.recenter)æ–¹æ³•è·å–ä¸€ä¸ªæ–°å›¾ç‰‡ï¼Œæœ€åå°†è¿™ä¸ªå›¾ç‰‡å½’ä¸€åŒ–ï¼Œç”±0-255å½’ä¸€åŒ–åˆ°0-1ã€‚

- é«˜æ–¯æ¸²æŸ“

å¯ä»¥çœ‹åˆ°é‡Œé¢æ¶‰åŠä¸€ä¸ªåº“ï¼š[diff-gaussian-rasterization](https://github.com/ashawkey/diff-gaussian-rasterization)

ä¿å­˜å’Œè¯»å–é«˜æ–¯æ¨¡å‹æ—¶ä½¿ç”¨å¦ä¸€ä¸ªåº“: [plyfile](https://python-plyfile.readthedocs.io/en/latest/) ï¼ˆè¿™ä¸¤ä¸ªå‡½æ•°éƒ½ä»–å¦ˆæ˜¯é™æ€çš„ï¼Œè¿˜éå¾—å†™åœ¨ç±»é‡Œé¢åŠ ä¸ªselfï¼‰

å¯ä»¥çœ‹åˆ°æ¯ä¸ªé«˜æ–¯æ¨¡å‹æ˜¯ä½¿ç”¨ä¸€ä¸ªçŸ©é˜µå­˜å‚¨ä¿¡æ¯ï¼ˆ3ç»´å¼ é‡ï¼Œå®é™…ä¸Šåªæœ‰2ç»´ï¼Œç¬¬ä¸€ä¸ªç»´åº¦é•¿åº¦æ˜¯1ï¼‰ï¼Œå½¢çŠ¶ä¸º(N,14),å…¶ä¸­ï¼Œé¢œè‰²ä¸ºæœ€å3ä¸ªç»´åº¦ï¼ˆå½’ä¸€åŒ–ï¼‰ï¼Œä¾æ¬¡ä¸ºR,G,Bã€‚
```python
        means3D = gaussians[0, :, 0:3].contiguous().float()
        opacity = gaussians[0, :, 3:4].contiguous().float()
        scales = gaussians[0, :, 4:7].contiguous().float()
        rotations = gaussians[0, :, 7:11].contiguous().float()
        shs = gaussians[0, :, 11:].unsqueeze(1).contiguous().float() # [N, 1, 3]
```

```python
gaussians[:, :, -3] # çº¢è‰²é€šé“
gaussians[:, :, -2] # ç»¿è‰²é€šé“
gaussians[:, :, -1] # è“è‰²é€šé“
```

### Unique3D
[Project](https://wukailu.github.io/Unique3D/) | [Paper](https://arxiv.org/abs/2405.20343) | [Demo](https://huggingface.co/spaces/Wuvin/Unique3D)

å¯ä»¥ç”Ÿæˆ`.glb`æ¨¡å‹ï¼Œæ•ˆæœæ¯”Stable Fast 3Då¥½ã€‚

<div class="theme-image">
  <img src="./assets/Unique3D.jpg" alt="Light Mode Image" class="light-mode">
  <img src="./assets/dark_Unique3D.jpg" alt="Dark Mode Image" class="dark-mode">
</div>


## åŠ¨ç‰©åŠ¨ä½œçš„ç”Ÿæˆ

### MANN

[Video](https://www.youtube.com/watch?v=uFJvRYtjQ4c) | [Paper](https://github.com/sebastianstarke/AI4Animation/blob/master/Media/SIGGRAPH_2018/Paper.pdf) | [Code](https://github.com/sebastianstarke/AI4Animation/tree/master/AI4Animation/SIGGRAPH_2018)

**è¾“å…¥çš„å…·ä½“æ ¼å¼**

| å†…å®¹ | æˆå‘˜ | å°ºå¯¸ |
| :---: | :---: | :---: |
| state (i)çš„Trajectoryï¼Œæ‰€æœ‰å€¼éƒ½**ç›¸å¯¹äº**æ ¹èŠ‚ç‚¹ | pos.x | 12 |
|| pos.z | 12 |
|| dir.x | 12 |
|| dir.z | 12 |
|| vel.x | 12 |
|| vel.z | 12 |
|| speed | 12 |
|| Stylesçš„6ç»´one-hotå‘é‡| 12 |
| state (i-1) çš„å…³èŠ‚ | pos.x | 27 |
|| pos.y | 27 |
|| pos.z | 27 |
|| foward.x | 27 |
|| foward.y | 27 | | 27 |
|| foward.z | 27 |
|| up.x | 27 |
|| up.y | 27 |
|| up.z | 27 |
|| vel.x | 27 |
|| vel.y | 27 |
|| vel.z | 27 |

**è¾“å‡ºçš„å…·ä½“æ ¼å¼**

| å†…å®¹ | æˆå‘˜ | å°ºå¯¸ |
| :---: | :---: | :---: |
| state (i+1)çš„Trajectoryï¼Œæ‰€æœ‰å€¼éƒ½**ç›¸å¯¹äº**æ ¹èŠ‚ç‚¹ | pos.x | 6 |
|| pos.z | 6 |
|| dir.x | 6 |
|| dir.z | 6 |
|| vel.x | 6 |
|| vel.z | 6 |
| state (i-1) çš„å…³èŠ‚ | pos.x | 27 |
|| pos.y | 27 |
|| pos.z | 27 |
|| foward.x | 27 |
|| foward.y | 27 | | 27 |
|| foward.z | 27 |
|| up.x | 27 |
|| up.y | 27 |
|| up.z | 27 |
|| vel.x | 27 |
|| vel.y | 27 |
|| vel.z | 27 |
| rootèŠ‚ç‚¹ç›¸å¯¹äºä¸Šä¸€å¸§çš„ä½ç§» | (x, è§’åº¦, z) | 3 |

MoE æŒ‡çš„æ˜¯ Mixture of Expertsï¼ˆä¸“å®¶æ··åˆæ¨¡å‹ï¼‰ï¼Œæ˜¯ä¸€ç§ç”¨äºæ„å»ºæ·±åº¦ç¥ç»ç½‘ç»œçš„æ¶æ„ã€‚è¿™ç§æ¶æ„é€šå¸¸åŒ…æ‹¬ä¸¤ä¸ªå…³é”®ç»„ä»¶ï¼šä¸“å®¶ç½‘ç»œå’Œé—¨æ§ç½‘ç»œã€‚

1. ä¸“å®¶ç½‘ç»œï¼ˆExpertsï¼‰ï¼š è¿™æ˜¯å¤šä¸ªç¥ç»ç½‘ç»œæ¨¡å—çš„é›†åˆï¼Œæ¯ä¸ªæ¨¡å—è¢«ç§°ä¸ºä¸€ä¸ªä¸“å®¶ã€‚æ¯ä¸ªä¸“å®¶è¢«è®¾è®¡ä¸ºåœ¨å¤„ç†è¾“å…¥æ•°æ®çš„ç‰¹å®šæ–¹é¢ä¸Šè¡¨ç°å‡ºè‰²ã€‚ä¾‹å¦‚ï¼Œå¯¹äºå›¾åƒåˆ†ç±»ä»»åŠ¡ï¼Œä¸åŒçš„ä¸“å®¶å¯èƒ½æ“…é•¿è¯†åˆ«ä¸åŒç±»åˆ«çš„ç‰©ä½“ã€‚

2. é—¨æ§ç½‘ç»œï¼ˆGating Networkï¼‰ï¼š é—¨æ§ç½‘ç»œç”¨äºç¡®å®šåœ¨ç»™å®šè¾“å…¥ä¸Šå“ªä¸ªä¸“å®¶åº”è¯¥å‘æŒ¥ä½œç”¨ã€‚é—¨æ§ç½‘ç»œè¾“å‡ºä¸€ç»„æƒé‡ï¼Œè¿™äº›æƒé‡è¡¨ç¤ºæ¯ä¸ªä¸“å®¶å¯¹ç»™å®šè¾“å…¥çš„è´¡çŒ®ã€‚è¿™äº›æƒé‡é€šå¸¸æ˜¯åœ¨0åˆ°1ä¹‹é—´çš„å€¼ï¼Œå®ƒä»¬çš„å’Œç­‰äº1ã€‚

æ•´ä¸ª MoE æ¨¡å‹çš„è¾“å‡ºæ˜¯æ‰€æœ‰ä¸“å®¶çš„è¾“å‡ºçš„åŠ æƒå’Œï¼Œæƒé‡ç”±é—¨æ§ç½‘ç»œç¡®å®šã€‚è¿™ä½¿å¾— MoE èƒ½å¤Ÿåœ¨ä¸åŒçš„è¾“å…¥æƒ…å†µä¸‹åŠ¨æ€åœ°é€‰æ‹©ä¸åŒçš„ä¸“å®¶æ¥æ‰§è¡Œä»»åŠ¡ã€‚

MoE çš„ä¼˜ç‚¹ä¹‹ä¸€æ˜¯å…¶èƒ½å¤Ÿå¤„ç†å¤æ‚çš„ã€å¤šæ¨¡æ€çš„æ•°æ®åˆ†å¸ƒï¼Œå› ä¸ºä¸åŒçš„ä¸“å®¶å¯ä»¥ä¸“æ³¨äºå¤„ç†ä¸åŒæ–¹é¢çš„æ•°æ®ã€‚è¿™ç§ç»“æ„ä¹Ÿæœ‰åŠ©äºæé«˜æ¨¡å‹çš„å®¹é‡å’Œè¡¨è¾¾èƒ½åŠ›ï¼Œä½¿å…¶èƒ½å¤Ÿæ›´å¥½åœ°é€‚åº”å¤æ‚çš„ä»»åŠ¡ã€‚ MoE ç»“æ„å¸¸å¸¸åœ¨æ¶‰åŠå¤§è§„æ¨¡ç¥ç»ç½‘ç»œå’Œå¤æ‚ä»»åŠ¡çš„æƒ…å†µä¸‹å–å¾—äº†è‰¯å¥½çš„æ€§èƒ½ã€‚

æƒé‡æ–‡ä»¶å‘½åè§„åˆ™ï¼š

1. `cp[0-2]_[a,b][0-7].bin`ï¼Œä¸€å…±3x2x8=48ä¸ªæ–‡ä»¶
  - `ExpertWeights.py`
  - a,b è¡¨ç¤º $\alpha$å’Œ$\beta$
  - 0-7 è¡¨ç¤ºä¸“å®¶çš„ç´¢å¼•

2. `wc[0-2]_[b,w].bin`ï¼Œä¸€å…±3x2=6ä¸ªæ–‡ä»¶
  - `Gating.py`
  - w è¡¨ç¤ºweight
  - b è¡¨ç¤ºbias

3. `[X,Y][mean,std].bin`ï¼Œä¸€å…±2x2=4ä¸ªæ–‡ä»¶
  - `Utils.py`
  - mean è¡¨ç¤ºå‡å€¼
  - std è¡¨ç¤ºæ–¹å·®

| å˜é‡ | å°ºå¯¸ |
| :---: | :---: |
| Xmean | 480 x 1 |
| Xstd | 480 x 1 |
| Ymean | 363 x 1 |
| Ystd | 363 x 1 |
| wc0_w | 32 x 19 |
| wc0_b | 32 x 1 |
| wc1_w | 32 x 32 |
| wc1_b | 32 x 1 |
| wc2_w | 8 x 32 |
| wc2_b | 8 x 1 |
| cp0_a0-7 | 512 x 480 |
| cp0_b0-7 | 512 x 1 |
| cp1_a0-7 | 512 x 512 |
| cp1_b0-7 | 512 x 1 |
| cp2_a0-7 | 363 x 512 |
| cp2_b0-7 | 363 x 1 |


## å…¶ä»–
**Custom Diffusion**
[Home](https://www.cs.cmu.edu/~custom-diffusion/results.html) |
[Github](https://github.com/adobe-research/custom-diffusion)

https://zhuanlan.zhihu.com/p/620852185


**æ•°å­—äºº**

Wav2lipï¼šhttps://github.com/Rudrabha/Wav2Lip

EasyWav2lip: https://github.com/anothermartz/Easy-Wav2Lip

facefusion2.5: https://github.com/facefusion/facefusion

SadTalker-Video-Lip-Sync: https://github.com/Zz-ww/SadTalker-Video-Lip-Sync

**æ¢è„¸**

IPAdapter ï¼ˆé€šå¸¸ä¼šä¼´éšå…¶ä»–å…ƒç´ æ›¿æ¢ï¼‰

ReActor

facefusion

DeepFaceLive

## Propainter
[code](https://github.com/sczhou/ProPainter) | [demo](https://huggingface.co/spaces/sczhou/ProPainter)

### Install
```shell
git clone https://github.com/sczhou/ProPainter.git
conda create -n propainter python=3.10 -y
conda activate propainter
pip install torch==2.3.0+cu121 torchvision==0.18.0+cu121--extra-index-url https://download.pytorch.org/whl/cu121

cd ProPainter
pip3 install -r requirements.txt

# Test
python inference_propainter.py --video inputs/object_removal/bmx-trees --mask inputs/object_removal/bmx-trees_mask
```

ç¤ºä¾‹ä»£ç ï¼š
```python
# -*- coding: utf-8 -*-
import os
import cv2
import numpy as np
import scipy.ndimage
import shutil
from PIL import Image
from tqdm import tqdm

import torch
import torchvision

from model.modules.flow_comp_raft import RAFT_bi
from model.recurrent_flow_completion import RecurrentFlowCompleteNet
from model.propainter import InpaintGenerator
from utils.download_util import load_file_from_url
from core.utils import to_tensors
from model.misc import get_device

import warnings

warnings.filterwarnings("ignore")

pretrain_model_url = 'https://github.com/sczhou/ProPainter/releases/download/v0.1.0/'


def imwrite(img, file_path, params=None, auto_mkdir=True):
    if auto_mkdir:
        dir_name = os.path.abspath(os.path.dirname(file_path))
        os.makedirs(dir_name, exist_ok=True)
    return cv2.imwrite(file_path, img, params)


# resize frames
def resize_frames(frames, size=None):
    if size is not None:
        out_size = size
        process_size = (out_size[0] - out_size[0] % 8, out_size[1] - out_size[1] % 8)
        frames = [f.resize(process_size) for f in frames]
    else:
        out_size = frames[0].size
        process_size = (out_size[0] - out_size[0] % 8, out_size[1] - out_size[1] % 8)
        if not out_size == process_size:
            frames = [f.resize(process_size) for f in frames]

    return frames, process_size, out_size


#  read frames from video
def read_frame_from_videos(frame_root):
    if frame_root.endswith(('mp4', 'mov', 'avi', 'MP4', 'MOV', 'AVI')):  # input video path
        video_name = os.path.basename(frame_root)[:-4]
        vframes, aframes, info = torchvision.io.read_video(filename=frame_root, pts_unit='sec')  # RGB
        frames = list(vframes.numpy())
        frames = [Image.fromarray(f) for f in frames]
        fps = info['video_fps']
    else:
        video_name = os.path.basename(frame_root)
        frames = []
        fr_lst = sorted(os.listdir(frame_root))
        for fr in fr_lst:
            frame = cv2.imread(os.path.join(frame_root, fr))
            frame = Image.fromarray(cv2.cvtColor(frame, cv2.COLOR_BGR2RGB))
            frames.append(frame)
        fps = None
    size = frames[0].size

    return frames, fps, size, video_name


def binary_mask(mask, th=0.1):
    mask[mask > th] = 1
    mask[mask <= th] = 0
    return mask


def mask_process(masks_img, size, flow_mask_dilates=8, mask_dilates=5):
    length = len(masks_img)
    masks_dilated = []
    flow_masks = []
    for mask_img in masks_img:
        if size is not None:
            mask_img = mask_img.resize(size, Image.NEAREST)
        mask_img = np.array(mask_img.convert('L'))

        # Dilate 8 pixel so that all known pixel is trustworthy
        if flow_mask_dilates > 0:
            flow_mask_img = scipy.ndimage.binary_dilation(mask_img, iterations=flow_mask_dilates).astype(np.uint8)
        else:
            flow_mask_img = binary_mask(mask_img).astype(np.uint8)
        # Close the small holes inside the foreground objects
        # flow_mask_img = cv2.morphologyEx(flow_mask_img, cv2.MORPH_CLOSE, np.ones((21, 21),np.uint8)).astype(bool)
        # flow_mask_img = scipy.ndimage.binary_fill_holes(flow_mask_img).astype(np.uint8)
        flow_masks.append(Image.fromarray(flow_mask_img * 255))

        if mask_dilates > 0:
            mask_img = scipy.ndimage.binary_dilation(mask_img, iterations=mask_dilates).astype(np.uint8)
        else:
            mask_img = binary_mask(mask_img).astype(np.uint8)
        masks_dilated.append(Image.fromarray(mask_img * 255))

    if len(masks_img) == 1:
        flow_masks = flow_masks * length
        masks_dilated = masks_dilated * length

    return flow_masks, masks_dilated


# read frame-wise masks
def read_mask(mpath, length, size, flow_mask_dilates=8, mask_dilates=5):
    masks_img = []
    masks_dilated = []
    flow_masks = []

    if mpath.endswith(('jpg', 'jpeg', 'png', 'JPG', 'JPEG', 'PNG')):  # input single img path
        masks_img = [Image.open(mpath)]
    else:
        mnames = sorted(os.listdir(mpath))
        for mp in mnames:
            masks_img.append(Image.open(os.path.join(mpath, mp)))

    for mask_img in masks_img:
        if size is not None:
            mask_img = mask_img.resize(size, Image.NEAREST)
        mask_img = np.array(mask_img.convert('L'))

        # Dilate 8 pixel so that all known pixel is trustworthy
        if flow_mask_dilates > 0:
            flow_mask_img = scipy.ndimage.binary_dilation(mask_img, iterations=flow_mask_dilates).astype(np.uint8)
        else:
            flow_mask_img = binary_mask(mask_img).astype(np.uint8)
        # Close the small holes inside the foreground objects
        # flow_mask_img = cv2.morphologyEx(flow_mask_img, cv2.MORPH_CLOSE, np.ones((21, 21),np.uint8)).astype(bool)
        # flow_mask_img = scipy.ndimage.binary_fill_holes(flow_mask_img).astype(np.uint8)
        flow_masks.append(Image.fromarray(flow_mask_img * 255))

        if mask_dilates > 0:
            mask_img = scipy.ndimage.binary_dilation(mask_img, iterations=mask_dilates).astype(np.uint8)
        else:
            mask_img = binary_mask(mask_img).astype(np.uint8)
        masks_dilated.append(Image.fromarray(mask_img * 255))

    if len(masks_img) == 1:
        flow_masks = flow_masks * length
        masks_dilated = masks_dilated * length

    return flow_masks, masks_dilated


def extrapolation(video_ori, scale):
    """Prepares the data for video outpainting.
    """
    nFrame = len(video_ori)
    imgW, imgH = video_ori[0].size

    # Defines new FOV.
    imgH_extr = int(scale[0] * imgH)
    imgW_extr = int(scale[1] * imgW)
    imgH_extr = imgH_extr - imgH_extr % 8
    imgW_extr = imgW_extr - imgW_extr % 8
    H_start = int((imgH_extr - imgH) / 2)
    W_start = int((imgW_extr - imgW) / 2)

    # Extrapolates the FOV for video.
    frames = []
    for v in video_ori:
        frame = np.zeros(((imgH_extr, imgW_extr, 3)), dtype=np.uint8)
        frame[H_start: H_start + imgH, W_start: W_start + imgW, :] = v
        frames.append(Image.fromarray(frame))

    # Generates the mask for missing region.
    masks_dilated = []
    flow_masks = []

    dilate_h = 4 if H_start > 10 else 0
    dilate_w = 4 if W_start > 10 else 0
    mask = np.ones(((imgH_extr, imgW_extr)), dtype=np.uint8)

    mask[H_start + dilate_h: H_start + imgH - dilate_h,
    W_start + dilate_w: W_start + imgW - dilate_w] = 0
    flow_masks.append(Image.fromarray(mask * 255))

    mask[H_start: H_start + imgH, W_start: W_start + imgW] = 0
    masks_dilated.append(Image.fromarray(mask * 255))

    flow_masks = flow_masks * nFrame
    masks_dilated = masks_dilated * nFrame

    return frames, flow_masks, masks_dilated, (imgW_extr, imgH_extr)


def get_ref_index(mid_neighbor_id, neighbor_ids, length, ref_stride=10, ref_num=-1):
    ref_index = []
    if ref_num == -1:
        for i in range(0, length, ref_stride):
            if i not in neighbor_ids:
                ref_index.append(i)
    else:
        start_idx = max(0, mid_neighbor_id - ref_stride * (ref_num // 2))
        end_idx = min(length, mid_neighbor_id + ref_stride * (ref_num // 2))
        for i in range(start_idx, end_idx, ref_stride):
            if i not in neighbor_ids:
                if len(ref_index) > ref_num:
                    break
                ref_index.append(i)
    return ref_index

class ProPainter:
    def __init__(self):
        self.device = get_device()

        self.raft_iter = 20
        self.ref_stride = 10
        self.resize_ratio = 1.0
        self.subvideo_length = 50
        self.mask_dilation = 4
        self.neighbor_length = 10
        # Use fp16 precision during inference to reduce running memory cost
        self.use_half = True

        ##############################################
        # set up RAFT and flow competition model
        ##############################################
        ckpt_path = load_file_from_url(url=os.path.join(pretrain_model_url, 'raft-things.pth'),
                                       model_dir='weights', progress=True, file_name=None)
        self.fix_raft = RAFT_bi(ckpt_path, self.device)

        ckpt_path = load_file_from_url(url=os.path.join(pretrain_model_url, 'recurrent_flow_completion.pth'),
                                       model_dir='weights', progress=True, file_name=None)
        self.fix_flow_complete = RecurrentFlowCompleteNet(ckpt_path)
        for p in self.fix_flow_complete.parameters():
            p.requires_grad = False
        self.fix_flow_complete.to(self.device)
        self.fix_flow_complete.eval()

        ##############################################
        # set up ProPainter model
        ##############################################
        ckpt_path = load_file_from_url(url=os.path.join(pretrain_model_url, 'ProPainter.pth'),
                                       model_dir='weights', progress=True, file_name=None)
        self.model = InpaintGenerator(model_path=ckpt_path).to(self.device)
        self.model.eval()

        self.model = self.model.half()

    def fill(self, frames, masks_img):
        size = frames[0].size

        if not self.resize_ratio == 1.0:
            size = (int(self.resize_ratio * size[0]), int(self.resize_ratio * size[1]))

        frames, size, out_size = resize_frames(frames, size)
        flow_masks, masks_dilated = mask_process(masks_img, size, flow_mask_dilates=self.mask_dilation,
                                                 mask_dilates=self.mask_dilation)
        w, h = size

        frames_inp = [np.array(f).astype(np.uint8) for f in frames]
        frames = to_tensors()(frames).unsqueeze(0) * 2 - 1
        flow_masks = to_tensors()(flow_masks).unsqueeze(0)
        masks_dilated = to_tensors()(masks_dilated).unsqueeze(0)
        frames, flow_masks, masks_dilated = frames.to(self.device), flow_masks.to(self.device), masks_dilated.to(
            self.device)

        ##############################################
        # ProPainter inference
        ##############################################
        video_length = frames.size(1)
        #print(f'\nProcessing: {video_name} [{video_length} frames]...')
        with torch.no_grad():
            # ---- compute flow ----
            if frames.size(-1) <= 640:
                short_clip_len = 12
            elif frames.size(-1) <= 720:
                short_clip_len = 8
            elif frames.size(-1) <= 1280:
                short_clip_len = 4
            else:
                short_clip_len = 2

            # use fp32 for RAFT
            if frames.size(1) > short_clip_len:
                gt_flows_f_list, gt_flows_b_list = [], []
                for f in range(0, video_length, short_clip_len):
                    end_f = min(video_length, f + short_clip_len)
                    if f == 0:
                        flows_f, flows_b = self.fix_raft(frames[:, f:end_f], iters=self.raft_iter)
                    else:
                        flows_f, flows_b = self.fix_raft(frames[:, f - 1:end_f], iters=self.raft_iter)

                    gt_flows_f_list.append(flows_f)
                    gt_flows_b_list.append(flows_b)
                    torch.cuda.empty_cache()

                gt_flows_f = torch.cat(gt_flows_f_list, dim=1)
                gt_flows_b = torch.cat(gt_flows_b_list, dim=1)
                gt_flows_bi = (gt_flows_f, gt_flows_b)
            else:
                gt_flows_bi = self.fix_raft(frames, iters=self.raft_iter)
                torch.cuda.empty_cache()

            if self.use_half:
                frames, flow_masks, masks_dilated = frames.half(), flow_masks.half(), masks_dilated.half()
                gt_flows_bi = (gt_flows_bi[0].half(), gt_flows_bi[1].half())
                fix_flow_complete = self.fix_flow_complete.half()

            # ---- complete flow ----
            flow_length = gt_flows_bi[0].size(1)
            if flow_length > self.subvideo_length:
                pred_flows_f, pred_flows_b = [], []
                pad_len = 5
                for f in range(0, flow_length, self.subvideo_length):
                    s_f = max(0, f - pad_len)
                    e_f = min(flow_length, f + self.subvideo_length + pad_len)
                    pad_len_s = max(0, f) - s_f
                    pad_len_e = e_f - min(flow_length, f + self.subvideo_length)
                    pred_flows_bi_sub, _ = fix_flow_complete.forward_bidirect_flow(
                        (gt_flows_bi[0][:, s_f:e_f], gt_flows_bi[1][:, s_f:e_f]),
                        flow_masks[:, s_f:e_f + 1])
                    pred_flows_bi_sub = fix_flow_complete.combine_flow(
                        (gt_flows_bi[0][:, s_f:e_f], gt_flows_bi[1][:, s_f:e_f]),
                        pred_flows_bi_sub,
                        flow_masks[:, s_f:e_f + 1])

                    pred_flows_f.append(pred_flows_bi_sub[0][:, pad_len_s:e_f - s_f - pad_len_e])
                    pred_flows_b.append(pred_flows_bi_sub[1][:, pad_len_s:e_f - s_f - pad_len_e])
                    torch.cuda.empty_cache()

                pred_flows_f = torch.cat(pred_flows_f, dim=1)
                pred_flows_b = torch.cat(pred_flows_b, dim=1)
                pred_flows_bi = (pred_flows_f, pred_flows_b)
            else:
                pred_flows_bi, _ = fix_flow_complete.forward_bidirect_flow(gt_flows_bi, flow_masks)
                pred_flows_bi = fix_flow_complete.combine_flow(gt_flows_bi, pred_flows_bi, flow_masks)
                torch.cuda.empty_cache()

            # ---- image propagation ----
            masked_frames = frames * (1 - masks_dilated)
            subvideo_length_img_prop = min(100,
                                           self.subvideo_length)  # ensure a minimum of 100 frames for image propagation
            if video_length > subvideo_length_img_prop:
                updated_frames, updated_masks = [], []
                pad_len = 10
                for f in range(0, video_length, subvideo_length_img_prop):
                    s_f = max(0, f - pad_len)
                    e_f = min(video_length, f + subvideo_length_img_prop + pad_len)
                    pad_len_s = max(0, f) - s_f
                    pad_len_e = e_f - min(video_length, f + subvideo_length_img_prop)

                    b, t, _, _, _ = masks_dilated[:, s_f:e_f].size()
                    pred_flows_bi_sub = (pred_flows_bi[0][:, s_f:e_f - 1], pred_flows_bi[1][:, s_f:e_f - 1])
                    prop_imgs_sub, updated_local_masks_sub = self.model.img_propagation(masked_frames[:, s_f:e_f],
                                                                                        pred_flows_bi_sub,
                                                                                        masks_dilated[:, s_f:e_f],
                                                                                        'nearest')
                    updated_frames_sub = frames[:, s_f:e_f] * (1 - masks_dilated[:, s_f:e_f]) + \
                                         prop_imgs_sub.view(b, t, 3, h, w) * masks_dilated[:, s_f:e_f]
                    updated_masks_sub = updated_local_masks_sub.view(b, t, 1, h, w)

                    updated_frames.append(updated_frames_sub[:, pad_len_s:e_f - s_f - pad_len_e])
                    updated_masks.append(updated_masks_sub[:, pad_len_s:e_f - s_f - pad_len_e])
                    torch.cuda.empty_cache()

                updated_frames = torch.cat(updated_frames, dim=1)
                updated_masks = torch.cat(updated_masks, dim=1)
            else:
                b, t, _, _, _ = masks_dilated.size()
                prop_imgs, updated_local_masks = self.model.img_propagation(masked_frames, pred_flows_bi, masks_dilated,
                                                                            'nearest')
                updated_frames = frames * (1 - masks_dilated) + prop_imgs.view(b, t, 3, h, w) * masks_dilated
                updated_masks = updated_local_masks.view(b, t, 1, h, w)
                torch.cuda.empty_cache()

        ori_frames = frames_inp
        comp_frames = [None] * video_length

        neighbor_stride = self.neighbor_length // 2
        if video_length > self.subvideo_length:
            ref_num = self.subvideo_length // self.ref_stride
        else:
            ref_num = -1

        # ---- feature propagation + transformer ----
        for f in tqdm(range(0, video_length, neighbor_stride)):
            neighbor_ids = [
                i for i in range(max(0, f - neighbor_stride),
                                 min(video_length, f + neighbor_stride + 1))
            ]
            ref_ids = get_ref_index(f, neighbor_ids, video_length, self.ref_stride, ref_num)
            selected_imgs = updated_frames[:, neighbor_ids + ref_ids, :, :, :]
            selected_masks = masks_dilated[:, neighbor_ids + ref_ids, :, :, :]
            selected_update_masks = updated_masks[:, neighbor_ids + ref_ids, :, :, :]
            selected_pred_flows_bi = (
            pred_flows_bi[0][:, neighbor_ids[:-1], :, :, :], pred_flows_bi[1][:, neighbor_ids[:-1], :, :, :])

            with torch.no_grad():
                # 1.0 indicates mask
                l_t = len(neighbor_ids)

                # pred_img = selected_imgs # results of image propagation
                pred_img = self.model(selected_imgs, selected_pred_flows_bi, selected_masks, selected_update_masks, l_t)

                pred_img = pred_img.view(-1, 3, h, w)

                pred_img = (pred_img + 1) / 2
                pred_img = pred_img.cpu().permute(0, 2, 3, 1).numpy() * 255
                binary_masks = masks_dilated[0, neighbor_ids, :, :, :].cpu().permute(
                    0, 2, 3, 1).numpy().astype(np.uint8)
                for i in range(len(neighbor_ids)):
                    idx = neighbor_ids[i]
                    img = np.array(pred_img[i]).astype(np.uint8) * binary_masks[i] \
                          + ori_frames[idx] * (1 - binary_masks[i])
                    if comp_frames[idx] is None:
                        comp_frames[idx] = img
                    else:
                        comp_frames[idx] = comp_frames[idx].astype(np.float32) * 0.5 + img.astype(np.float32) * 0.5

                    comp_frames[idx] = comp_frames[idx].astype(np.uint8)

            torch.cuda.empty_cache()

        return comp_frames


if __name__ == '__main__':
    video = "inputs/res/rgb"
    mask = "inputs/res/mask"
    masks_img = []
    masks_dilated = []
    flow_masks = []

    frames, fps, size, video_name = read_frame_from_videos(video)

    mnames = sorted(os.listdir(mask))
    for mp in mnames:
        masks_img.append(Image.open(os.path.join(mask, mp)))

    p_painter = ProPainter()
    comp_frames = p_painter.fill(frames, masks_img)
    comp_frames = p_painter.fill(frames, masks_img)
    comp_frames = p_painter.fill(frames, masks_img)

    # save videos frame
    save_dir = 'out'
    if os.path.exists(save_dir):
        # ä½¿ç”¨shutil.rmtree()å‡½æ•°æ¥åˆ é™¤ç›®å½•åŠå…¶å†…å®¹
        shutil.rmtree(save_dir)

    os.makedirs(save_dir)
    for i, frame in enumerate(comp_frames):
        save_frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)
        cv2.imwrite("out/%02d.jpg" % i, save_frame)
        # cv2.waitKey(1)

    torch.cuda.empty_cache()
    print(f'\nAll results are saved in {save_dir}')

```

## åè¯è§£é‡Š
- **DreamBooth**
is a training technique that updates the entire diffusion model by training on just a few images of a subject or style. It works by associating a special word in the prompt with the example images.

- **LoRA**
(Low-Rank Adaptation of Large Language Models) is a popular and lightweight training technique that significantly reduces the number of trainable parameters. It works by inserting a smaller number of new weights into the model and only these are trained.

- **SD[1.4/1.5/2.0]**
Stable Diffusion.

- **SVD**
Stable Video Diffusion.

- **Waifu**
https://zh.wiktionary.org/wiki/waifu

