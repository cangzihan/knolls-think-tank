---
tags:
  - 嵌入式
  - 机器视觉
  - Computer Vision
  - YOLO
---

# RK3588
## Radxa
Rock 5b资料：https://docs.radxa.com/rock5/rock5b/getting-started/overview

The following signatures couldn't be verified because the public key is not available: NO_PUBKEY 9B98116C9AA302C7

Please use the following command to refresh the public key

```shell
export DISTRO=focal-stable
wget -O - apt.radxa.com/$DISTRO/public.key | sudo apt-key add -
```

### 软件源

先设置阿里云
```shell
sudo cp /etc/apt/sources.list /etc/apt/sources.list.bak
sudo vim /etc/apt/sources.list
```

```
deb [arch=amd64] http://mirrors.aliyun.com/ubuntu/ focal main restricted universe multiverse
deb [arch=amd64] http://mirrors.aliyun.com/ubuntu/ focal-security main restricted universe multiverse
deb [arch=amd64] http://mirrors.aliyun.com/ubuntu/ focal-updates main restricted universe multiverse
deb [arch=amd64] http://mirrors.aliyun.com/ubuntu/ focal-backports main restricted universe multiverse

# deb-src http://mirrors.aliyun.com/ubuntu/ focal main restricted universe multiverse
# deb-src http://mirrors.aliyun.com/ubuntu/ focal-security main restricted universe multiverse
# deb-src http://mirrors.aliyun.com/ubuntu/ focal-updates main restricted universe multiverse
# deb-src http://mirrors.aliyun.com/ubuntu/ focal-backports main restricted universe multiverse

## Pre-released source, not recommended.
# deb http://mirrors.aliyun.com/ubuntu/ focal-proposed main restricted universe multiverse
# deb-src http://mirrors.aliyun.com/ubuntu/ focal-proposed main restricted universe multiverse
```

https://docs.radxa.com/general-tutorial/apt
从官方文档可知，在中国有特定的两个源。按照上面的教程后，这里补充

一定要看清系统是啥然后再执行相应的命令，目前 Radxa APT 支持 Debian Bookworm、Debian Bullseye、Debian Buster、Ubuntu Jammy 和 Ubuntu Focal，如果是Ubuntu Focal 那么就是：
```shell
export DISTRO=focal
```
然后
```shell
echo "deb [signed-by=/usr/share/keyrings/radxa-archive-keyring.gpg] https://radxa-apt.aghost.cn/$DISTRO/ $DISTRO main" | sudo tee -a /etc/apt/sources.list.d/radxa.list
echo "deb [signed-by=/usr/share/keyrings/radxa-archive-keyring.gpg] https://radxa-apt.aghost.cn/$DISTRO/ rockchip-$DISTRO main" | sudo tee -a /etc/apt/sources.list.d/radxa-rockchip.list
#sudo vim /etc/apt/sources.list.d/radxa-rockchip.list
#sudo vim /etc/apt/sources.list.d/radxa.list
wget https://radxa-apt.aghost.cn/$DISTRO/pubkey.gpg
sudo gpg -o /usr/share/keyrings/radxa-archive-keyring.gpg --dearmor ./pubkey.gpg
sudo apt-get update
```

### GPIO

ROCK 5B 支持 libmraa GPIO 库
```shell
apt-get install libmraa
```
测试GPIO
```shell
mraa-gpio list
```

#### 命令行控制

https://wiki.radxa.com/Rock5/hardware/5b/gpio

For Rockchip 5.10 kernel, the GPIO number can be calculated as below, take GPIO4_D5 as an example:

GPIO4_D5 = 32$\times$4 + 8$\times$3 + 5 = 157
(A=0, B=1, C=2, D=3)

1. GPIO输出

To set GPIO4_D5 output
```shell
cd /sys/class/gpio
echo 157 > export
cd gpio157
echo out > direction
echo 1 > value     # output high
echo 0 > value     # output low
```

-bash: export: Permission denied
```shell
sudo chmod -R 777 /sys/class/gpio
sudo echo 157 > export
```

附：

如果我不想控制157引脚了：
```shell
sudo echo 157 > unexport
```
这会将 GPIO 引脚 157 取消导出，将其还原为初始状态。

同理：

GPIO3_A4 就是 32$\times$3 + 8$\times$0 + 4 = 100
```shell
cd /sys/class/gpio
echo 100 > export
cd gpio100
sudo chmod -R 777 /sys/class/gpio/gpio100
echo out > direction
echo 1 > value     # output high
echo 0 > value     # output low
```

2. GPIO输入

设置 GPIO 引脚为输入模式（"in"）：
```shell
echo in > direction
```

读取 GPIO 引脚的状态（高电平或低电平）：
```shell
cat value
```

Conclusion
- 输入和输出本质就是读取或修改文件夹下的value文件。使用Python控制时，可直接open这个文件进行读或写。

|5  |  5 | G | 0_B5 | 0_B6|3_B5|G|3_A4|4_C4| G |
|-  | -  | - | -  | -  | -| -| -| -| -|
| |   |   |    | | | | 100 | | |
|3.3 |   |   |    | G | 3_C1 | 3_B7 | 3_C0 |3.3 |
|   |   |   |    |  | 113  | 111 | 112 |   |

```python
_pin_map = {
    # Physical pin to actual GPIO pin
    BOARD: {
        # 1: 3.3V
        # 2: 5V
        3: 139,
        # 4: 5V
        5: 138,
        # 6: GND
        7: 115,
        8: 13,
        # 9: GND
        10: 14,
        11: 113,
        12: 109,
        13: 111,
        # 14: GND
```

#### Python库
https://blog.csdn.net/qq_34482285/article/details/128013553

虽然RK3568的开发版不同厂商不同，但仍可以使用[香橙派的python GPIO库](https://github.com/rm-hull/OPi.GPIO/tree/master)。这里需要修改配置文件。

首先安装库：
```shell
sudo pip3 install --upgrade OPi.GPIO
```

安装完，到Python工具包的目录下找到OPi文件夹，
第三方系统自带的python3的路径是：`/usr/local/lib/python3.10/dist-packages/OPi`

如果安装了Anaconda，那么路径为：`~/anaconda3/lib/python3.11/site-packages/OPi`。但是用这个库需要权限，开了之后默认指向的是系统本身的Python

编辑OPi文件夹下的pin_mappings.py文件，找到_pin_map，根据自己的主板的GPIO口修改BOARD元素。

```python
...
_pin_map = {
    # Physical pin to actual GPIO pin
    BOARD: {
        # 1: 3.3V
        # 2: 5V
        3: 139,
        # 4: 5V
        5: 138,
        # 6: GND
        7: 115,
        8: 13,
        # 9: GND
        10: 14,
        11: 113,
        12: 109,
        13: 111,
        # 14: GND
        15: 112,
        16: 100,
        # 17: 3.3V
        18: 148,
        19: 42,
        # 20: GND
        21: 41,
        # 22:
        23: 43,
        24: 44,
        # 25: GND
        26: 45,
        27: 150,
        28: 149,
        29: 63,
        # 30: GND
        31: 47,
        32: 114,
        33: 103,
        # 34: GND
        35: 110,
        36: 105,
        38: 106,
        # 39: GND
        40: 107
    },
    ...
```

**测试代码**

- GPIO输出

```python
# test.py
import OPi.GPIO as GPIO
from time import sleep

# BOARD编号方式，基于插座引脚编号
GPIO.setmode(GPIO.BOARD)

# 输出模式
GPIO.setup(16, GPIO.OUT)

while True:
        GPIO.output(16, GPIO.HIGH)
        sleep(2)
        print("Port16=High")
        GPIO.output(16, GPIO.LOW)
        sleep(2)
        print("Port16=Low")

```

- GPIO输入

```python
import OPi.GPIO as GPIO
import time

GPIO.setmode(GPIO.BOARD)  # 使用物理引脚编号模式


# 定义GPIO引脚与按键名称的映射关系
gpio_dict = {"key_up": 16, "key_left": 11, "key_down": 13, "key_right": 15}

for pin_number in gpio_dict:
    GPIO.setup(gpio_dict[pin_number], GPIO.IN)


def key_read(key_name):
    # 读取GPIO值
    k_value = GPIO.input(gpio_dict[key_name])
    if k_value == 1:
        pass
    else:
        return False

    # 按键消抖
    time.sleep(0.01)

    # 再次读取GPIO值以确保按键稳定
    k_value = GPIO.input(gpio_dict[key_name])
    if k_value == 1:
        return True
    else:
        return False

# 初始化按键状态
key_up = False
key_left = False
key_down = False
key_right = False

while True:
    if key_read("key_up") and not key_up:
        print("上键被按下")
        key_up = True
    elif not key_read("key_up") and key_up:
        print("上键被松开")
        key_up = False

    if key_read("key_left") and not key_left:
        print("左键被按下")
        key_left = True
    elif not key_read("key_left") and key_left:
        print("左键被松开")
        key_left = False

    if key_read("key_down") and not key_down:
        print("下键被按下")
        key_down = True
    elif not key_read("key_down") and key_down:
        print("下键被松开")
        key_down = False

    if key_read("key_right") and not key_right:
        print("右键被按下")
        key_right = True
    elif not key_read("key_right") and key_right:
        print("右键被松开")
        key_right = False

    time.sleep(0.02)
```

### GPU 驱动安装

RK3588的GPU：Mali G610MP4 GPU，支持 OpenGLES 1.1、2.0 和 3.2，OpenCL 最高 2.2 和 Vulkan 1.2

在Linux下有一个官方驱动，但不能运行UE程序。除此外还可安装[Panfork](https://docs.mesa3d.org/drivers/panfrost.html)驱动（也不能运行）

Radxa官方的阴间教程https://docs.radxa.com/general-tutorial/panfork

对于Ubuntu jammy镜像
```shell
sudo apt update
sudo apt-get install xserver-xorg-core/jammy
```

```shell
sudo apt-get install xserver-xorg-core/bullseye
```

XXXXX(安就对了)
```shell
sudo apt update
sudo apt install build-essential meson git python3-mako libexpat1-dev bison flex libwayland-egl-backend-dev libxext-dev libxfixes-dev libxcb-glx0-dev libxcb-shm0-dev libxcb-dri2-0-dev libxcb-dri3-dev libxcb-present-dev libxshmfence-dev libxxf86vm-dev libxrandr-dev zlib1g-dev pkg-config cmake libwayland-*

```

安libdrm
```shell
cd ~/
git clone https://gitlab.freedesktop.org/mesa/drm
mkdir drm/build
cd drm/build
meson # 出错 ERROR: <ExternalProgram 'python3' -> ['/usr/bin/python3']> is not a valid python or it is missing setuptools
sudo apt-get install python3-distutils
meson
sudo ninja install
```

编译并安装 dri2to3
```shell
cd ~/
git clone https://gitlab.com/panfork/dri2to3.git
mkdir dri2to3/build
cd dri2to3/build
meson setup
sudo ninja install
```

编译并安装 Wayland protocols
```shell
cd ~/
git clone https://gitlab.freedesktop.org/wayland/wayland-protocols
mkdir wayland-protocols/build
cd wayland-protocols/build
git checkout 1.24
meson # 错误 提示版本问题
sudo apt install python3-pip # 如果没有安pip
sudo pip install --upgrade meson
meson --version
# In order to use sudo ninja install, meson must be installed as root
sudo ninja install
```

编译并安装 Mesa
```shell
cd ~/
git clone https://gitlab.com/panfork/mesa.git
mkdir mesa/build
cd mesa/build
meson -Dgallium-drivers=panfrost -Dvulkan-drivers= -Dllvm=disabled --prefix=/opt/panfrost # 错误 ERROR: Dependency "x11-xcb" not found, tried pkgconfig and cmake
sudo apt-get install libx11-xcb-dev
meson -Dgallium-drivers=panfrost -Dvulkan-drivers= -Dllvm=disabled --prefix=/opt/panfrost
sudo ninja install
```

验证

对于Armbian 23.8.1 Jammy with Linux 5.10.160-legacy-rk35xx
```shell
sudo nala install glmark2
# 在有显示屏的窗口
glmark2
```

可以使用APT来安装CMake。但是，通常Ubuntu的软件库中可能包含较旧的版本。如果你需要安装较新版本的CMake，可以使用以下步骤：

a. 访问Kitware的CMake下载页面：https://cmake.org/download/

b. 下载适合你系统的CMake二进制发行版。你可以使用wget或浏览器下载。

```shell
echo 'export PATH="/home/ubuntu/cmake-3.27.6-linux-aarch64/bin:$PATH"' >> ~/.bashrc
source ~/.bashrc
```
现在，再次运行 cmake --version 应该显示新的CMake版本。

### 第三方镜像

全部第三方镜像：https://wiki.radxa.com/Rock5/downloads

推荐镜像：[Ubuntu Desktop/Server by Joshua-Riek aka Spooky for Rock 5](https://github.com/Joshua-Riek/ubuntu-rockchip)

#### 装Android 容器
参考教程 https://forum.radxa.com/t/guide-best-option-for-ubuntu-desktop/14552

基于[Ubuntu Desktop/Server by Joshua-Riek aka Spooky for Rock 5](https://github.com/Joshua-Riek/ubuntu-rockchip)镜像

**装现代安装工具**

```shell
sudo apt update && sudo apt install nala -y && sudo nala upgrade -y
```

**Optional**
```shell
sudo vim /etc/chromium-browser/default
```
```
#Set Chromium ozone to Wayland only if you intend to use Gnome Wayland
--ozone-platform=wayland
--enable-zero-copy
#Set user agents specifically so it works on Netflix with widevine
--user-agent="Mozilla/5.0 (X11; Linux aarch64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/110.0.0.0 Safari/537.36" --accept-lang=en-US
#Fixes Chromium browser pixelation bug on wayland
--use-gl=angle
```

**装malior-droid工具**
```shell
# 安装docker -y: 自动确认软件包的安装。
sudo nala install docker docker.io adb -y

# 创建文件路径
sudo mkdir /dev/binderfs
# 将指定的文件系统挂载到指定的挂载点，文件系统类型是binder。binder是一种用于Android系统中进程间通信的文件系统类型
sudo mount -t binder binder /dev/binderfs

# 安装Malior(安装后，工具在/usr/local/bin里)
wget -O - https://github.com/ChisBread/malior/raw/main/install.sh > /tmp/malior-install.sh && bash /tmp/malior-install.sh  && rm /tmp/malior-install.sh

# Malior Redroid
malior update
malior install malior-droid
malior-droid update # 网络问题就连手机热点
```

**装scrcpy工具**
```shell
sudo nala install ffmpeg libsdl2-2.0-0 adb wget gcc git pkg-config meson ninja-build libsdl2-dev libavcodec-dev libavdevice-dev libavformat-dev libavutil-dev libswresample-dev libusb-1.0-0 libusb-1.0-0-dev -y
git clone https://github.com/Genymobile/scrcpy
cd scrcpy
./install_release.sh
```

```shell
#malior-droid start / stop / restart
malior-droid start
adb connect localhost:5555
scrcpy -s localhost:5555
scrcpy -s localhost:5555 -f # 全屏显示

# pgrep -o scrcpy # 获取PID
# pkill scrcpy # 停止进程
```
修改分辨率
```shell
malior-droid resize 720x1280
#malior-droid resize 1920x1080
```

**使用**
返回：鼠标右键

安装app:直接将apk文件拖入

当谈论容器化和虚拟机时，你可以将其比喻为两种不同的方式来“打包”和运行软件应用程序，就像在不同种类的盒子中运输货物一样。

- 容器化：

容器化类似于用一种魔法盒子来打包和运输你的应用程序。这个盒子包含了你的应用程序和所有需要的东西，比如库、配置文件等。不同之处在于，这个盒子非常轻便，几乎没有额外的重量，因此非常高效。

轻盒子： 容器非常轻便，因为它们与主机操作系统共享许多组件，只包含应用程序及其依赖项。

快速启动： 容器可以在瞬间启动，就像打开盒子一样迅速。

共享资源： 多个容器可以在同一台机器上运行，共享操作系统的资源，而互不干扰。

- 虚拟机：

虚拟机则像一台小型的模拟计算机。你把你的应用程序和所有东西都放在这台模拟机器中，然后在主机机器上运行它。但这个模拟机器比容器要重，因为它需要一个完整的操作系统，就像在一个大箱子中运输一个小盒子。

重机器： 虚拟机包含了一个完整的操作系统，所以相对较重。

较慢启动： 启动虚拟机通常需要更多时间，就像启动一台真正的计算机一样。

资源隔离： 虚拟机提供了更强的资源隔离，但也需要更多的资源。

综上所述，容器化更加轻便和高效，适用于快速部署和运行应用程序，而虚拟机提供了更严格的隔离，但通常需要更多资源和时间。你可以根据你的需求来选择使用容器化或虚拟机技术。

**使用**
详见 【Android】-【Android容器使用】

### Python

#### 显示图像

cv2.imshow()对RK系列的芯片支持不好，显示图片的思路为：cv2.image转pillow.image，tkinter显示pillow.image

#### Pyaudio
`pip install pyaudio`在build过程发生报错，提示没有`portaudio.h`

参考https://blog.csdn.net/qq_34638161/article/details/80383914
其中只需要安装python3的库
```shell
sudo nala install portaudio19-dev python3-all-dev
pip install pyaudio
```

#### Websocket
AttributeError: module 'websocket' has no attribute 'WebSocketApp'

解决：
https://stackoverflow.com/

You've installed wrong websocket package. You installed websocket but you need websocket-client. Uninstall both to cleanup the mess and reinstall websocket-client.

### RKNN
https://github.com/airockchip/rknn_model_zoo/blob/main/README.md

​Rockchip Neural Network(RKNN)是瑞芯微为了加速模型推理而基于自身NPU硬件架构定义的一套模型格式.使用该格式定义的模型在Rockchip NPU上可以获得远高于CPU/GPU的性能.

ONNX（Open Neural Network Exchange）是一种针对机器学习所设计的开放式的文件格式，用于存储训练好的模型。它使得不同的人工智能框架（如PyTorch、MXNet）可以采用相同格式存储模型数据并交互。 ONNX的规范及代码主要由微软，亚马逊，Facebook和IBM等公司共同开发。目前官方支持加载ONNX模型并进行推理的深度学习框架有： Caffe2, PyTorch, MXNet，ML.NET，TensorRT 和 Microsoft CNTK，并且 TensorFlow 也非官方的支持ONNX。

其他教程：
- https://zhuanlan.zhihu.com/p/590368969
- 流程: https://wiki.t-firefly.com/en/3399pro_npu/npu_rknn_toolkit.html

工具仓库：
- https://github.com/rockchip-linux/rknn-toolkit2.git

- https://github.com/rockchip-linux/rknpu2

#### rknn_toolkit_lite2（Python）

**Resnet18**
```shell
# 安装
cd rknn-toolkit2-master/rknn_toolkit_lite2/packages/
pip install rknn_toolkit_lite2-1.5.2-cp310-cp310-linux_aarch64.whl
##################
# tensorflow2.8.0报错
pip install tf-estimator-nightly==2.8.0.dev2021122109
pip install tensorflow==2.8.0  -i https://pypi.tuna.tsinghua.edu.cn/simple
pip install rknn_toolkit2-1.5.2+b642f30c-cp310-cp310-linux_x86_64.whl  -i https://pypi.tuna.tsinghua.edu.cn/simple
######################

# 测试
cd ../examples/inference_with_lite/
python3 test.py
```

#### 调用RKNN SDK的C语言API

**YOLO v5 demo**

参考：https://github.com/rockchip-linux/rknpu2/tree/master/examples/rknn_yolov5_demo
这里可以直接在开发板上编译，在PC上可能存在报错。
```
Could not find compiler set in environment variable CC:

  aarch64-linux-gnu-gcc.
```
直接在板子上：
```shell
sudo nala install cmake
# 在.../rknpu2-master/examples/rknn_yolov5_demo目录下
./build-linux_RK3588.sh
```

使用：
```shell
cd install/rknn_yolov5_demo_Linux
export LD_LIBRARY_PATH=./lib
./rknn_yolov5_demo model/RK3588/yolov5s-640-640.rknn model/bus.jpg
```

### YOLOv8的移植
https://forum.radxa.com/t/use-yolov8-in-rk3588-npu/15838

前置工作
- 在PC上有正常运行的深度学习环境，已经安装好YOLO v8
- 可以另外装一个onnx转rknn的Python3.8环境

**第一步: PyTorch转onnx**

方法1
```python
from ultralytics import YOLO

# Load a model
model = YOLO('yolov8s.pt')  # load an official model

# Export the model
model.export(format='onnx', imgsz=(640, 640), opset=12)
```

如果提示没有onnx库就按照提示安装
```shell
pip install --no-cache "onnx>=1.12.0"  -i https://pypi.tuna.tsinghua.edu.cn/simple
```

方法2
```shell
yolo export model=yolov8s.pt imgsz=640,640 format=onnx opset=12
```


**第二步: onnx转RKNN**

编辑onnx(没有用到，工具地址在https://netron.app/)

本地部署方法
```shell
git clone https://github.com/ZhangGe6/onnx-modifier.git
pip install flask
python app.py
```
PC上下载[rknn-toolkit2](https://github.com/rockchip-linux/rknn-toolkit2.git)工具

PC上部署
```shell
pip install packages/rknn_toolkit2-1.5.2+b642f30c-cp38-cp38-linux_x86_64.whl
# pip install packages/rknn_toolkit2-1.5.2+b642f30c-cp38-cp38-linux_x86_64.whl -i https://pypi.tuna.tsinghua.edu.cn/simple
```
可以先测试YOLOv5的demo
```shell
cd examples/onnx/yolov5
python3 test.py
```
如果没有问题，那么将步骤一得到的`yolov8s.onnx`放到`examples/onnx/yolov5`文件夹下
```shell
cp test.py test_v8.py
```
编辑`test_v8.py`
```python
ONNX_MODEL = 'yolov8s.onnx'
RKNN_MODEL = 'yolov8s.rknn'
```
```python
QUANTIZE_ON = False # 重要，不然输出分数都会为0
```
```python
rknn.config(mean_values=[[0, 0, 0]], std_values=[[255, 255, 255]], target_platform="rk3588")
```

后处理参考 https://github.com/ultralytics/ultralytics/blob/main/examples/YOLOv8-OpenCV-ONNX-Python/main.py
```python
    img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)
    #img = cv2.resize(img, (IMG_SIZE, IMG_SIZE))
    img = np.reshape(img,(1, IMG_SIZE, IMG_SIZE, 3))

    # Inference
    print('--> Running model')
    outputs = rknn.inference(inputs=[img])
    #np.save('./onnx_yolov8_0.npy', outputs[0])
    #np.save('./onnx_yolov8_1.npy', outputs[1])
    #np.save('./onnx_yolov8_2.npy', outputs[2])
    print('done')

    # Prepare output array
    outputs = np.array([cv2.transpose(outputs[0][0])])
    print(outputs.shape)
    rows = outputs.shape[1]

    boxes = []
    scores = []
    class_ids = []

    # Iterate through output to collect bounding boxes, confidence scores, and class IDs
    for i in range(rows):
        classes_scores = outputs[0][i][4:]
        (minScore, maxScore, minClassLoc, (x, maxClassIndex)) = cv2.minMaxLoc(classes_scores)
        if maxScore >= 0.25:
            box = [
                outputs[0][i][0] - (0.5 * outputs[0][i][2]), outputs[0][i][1] - (0.5 * outputs[0][i][3]),
                outputs[0][i][2], outputs[0][i][3]]
            boxes.append(box)
            scores.append(maxScore)
            class_ids.append(maxClassIndex)

    print(boxes)
    # Apply NMS (Non-maximum suppression)
    result_boxes = cv2.dnn.NMSBoxes(boxes, scores, 0.25, 0.45, 0.5)

    detections = []

    scale = 1
    original_image = cv2.imread(IMG_PATH)
    # Iterate through NMS results to draw bounding boxes and labels
    for i in range(len(result_boxes)):
        index = result_boxes[i]
        box = boxes[index]
        detection = {
            'class_id': class_ids[index],
            'class_name': CLASSES[class_ids[index]],
            'confidence': scores[index],
            'box': box,
            'scale': scale}
        detections.append(detection)
        draw_bounding_box(original_image, class_ids[index], scores[index], round(box[0] * scale), round(box[1] * scale),
                          round((box[0] + box[2]) * scale), round((box[1] + box[3]) * scale))

    # Display the image with bounding boxes
    cv2.imshow('image', original_image)
    cv2.waitKey(0)
    cv2.destroyAllWindows()

```
新增后处理函数
```python
np.random.seed(1200)
colors = np.random.uniform(0, 255, size=(len(CLASSES), 3))
def draw_bounding_box(img, class_id, confidence, x, y, x_plus_w, y_plus_h):
    """
    Draws bounding boxes on the input image based on the provided arguments.

    Args:
        img (numpy.ndarray): The input image to draw the bounding box on.
        class_id (int): Class ID of the detected object.
        confidence (float): Confidence score of the detected object.
        x (int): X-coordinate of the top-left corner of the bounding box.
        y (int): Y-coordinate of the top-left corner of the bounding box.
        x_plus_w (int): X-coordinate of the bottom-right corner of the bounding box.
        y_plus_h (int): Y-coordinate of the bottom-right corner of the bounding box.
    """
    label = f'{CLASSES[class_id]} ({confidence:.2f})'
    color = colors[class_id]
    cv2.rectangle(img, (x, y), (x_plus_w, y_plus_h), color, 2)
    cv2.putText(img, label, (x - 10, y - 10), cv2.FONT_HERSHEY_SIMPLEX, 0.5, color, 2)
```

开始转换
```shell
python3 test_v8.py
```


**移植到板子**

- 对于1.5.2版工具箱

下载 https://github.com/rockchip-linux/rknpu2/blob/master/runtime/RK3588/Linux/librknn_api/aarch64/librknnrt.so 然后
```shell
sudo cp librknnrt.so /usr/lib/
```

- 对于1.6.0版工具箱(和其他rknpu2和工具箱版本不同步的问题)

直接在rknn_toolkit2工具箱找到`rknpu2/blob/master/runtime/RK3588/Linux/librknn_api/aarch64/librknnrt.so`然后
```shell
sudo cp librknnrt.so /usr/lib/
```

将rknn文件放到开发版上，确保已成功安装rknn_toolkit_lite2在板子上。然后创建一个Python脚本并运行
```python
import cv2
import numpy as np
import platform
from rknnlite.api import RKNNLite

# decice tree for rk356x/rk3588
DEVICE_COMPATIBLE_NODE = '/proc/device-tree/compatible'

def get_host():
    # get platform and device type
    system = platform.system()
    machine = platform.machine()
    os_machine = system + '-' + machine
    if os_machine == 'Linux-aarch64':
        try:
            with open(DEVICE_COMPATIBLE_NODE) as f:
                device_compatible_str = f.read()
                if 'rk3588' in device_compatible_str:
                    host = 'RK3588'
                elif 'rk3562' in device_compatible_str:
                    host = 'RK3562'
                else:
                    host = 'RK3566_RK3568'
        except IOError:
            print('Read device node {} failed.'.format(DEVICE_COMPATIBLE_NODE))
            exit(-1)
    else:
        host = os_machine
    return host

INPUT_SIZE = 640

RK3566_RK3568_RKNN_MODEL = 'resnet18_for_rk3566_rk3568.rknn'
RK3588_RKNN_MODEL = 'yolov8s.rknn'
RK3562_RKNN_MODEL = 'resnet18_for_rk3562.rknn'


CLASSES = ("person", "bicycle", "car", "motorbike ", "aeroplane ", "bus ", "train", "truck ", "boat", "traffic light",
           "fire hydrant", "stop sign ", "parking meter", "bench", "bird", "cat", "dog ", "horse ", "sheep", "cow", "elephant",
           "bear", "zebra ", "giraffe", "backpack", "umbrella", "handbag", "tie", "suitcase", "frisbee", "skis", "snowboard", "sports ball", "kite",
           "baseball bat", "baseball glove", "skateboard", "surfboard", "tennis racket", "bottle", "wine glass", "cup", "fork", "knife ",
           "spoon", "bowl", "banana", "apple", "sandwich", "orange", "broccoli", "carrot", "hot dog", "pizza ", "donut", "cake", "chair", "sofa",
           "pottedplant", "bed", "diningtable", "toilet ", "tvmonitor", "laptop	", "mouse	", "remote ", "keyboard ", "cell phone", "microwave ",
           "oven ", "toaster", "sink", "refrigerator ", "book", "clock", "vase", "scissors ", "teddy bear ", "hair drier", "toothbrush ")


np.random.seed(1200)
colors = np.random.uniform(0, 255, size=(len(CLASSES), 3))
def draw_bounding_box(img, class_id, confidence, x, y, x_plus_w, y_plus_h):
    """
    Draws bounding boxes on the input image based on the provided arguments.

    Args:
        img (numpy.ndarray): The input image to draw the bounding box on.
        class_id (int): Class ID of the detected object.
        confidence (float): Confidence score of the detected object.
        x (int): X-coordinate of the top-left corner of the bounding box.
        y (int): Y-coordinate of the top-left corner of the bounding box.
        x_plus_w (int): X-coordinate of the bottom-right corner of the bounding box.
        y_plus_h (int): Y-coordinate of the bottom-right corner of the bounding box.
    """
    label = f'{CLASSES[class_id]} ({confidence:.2f})'
    color = colors[class_id]
    cv2.rectangle(img, (x, y), (x_plus_w, y_plus_h), color, 2)
    cv2.putText(img, label, (x - 10, y - 10), cv2.FONT_HERSHEY_SIMPLEX, 0.5, color, 2)


if __name__ == '__main__':

    host_name = get_host()
    if host_name == 'RK3566_RK3568':
        rknn_model = RK3566_RK3568_RKNN_MODEL
    elif host_name == 'RK3562':
        rknn_model = RK3562_RKNN_MODEL
    elif host_name == 'RK3588':
        rknn_model = RK3588_RKNN_MODEL
    else:
        print("This demo cannot run on the current platform: {}".format(host_name))
        exit(-1)

    rknn_lite = RKNNLite()

    # load RKNN model
    print('--> Load RKNN model')
    ret = rknn_lite.load_rknn(rknn_model)
    if ret != 0:
        print('Load RKNN model failed')
        exit(ret)
    print('done')

    ori_img = cv2.imread('./bus.jpg')
    img = cv2.cvtColor(ori_img, cv2.COLOR_BGR2RGB)

    # init runtime environment
    print('--> Init runtime environment')
    # run on RK356x/RK3588 with Debian OS, do not need specify target.
    if host_name == 'RK3588':
        ret = rknn_lite.init_runtime(core_mask=RKNNLite.NPU_CORE_0)
    else:
        ret = rknn_lite.init_runtime()
    if ret != 0:
        print('Init runtime environment failed')
        exit(ret)
    print('done')

    # Inference
    print('--> Running model')
    outputs = rknn_lite.inference(inputs=[img])
    print(outputs[0].shape)
    print('done')

    print('Result:')
    # Prepare output array
    outputs = np.array([cv2.transpose(outputs[0][0])])
    print('  Output shape', outputs.shape)
    rows = outputs.shape[1]

    boxes = []
    scores = []
    class_ids = []

    # Iterate through output to collect bounding boxes, confidence scores, and class IDs
    for i in range(rows):
        classes_scores = outputs[0][i][4:]
        (minScore, maxScore, minClassLoc, (x, maxClassIndex)) = cv2.minMaxLoc(classes_scores)
        if maxScore >= 0.25:
            box = [
                outputs[0][i][0] - (0.5 * outputs[0][i][2]), outputs[0][i][1] - (0.5 * outputs[0][i][3]),
                outputs[0][i][2], outputs[0][i][3]]
            boxes.append(box)
            scores.append(maxScore)
            class_ids.append(maxClassIndex)

    # Apply NMS (Non-maximum suppression)
    result_boxes = cv2.dnn.NMSBoxes(boxes, scores, 0.25, 0.45, 0.5)

    detections = []

    scale = 1
    original_image = cv2.imread("./bus.jpg")
    # Iterate through NMS results to draw bounding boxes and labels
    for i in range(len(result_boxes)):
        index = result_boxes[i]
        box = boxes[index]
        detection = {
            'class_id': class_ids[index],
            'class_name': CLASSES[class_ids[index]],
            'confidence': scores[index],
            'box': box,
            'scale': scale}
        print(' ',detection)
        detections.append(detection)
        draw_bounding_box(original_image, class_ids[index], scores[index], round(box[0] * scale), round(box[1] * scale),
                          round((box[0] + box[2]) * scale), round((box[1] + box[3]) * scale))

    # Display the image with bounding boxes
    cv2.imwrite('result.jpg', original_image)

    rknn_lite.release()
```

### ChatGLM


**CLI** (Command Line Interface) 版本的程序指的是那些通过命令行或终端界面运行的应用程序。

可以不用装各种conda

#### TVM

Apache TVM 是一个正在 Apache 软件基金会（ASF）孵化的开源项目。

https://zhuanlan.zhihu.com/p/353660224

实 际 上，“运行模型/代码到任意种类的硬件”并不是一个概念上全新的课题。在计算机编程语言发展的早期阶段（第二代编程语言），人们也曾遇到过类似的困境，即一种硬件平台必须配套一种汇编语言且不同汇编语言无法跨平台运行的情况。随着该领域的发展，人们给出了解决之道——引入高级语言和编译器。

TVM框架正是借鉴了这种思想，我们可以把TVM理解成一种广义的“编译器”：TensorFlow、PyTorch等训练框架导出的模型可以认为是“高级语言”，而TVM内部的图级别表达式树、算子级的调度Stages则可以认为是“高级语言”的“中间表示”。

TVM Unity, the latest development in Apache TVM, **is required to build** `MLC LLM`. Its features include:

- High-performance CPU/GPU code generation instantly without tuning;

- Dynamic shape and symbolic shape tracking by design;

- Supporting both inference and training;

- Productive python-first compiler implementation. As a concrete example, MLC LLM compilation is implemented in pure python using its API.

安装过程可参考：https://llm.mlc.ai/docs/install/tvm.html
```shell
# https://github.com/llvm/llvm-project/releases 页面下载文件，传到开发板中
sudo tar -xvf clang+llvm-17.0.6-aarch64-linux-gnu.tar.xz
# 如果解压的clang+llvm-17.0.6-aarch64-linux-gnu文件夹不是在/root文件夹下，需要转移
sudo mv clang+llvm-17.0.6-aarch64-linux-gnu /root/

# 通常，当你克隆一个包含子模块的 Git 仓库而不使用 --recursive 选项时，子模块的文件夹会存在，但里面是空的。这是因为子模块本身就是指向特定提交的独立 Git 仓库。使用 --recursive 选项，Git 将会初始化这些子模块的路径，并且克隆它们指向的仓库到这些路径下，相当于也克隆了子模块的内容。
# 克隆的目标路径为本地目录 "tvm_unity"。
git clone --recursive https://github.com/mlc-ai/relax.git tvm_unity && cd tvm_unity
mkdir -p build && cd build
cp ../cmake/config.cmake .

```

使用vim在config.cmake文件中修改下面几项：
```
set(CMAKE_BUILD_TYPE RelWithDebInfo) #这一项在文件中没有，需要添加
set(USE_OPENCL ON) #这一项在文件中可以找到，需要修改
set(HIDE_PRIVATE_SYMBOLS ON) #这一项在文件中没有，需要添加
set(USE_LLVM /root/clang+llvm-17.0.6-aarch64-linux-gnu/bin/llvm-config) #这一项在文件中可以找到，需要修改
```

```shell
# 加sudo防止无权限访问/root文件夹
# sudo nala install cmake
sudo cmake ..
sudo nala install libncurses5-dev
sudo make -j4

cd ../python
pip3 install --user .

# 验证安装
tvmc
```

#### MLC LLM
**M**achine **L**earning **C**ompilation for **L**arge **L**anguage **M**odels ([MLC LLM](https://github.com/mlc-ai/mlc-llm)) is a high-performance universal deployment solution that allows native deployment of any large language models with **native APIs** with compiler acceleration. The mission of this project is to enable everyone to develop, optimize and deploy AI models natively on everyone's devices with ML compilation techniques.

```shell
sudo nala install -y rustc cargo

# 通常，当你克隆一个包含子模块的 Git 仓库而不使用 --recursive 选项时，子模块的文件夹会存在，但里面是空的。这是因为子模块本身就是指向特定提交的独立 Git 仓库。使用 --recursive 选项，Git 将会初始化这些子模块的路径，并且克隆它们指向的仓库到这些路径下，相当于也克隆了子模块的内容。
cd ~/yuchen
git clone --recursive https://github.com/mlc-ai/mlc-llm.git && cd mlc-llm

pip3 install --user .
pip install typing_extensions==4.4.0 # 有一些和nala的版本警告
# 验证安装
python3 -m mlc_llm.build --help
```

#### 编译模型
```shell
mkdir -p dist/models && cd dist/models
#sudo nala install git-lfs
#git lfs install
#手动下载hunggingface仓库放到models文件夹里，命名为chatglm2-6b

```

安OpenCL https://llm.mlc.ai/docs/install/gpu.html#orange-pi-5-rk3588-based-sbc
```shell
cd /usr/lib
ls libm* #没有libmali-g610.so，执行下面的命令
sudo wget https://github.com/JeffyCN/mirrors/raw/libmali/lib/aarch64-linux-gnu/libmali-valhall-g610-g6p0-x11-wayland-gbm.so

cd /lib/firmware/
ls # 存在mali_csffw.bin文件，故不用操作

cd ~
sudo nala install mesa-opencl-icd
sudo mkdir -p /etc/OpenCL/vendors
echo "/usr/lib/libmali-valhall-g610-g6p0-x11-wayland-gbm.so" | sudo tee /etc/OpenCL/vendors/mali.icd
sudo nala install ocl-icd-opencl-dev
sudo nala install libxcb-dri2-0 libxcb-dri3-0 libwayland-client0 libwayland-server0 libx11-xcb1
sudo nala install clinfo
```

```shell
cd ../..
# 需要12G存储空间，可以清理tvm中build文件夹的一个大文件
python3 -m mlc_llm.build --model chatglm2-6b --target opencl --max-seq-len 8192 --quantization q0f16
```
1. You can change quantization to different option such as: `autogptq_llama_q4f16_0`, `autogptq_llama_q4f16_1`, `q0f16`, `q0f32`, `q3f16_0`, `q3f16_1`, `q4f16_0`, `q4f16_1`, `q4f16_2`, `q4f16_ft`, `q4f32_0`, `q4f32_1`, `q8f16_ft`, `q8f16_1`.

2. `q6f16_1` takes about 5GB memory, `q8f16_1` takes about 8GB memory. Make sure your device have enough memory, 16GB memory is necessary in most cases.

```shell
# 安装rust: curl --proto '=https' --tlsv1.2 -sSf https://sh.rustup.rs | sh
# create build directory
mkdir -p build && cd build
# generate build configuration
python3 ../cmake/gen_cmake_config.py
# build `mlc_chat_cli`
cmake .. && cmake --build . --parallel $(nproc) && cd ..
```

#### 使用

```shell
./build/mlc_chat_cli --help
./build/mlc_chat_cli --model chatglm2-6b-q4f16_1 --device opencl
```

#### 添加新模型

见`mlc-llm/mlc_llm/utils.py`，支持的模型有：
```python
supported_model_types = set(
    [
        "llama",
        "gpt_neox",
        "gpt_bigcode",
        "minigpt",
        "moss",
        "rwkv",
        "gptj",
        "chatglm",
        "mistral",
        "stablelm_epoch",
    ]
)
```

大语言模型项目:
- https://huggingface.co/THUDM/chatglm3-6b/tree/main
- https://github.com/baichuan-inc/Baichuan2
- https://github.com/QwenLM/Qwen

```shell
# 准备
# 提前下好放到dist/model里
# config.json还是那么改,add "vocab_size": 65024 或直接用准备好的
# 尝试解决WARNING: The model has `tokenizer.model` but not `tokenizer.json`. It isrecommended to use `tokenizer.json`, so we try convert it with `transformers`.
cd dist/models/chatglm3-6b
mv tokenizer_config.json tokenizer.json

# 在mlc_llm文件夹下
sudo python3 -m mlc_llm.build --model chatglm3-6b --target opencl --max-seq-len 8192 --quantization q0f16
# 然后直接运行，无需再编译mlc chat
./build/mlc_chat_cli --model chatglm3-6b-q0f16 --device opencl
```

### 其他模型

```shell
# WAV2VEC
# 在PC上
git clone https://github.com/ccoreilly/wav2vec2-service.git
python wav2vec2-service/convert_torch_to_onnx.py --model checkpoint-883500/  # 代码中默认opset=11
mv .onnx sss.onnx
pip install python-multipart

# 在开发板上
pip install onnxruntime # 慢

# RKNN移植
# 改wav2vec2-service/convert_torch_to_onnx.py的opset=12
python wav2vec2-service/convert_torch_to_onnx.py --model checkpoint-883500/
mv .onnx model_opset12.onnx
```

### 镜像备份

必须是官方镜像，如`rock-5b_ubuntu_jammy_kde_b39.img.xz`

教程：https://github.com/Ken-Vamrs/rockpi-toolkit/tree/master

https://github.com/radxa/backup-sh/tree/master

装官方的，不要装错版本了

这里需要保证apt能正常使用

```shell
chmod +x rockpi-backup.sh
sudo ./rockpi-backup.sh
```

## OpenCL

**OpenCL**（Open Computing Language，开放计算语言）是一个为异构平台编写程序的框架，此异构平台可由CPU、GPU、DSP、FPGA或其他类型的处理器与硬件加速器所组成。OpenCL由一门用于编写kernels（在OpenCL设备上运行的函数）的语言（基于C99）和一组用于定义并控制平台的API组成。



